<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script></script>
  <title>Scholar Parser</title>
</head>

<body>
  <div id="bar"></div>
  <div id="wrapper">
    <!-- <div class="paper">
            <h1>paper.title</h1>
            <ol>
                <li>
                    <div class="name"><a href="#">citationName</a></div>
                    <div class="author">citation.authors</div>
                    <div class="conment">
                        <ul>
                            <li>conment</li>
                            <li>conment</li>
                        </ul>
                    </div>
                </li>
                <li>
                    ....
                </li>
            </ol>
        </div> -->
  </div>
</body>
<script>
  // 侧边栏js
  var count = 0;
  var fellows = {
    "Daniel Andrews": "IEEE Fellow",
    "David August": "IEEE Fellow",
    "Ewert Bengtsson": "IEEE Fellow",
    "Ricardo Bianchini": "IEEE Fellow, ACM Fellow",
    "Kenneth Birman": "IEEE Fellow",
    "Aaron Bobick": "IEEE Fellow",
    "Azzedine Boukerche": "IEEE Fellow",
    "Rajkumar Buyya": "IEEE Fellow",
    "Christian Cachin": "IEEE Fellow, ACM Fellow",
    "Jiannong Cao": "IEEE Fellow",
    "Joseph Cavallaro": "IEEE Fellow",
    "Elizabeth Chang": "IEEE Fellow",
    "Xiuzhen Cheng": "IEEE Fellow",
    "Henrik Christensen": "IEEE Fellow",
    "Sajal Das": "IEEE Fellow",
    "Dipankar Dasgupta": "IEEE Fellow",
    "Joe Decuir": "IEEE Fellow",
    "Murthy Devarakonda": "IEEE Fellow",
    "Peter Dinda": "IEEE Fellow",
    "Rolf Drechsler": "IEEE Fellow",
    "Stephanie Forrest": "IEEE Fellow",
    "Henry Fuchs": "IEEE Fellow, ACM Fellow",
    "Pascale Fung": "IEEE Fellow",
    "Patrick Girard": "IEEE Fellow",
    "Manimaran Govindarasu": "IEEE Fellow",
    "S. Gunasekaran": "IEEE Fellow",
    "Dan Gusfield": "IEEE Fellow, ACM Fellow",
    "Dan Halperin": "IEEE Fellow, ACM Fellow",
    "Constance Heitmeyer": "IEEE Fellow",
    "Abdelsalam (Sumi) Helal": "IEEE Fellow",
    "Joerg Henkel": "IEEE Fellow, ACM Fellow",
    "Jianying Hu": "IEEE Fellow",
    "Ravishankar Iyer": "IEEE Fellow, ACM Fellow",
    "Qiang Ji": "IEEE Fellow",
    "Hong Jiang": "IEEE Fellow",
    "Anupam Joshi": "IEEE Fellow",
    "Christoforos Kozyrakis": "IEEE Fellow",
    "David Kriegman": "IEEE Fellow",
    "Deepa Kundur": "IEEE Fellow",
    "Edmund Lam": "IEEE Fellow",
    "Steven Levitan": "IEEE Fellow",
    "Keqin Li": "IEEE Fellow",
    "Baochun Li": "IEEE Fellow",
    "Ling Liu": "IEEE Fellow",
    "Cheng-Lin Liu": "IEEE Fellow",
    "Yunhao Liu": "IEEE Fellow, ACM Fellow",
    "Wenjing Lou": "IEEE Fellow",
    "Scott Mahlke": "IEEE Fellow, ACM Fellow",
    "Diana Marculescu": "IEEE Fellow, ACM Fellow",
    "Patrick McDaniel": "IEEE Fellow, ACM Fellow",
    "Hong Mei": "IEEE Fellow, ACM Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ethan Miller": "IEEE Fellow",
    "Stefan Mozar": "IEEE Fellow",
    "Alexandru Nicolau": "IEEE Fellow",
    "Radha Poovendran": "IEEE Fellow",
    "Rasheek Rifaat": "IEEE Fellow",
    "Eric Rotenberg": "IEEE Fellow",
    "Ponnuswamy Sadayappan": "IEEE Fellow",
    "Michael Shebanow": "IEEE Fellow",
    "Wonyong Sung": "IEEE Fellow",
    "James Truchard": "IEEE Fellow",
    "Mahesh Viswanathan": "IEEE Fellow",
    "Geoffrey Webb": "IEEE Fellow",
    "David Weiss": "IEEE Fellow",
    "Yuan Xie": "IEEE Fellow, ACM Fellow",
    "Bulent Yener": "IEEE Fellow",
    "Wang Yi": "IEEE Fellow, ACM Fellow",
    "Moti Yung": "IEEE Fellow, ACM Fellow",
    "Yin Zhang": "IEEE Fellow",
    "Kun Zhou": "IEEE Fellow, ACM Fellow, IEEE/ACM Fellow",
    "Yuanyuan Zhou": "IEEE Fellow, ACM Fellow",
    "David Abramso": "IEEE Fellow",
    "Kiyoharu Aizaw": "IEEE Fellow",
    "Alexandre Alves da Silv": "IEEE Fellow",
    "Lorenzo Alvis": "IEEE Fellow",
    "David Atienza\u2014EPFL": "IEEE Fellow",
    "Ronald Azum": "IEEE Fellow",
    "Michael Branick": "IEEE Fellow",
    "David Brook": "IEEE Fellow",
    "Umit Catalyure": "IEEE Fellow",
    "Tony Cha": "IEEE Fellow",
    "Shigang Che": "IEEE Fellow",
    "Shu-Ching Che": "IEEE Fellow",
    "Degang Che": "IEEE Fellow",
    "Xilin Che": "IEEE Fellow",
    "Armando Colomb": "IEEE Fellow",
    "Tracy Cam": "IEEE Fellow",
    "Lorrie Crano": "IEEE Fellow",
    "Timothy Davi": "IEEE Fellow",
    "Leila De Florian": "IEEE Fellow",
    "Robert Den": "IEEE Fellow",
    "Yixin Dia": "IEEE Fellow",
    "Schahram Dustda": "IEEE Fellow",
    "Dinei Florenci": "IEEE Fellow",
    "Michael Fran": "IEEE Fellow",
    "Fernando Gomid": "IEEE Fellow",
    "Gerhard Hanck": "IEEE Fellow",
    "Edwin Hancoc": "IEEE Fellow",
    "Scott Hauc": "IEEE Fellow",
    "Larry Hec": "IEEE Fellow",
    "Gernot Heise": "IEEE Fellow",
    "Xiaobo H": "IEEE Fellow",
    "Yu Charlie H": "IEEE Fellow",
    "Mahmut Kandemi": "IEEE Fellow",
    "Nam Sung Ki": "IEEE Fellow",
    "Mark Laubac": "IEEE Fellow",
    "Hui Le": "IEEE Fellow",
    "Charles Leiserso": "IEEE Fellow",
    "Xuemin Lin\u2014UNSW": "IEEE Fellow",
    "Stefano Lonard": "IEEE Fellow",
    "Chenyang L": "IEEE Fellow",
    "Shih-Lien L": "IEEE Fellow",
    "Joao Marques Silv": "IEEE Fellow",
    "Nenad Medvidovi": "IEEE Fellow",
    "Dimitri Metaxa": "IEEE Fellow",
    "Frank Muelle": "IEEE Fellow",
    "Alexandros Potamiano": "IEEE Fellow",
    "Calton P": "IEEE Fellow",
    "Wendi Rabiner Heinzelma": "IEEE Fellow",
    "Sreeranga Raja": "IEEE Fellow",
    "Kui Re": "IEEE Fellow",
    "Chris Rowe": "IEEE Fellow",
    "Jagannathan Sarangapan": "IEEE Fellow",
    "Karsten Schwan": "IEEE Fellow",
    "Sudipta Sengupt": "IEEE Fellow",
    "Weisong Sh": "IEEE Fellow",
    "Matteo Sonza Reord": "IEEE Fellow",
    "Diane Thiede Rove": "IEEE Fellow",
    "K. Venugopa": "IEEE Fellow",
    "Sarma Bala Vrudhul": "IEEE Fellow",
    "Cliff Wan": "IEEE Fellow",
    "Jia Wan": "IEEE Fellow",
    "Chengzhong X": "IEEE Fellow",
    "Daniel Zen": "IEEE Fellow",
    "Xi Zhan": "IEEE Fellow",
    "N. Asoka": "IEEE Fellow",
    "Todd Austi": "IEEE Fellow",
    "Alper Buyuktosunogl": "IEEE Fellow",
    "Franck Cappell": "IEEE Fellow",
    "Michael Care": "IEEE Fellow",
    "Luca Carlon": "IEEE Fellow",
    "Edward Chan": "IEEE Fellow",
    "Kiyoung Cho": "IEEE Fellow",
    "Sorin Cotofan": "IEEE Fellow",
    "Robert Cunningha": "IEEE Fellow",
    "Tamal De": "IEEE Fellow",
    "Danny Dole": "IEEE Fellow",
    "Dov Dor": "IEEE Fellow",
    "Frederick Dougli": "IEEE Fellow",
    "Falko Dressle": "IEEE Fellow",
    "Ram Duvvuru Srira": "IEEE Fellow",
    "Sandhya Dwarkada": "IEEE Fellow",
    "Pablo Esteve": "IEEE Fellow",
    "Edward Fo": "IEEE Fellow",
    "Minos Garofalaki": "IEEE Fellow",
    "Soonhoi H": "IEEE Fellow",
    "Saman Halgamug": "IEEE Fellow",
    "Hossam Hassanei": "IEEE Fellow",
    "Payam Heydar": "IEEE Fellow",
    "Hugues Hopp": "IEEE Fellow",
    "Zhenyu Huan": "IEEE Fellow",
    "Deog-Kyoon Jeon": "IEEE Fellow",
    "Hironori Kasahar": "IEEE Fellow",
    "Alvin Lebec": "IEEE Fellow",
    "Kuen-Jon Le": "IEEE Fellow",
    "Hsien-Hsin Le": "IEEE Fellow",
    "Hang L": "IEEE Fellow",
    "Phone Li": "IEEE Fellow",
    "Tieyan Li": "IEEE Fellow",
    "Gabriel Lo": "IEEE Fellow",
    "David Lowthe": "IEEE Fellow",
    "David Luebk": "IEEE Fellow",
    "Joseph Lyle": "IEEE Fellow",
    "Pitu Mirchandan": "IEEE Fellow",
    "Jean Michel Mulle": "IEEE Fellow",
    "Nuria Olive": "IEEE Fellow",
    "Rafail Ostrovsk": "IEEE Fellow",
    "Haesun Par": "IEEE Fellow",
    "Jung-Min Par": "IEEE Fellow",
    "Li-Shiuan Pe": "IEEE Fellow",
    "Lili Qi": "IEEE Fellow",
    "Ravi Ramamoorth": "IEEE Fellow",
    "William Regl": "IEEE Fellow",
    "Reza Rejai": "IEEE Fellow",
    "Gregg Rotherme": "IEEE Fellow",
    "Bernt Schiel": "IEEE Fellow",
    "Michael Schult": "IEEE Fellow",
    "Stanley Sclarof": "IEEE Fellow",
    "Behzad Shahrara": "IEEE Fellow",
    "Hamid Shari": "IEEE Fellow",
    "Cristina Silvan": "IEEE Fellow",
    "Alan Smeato": "IEEE Fellow",
    "Carol Smidt": "IEEE Fellow",
    "Anuj Srivastav": "IEEE Fellow",
    "Sabine Susstrun": "IEEE Fellow",
    "Nuno Vasconcelo": "IEEE Fellow",
    "Jeffrey Vette": "IEEE Fellow",
    "Wenping Wan": "IEEE Fellow",
    "Jianyong Wan": "IEEE Fellow",
    "Chonggang Wan": "IEEE Fellow",
    "David Whalle": "IEEE Fellow",
    "Richard Wolsk": "IEEE Fellow",
    "Ying W": "IEEE Fellow",
    "Alexandre Yakovle": "IEEE Fellow",
    "Mohammed Zak": "IEEE Fellow",
    "Ce Zh": "IEEE Fellow",
    "Anastasia Ailamaki": "IEEE Fellow, ACM Fellow",
    "Walid Are": "IEEE Fellow",
    "Kohtaro Asai": "IEEE Fellow",
    "Marco Caccam": "IEEE Fellow",
    "Linda Cam": "IEEE Fellow",
    "Yiran Che": "IEEE Fellow",
    "Yiu-Ming Cheun": "IEEE Fellow",
    "Gary Christense": "IEEE Fellow",
    "Thomas Coughli": "IEEE Fellow",
    "Ewa Deelma": "IEEE Fellow",
    "Alan Edelma": "IEEE Fellow",
    "Lieven Eeckhou": "IEEE Fellow",
    "Steven Feine": "IEEE Fellow",
    "Pascal Frossar": "IEEE Fellow",
    "Kevin F": "IEEE Fellow",
    "Thomas Furnes": "IEEE Fellow",
    "Mounir Ghogh": "IEEE Fellow",
    "Yihong Gon": "IEEE Fellow",
    "Minyi Gu": "IEEE Fellow",
    "Haibo H": "IEEE Fellow",
    "Tian He\u2014University of Minnesota": "IEEE Fellow",
    "Pan Hu": "IEEE Fellow",
    "Brad Hutching": "IEEE Fellow",
    "Somesh Jh": "IEEE Fellow",
    "Jiaya Ji": "IEEE Fellow",
    "Lynette Jone": "IEEE Fellow",
    "George Karypi": "IEEE Fellow",
    "Angelos Keromyti": "IEEE Fellow",
    "Roberta Klatzk": "IEEE Fellow",
    "Xenofon Koutsouko": "IEEE Fellow",
    "D. Richard Kuh": "IEEE Fellow",
    "Susan Lan": "IEEE Fellow",
    "Tao L": "IEEE Fellow",
    "Qun L": "IEEE Fellow",
    "Jiao Li-chen": "IEEE Fellow",
    "Zhouchen Li": "IEEE Fellow",
    "Giuseppe Lipar": "IEEE Fellow",
    "Jie Li": "IEEE Fellow",
    "Deepankar Medh": "IEEE Fellow",
    "Russell Meie": "IEEE Fellow",
    "Tommaso Melodi": "IEEE Fellow",
    "Abraham Mendelso": "IEEE Fellow",
    "Konstantina Nikit": "IEEE Fellow",
    "Hidetoshi Onoder": "IEEE Fellow",
    "Jignesh Pate": "IEEE Fellow",
    "Joseph Pawlowsk": "IEEE Fellow",
    "Konstantinos Psouni": "IEEE Fellow",
    "Hairong Q": "IEEE Fellow",
    "Bjoern Schulle": "IEEE Fellow",
    "Assaf Schuster\u2014Technion": "IEEE Fellow",
    "Andrew Senio": "IEEE Fellow",
    "Sanjit Seshi": "IEEE Fellow",
    "Jiwu Sh": "IEEE Fellow",
    "Yan Solihi": "IEEE Fellow",
    "Salvatore Stolf": "IEEE Fellow",
    "Peter Ston": "IEEE Fellow",
    "David Stor": "IEEE Fellow",
    "Rahul Sukthanka": "IEEE Fellow",
    "Mark Tehranipoo": "IEEE Fellow",
    "Juergen Teic": "IEEE Fellow",
    "Yingli Tia": "IEEE Fellow",
    "John Tsotso": "IEEE Fellow",
    "Giovanni Vigna\u2014University of California": "IEEE Fellow",
    "Haixun Wan": "IEEE Fellow",
    "Li-C Wan": "IEEE Fellow",
    "Mark Weis": "IEEE Fellow",
    "Laurie William": "IEEE Fellow",
    "Rebecca Wrigh": "IEEE Fellow",
    "Tao Xi": "IEEE Fellow",
    "Yanyong Zhan": "IEEE Fellow",
    "Lei Zhan": "IEEE Fellow",
    "Xiaofang Zho": "IEEE Fellow",
    "Edward Adelso": "IEEE Fellow",
    "Michael Backe": "IEEE Fellow",
    "Meng-fan Chan": "IEEE Fellow",
    "Deming Che": "IEEE Fellow",
    "Jong-Deok Cho": "IEEE Fellow",
    "Paul Cho": "IEEE Fellow",
    "Peter Clou": "IEEE Fellow",
    "Michael Condr": "IEEE Fellow",
    "Kerstin Dautenhah": "IEEE Fellow",
    "Xiaotie Den": "IEEE Fellow",
    "Meng Hwa E": "IEEE Fellow",
    "Joseph Evan": "IEEE Fellow",
    "Robert Fis": "IEEE Fellow",
    "Dimitrios Fotiadi": "IEEE Fellow",
    "Mark Fo": "IEEE Fellow",
    "Anne Gattike": "IEEE Fellow",
    "Simson Garfinke": "IEEE Fellow",
    "Mor Harchol-balte": "IEEE Fellow",
    "Ahmed Hassa": "IEEE Fellow",
    "Xiaodong H": "IEEE Fellow",
    "Ahmed A-g Helm": "IEEE Fellow",
    "Gang Hu": "IEEE Fellow",
    "Hans-arno Jacobse": "IEEE Fellow",
    "Lee Jaeji": "IEEE Fellow",
    "Hai Ji": "IEEE Fellow",
    "Irwin Kin": "IEEE Fellow",
    "Farinaz Koushanfa": "IEEE Fellow",
    "Hai L": "IEEE Fellow",
    "Shaoying Li": "IEEE Fellow",
    "Cristina Lope": "IEEE Fellow",
    "Tim Menzie": "IEEE Fellow",
    "Onur Mutl": "IEEE Fellow",
    "Jason Nie": "IEEE Fellow",
    "Danilo Pa": "IEEE Fellow",
    "Srinivasan Raman": "IEEE Fellow",
    "Mary Ellen Randal": "IEEE Fellow",
    "Amit Roy-chowdhur": "IEEE Fellow",
    "Dan Rubenstei": "IEEE Fellow",
    "Stuart Rubi": "IEEE Fellow",
    "Rajiv Sabherwa": "IEEE Fellow",
    "Kyuseok Shi": "IEEE Fellow",
    "Mei-ling Shy": "IEEE Fellow",
    "Ramesh Sitarama": "IEEE Fellow",
    "Dawn Son": "IEEE Fellow",
    "Chi-keung Tan": "IEEE Fellow",
    "Jian Tan": "IEEE Fellow",
    "Zhuowen T": "IEEE Fellow",
    "Paul Vanoorscho": "IEEE Fellow",
    "Xiaofeng Wan": "IEEE Fellow",
    "Liang Wan": "IEEE Fellow",
    "Simon Warfiel": "IEEE Fellow",
    "John Turner Whitte": "IEEE Fellow",
    "Zhaohui W": "IEEE Fellow",
    "Eric Xin": "IEEE Fellow",
    "Ming-hsuan Yan": "IEEE Fellow",
    "Xiaokang Yan": "IEEE Fellow",
    "Hiroto Yasuur": "IEEE Fellow",
    "Moustafa Yousse": "IEEE Fellow",
    "Yizhou Y": "IEEE Fellow",
    "Daqing Zhan": "IEEE Fellow",
    "Mengjie Zhan": "IEEE Fellow",
    "Lin Zhon": "IEEE Fellow",
    "Jingren Zho": "IEEE Fellow",
    "Lidong Zho": "IEEE Fellow",
    "Michael Zyd": "IEEE Fellow",
    "Hussein Abbas": "IEEE Fellow",
    "Hari Balakrishna": "IEEE Fellow",
    "Vaughn Bet": "IEEE Fellow",
    "Wei Che": "IEEE Fellow",
    "Yingying Che": "IEEE Fellow",
    "Baoquan Che": "IEEE Fellow",
    "Lei Che": "IEEE Fellow",
    "Sangyeun Ch": "IEEE Fellow",
    "Christopher Clifto": "IEEE Fellow",
    "Carolina Cruz-Neir": "IEEE Fellow",
    "Gautam Da": "IEEE Fellow",
    "Jack Davidso": "IEEE Fellow",
    "Shlomi Dole": "IEEE Fellow",
    "Touradj Ebrahim": "IEEE Fellow",
    "Rudolf Eigenman": "IEEE Fellow",
    "Ian Foste": "IEEE Fellow",
    "Edward Fran": "IEEE Fellow",
    "Richard Fujimot": "IEEE Fellow",
    "Johannes Gehrk": "IEEE Fellow",
    "Kristen Grauma": "IEEE Fellow",
    "Song Gu": "IEEE Fellow",
    "Mohammad Hajiaghay": "IEEE Fellow",
    "Mary Hal": "IEEE Fellow",
    "Aaron Hertzman": "IEEE Fellow",
    "Alan Hevne": "IEEE Fellow",
    "Markus Hofman": "IEEE Fellow",
    "Zhenjiang H": "IEEE Fellow",
    "Weijia Ji": "IEEE Fellow",
    "William Kaise": "IEEE Fellow",
    "Ramesh Karr": "IEEE Fellow",
    "Sven Koeni": "IEEE Fellow",
    "Yun L": "IEEE Fellow",
    "Mo L": "IEEE Fellow",
    "Richard Lippman": "IEEE Fellow",
    "Xue Li": "IEEE Fellow",
    "Michael Mitzenmache": "IEEE Fellow",
    "Mohamed Mokbel\u2014University of Minnesota": "IEEE Fellow",
    "Saeid Nahavand": "IEEE Fellow",
    "Marc A. Najor": "IEEE Fellow",
    "Partha Pand": "IEEE Fellow",
    "Panagiotis Papadimitrato": "IEEE Fellow",
    "Punam Sah": "IEEE Fellow",
    "Vivek Sarka": "IEEE Fellow",
    "Evgenia Smirn": "IEEE Fellow",
    "Joshua Smit": "IEEE Fellow",
    "Alex Snoere": "IEEE Fellow",
    "Gookwon Su": "IEEE Fellow",
    "Peter Varma": "IEEE Fellow",
    "Hongyi W": "IEEE Fellow",
    "Yang Xian": "IEEE Fellow",
    "Hui Xion": "IEEE Fellow",
    "Laurence Tianruo Yan": "IEEE Fellow",
    "Jieping Y": "IEEE Fellow",
    "Tong Zhan": "IEEE Fellow",
    "Yan Zhan": "IEEE Fellow",
    "Zhongfei Zhan": "IEEE Fellow",
    "Gail-Joon Ahn": "IEEE Fellow",
    "Kemal Akkaya": "IEEE Fellow",
    "Jason H. Anderson": "IEEE Fellow",
    "Vijayalakshmi Atluri": "IEEE Fellow",
    "Raymond G. Beausoleil": "IEEE Fellow",
    "Mark N. Billinghurst": "IEEE Fellow",
    "Carlos A. Busso": "IEEE Fellow",
    "Srdjan Capkun": "IEEE Fellow, ACM Fellow",
    "Yuan-Hao Chang": "IEEE Fellow",
    "Haibo Chen": "IEEE Fellow",
    "Hao Chen": "IEEE Fellow",
    "Guihai Chen": "IEEE Fellow",
    "Yixin Chen": "IEEE Fellow",
    "Shing C. Cheung": "IEEE Fellow",
    "Frederic Chong": "IEEE Fellow",
    "Andre M. Dehon": "IEEE Fellow",
    "Wei Ding": "IEEE Fellow",
    "Wenliang Du": "IEEE Fellow",
    "Juan E Gilbert": "IEEE Fellow, ACM Fellow",
    "Amar Gupta": "IEEE Fellow",
    "Chen He": "IEEE Fellow",
    "Derek W. Hoiem": "IEEE Fellow",
    "Bin Hu": "IEEE Fellow",
    "Shuiwang Ji": "IEEE Fellow",
    "Zhi Jin": "IEEE Fellow",
    "James B. Joshi": "IEEE Fellow",
    "Shin-Ichi Kawamura": "IEEE Fellow",
    "Carl Kesselman": "IEEE Fellow, ACM Fellow",
    "Tadayoshi Kohno": "IEEE Fellow",
    "Minglu Li": "IEEE Fellow",
    "Chen Li": "IEEE Fellow",
    "Guoliang Li": "IEEE Fellow",
    "Jia Li": "IEEE Fellow",
    "Haibin Ling": "IEEE Fellow",
    "Wei Liu": "IEEE Fellow",
    "Ce Liu": "IEEE Fellow",
    "Xiaoming Liu": "IEEE Fellow",
    "Rajit Manohar": "IEEE Fellow",
    "James B. Michael": "IEEE Fellow",
    "Tamara Munzner": "IEEE Fellow",
    "Premkumar Natarajan": "IEEE Fellow",
    "Suman Nath": "IEEE Fellow",
    "Ye Ouyang": "IEEE Fellow",
    "Srinivasan Parthasarathy": "IEEE Fellow",
    "Hanspeter Pfister": "IEEE Fellow, ACM Fellow",
    "Vir V. Phoha": "IEEE Fellow",
    "Moinuddin K. Qureshi": "IEEE Fellow",
    "Sherief M. Reda": "IEEE Fellow",
    "Miguel Raul D. Rodrigues": "IEEE Fellow",
    "Eunice E Santos": "IEEE Fellow",
    "Houbing Song": "IEEE Fellow",
    "Ankur Srivastava": "IEEE Fellow",
    "Zhendong Su": "IEEE Fellow, ACM Fellow",
    "Ke Tang": "IEEE Fellow",
    "Birgit Vogel-heuser": "IEEE Fellow",
    "Jianping Wang": "IEEE Fellow",
    "Jue Wang": "IEEE Fellow",
    "Wei Wang": "IEEE Fellow, ACM Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Tilman Wolf": "IEEE Fellow",
    "Yongwei Wu": "IEEE Fellow",
    "Dongrui Wu": "IEEE Fellow",
    "Xiaokui Xiao": "IEEE Fellow",
    "Jingling Xue": "IEEE Fellow",
    "Danfeng Yao": "IEEE Fellow",
    "Shui Yu": "IEEE Fellow",
    "Gang Zhou": "IEEE Fellow",
    "Jun Zhu": "IEEE Fellow, IEEE Fellow, AE of TPAMI, AI",
    "Xingquan Zhu": "IEEE Fellow",
    "Chengqing Zong": "IEEE Fellow",
    "Tarek Abdelzaher": "IEEE Fellow, ACM Fellow",
    "Rajeev Balasubramonian": "IEEE Fellow",
    "David Basin": "IEEE Fellow, ACM Fellow",
    "Wim Bogaerts": "IEEE Fellow",
    "Achintya Bhowmik": "IEEE Fellow",
    "Kirk Cameron": "IEEE Fellow",
    "Calin Cascaval": "IEEE Fellow",
    "Bruce Croft": "IEEE Fellow, ACM Fellow",
    "Bronis De Supinski": "IEEE Fellow",
    "Sebastian Elbaum": "IEEE Fellow, ACM Fellow",
    "Natalie Enright Jerger": "IEEE Fellow",
    "Michael Ernst": "IEEE Fellow",
    "Evgeniy Gabrilovich": "IEEE Fellow, ACM Fellow",
    "James Gee": "IEEE Fellow",
    "Yolanda Gil": "IEEE Fellow, ACM Fellow",
    "Matthias Grossglauser": "IEEE Fellow",
    "Wolfgang Heidrich": "IEEE Fellow",
    "Stephen Hodges": "IEEE Fellow",
    "Ayanna Howard": "IEEE Fellow",
    "Bruce Jacob": "IEEE Fellow",
    "Daniel Jimenez": "IEEE Fellow",
    "Stefanos Kaxiras": "IEEE Fellow",
    "Kimberly Keeton": "IEEE Fellow, ACM Fellow",
    "Craig Knoblock": "IEEE Fellow, ACM Fellow",
    "Svetlana Lazebnik": "IEEE Fellow",
    "Huadong Ma": "IEEE Fellow",
    "Kyoung Mu Lee": "IEEE Fellow",
    "Wenke Lee": "IEEE Fellow, ACM Fellow",
    "Wonjun Lee": "IEEE Fellow",
    "Houqiang Li": "IEEE Fellow",
    "Ninghui Li": "IEEE Fellow, ACM Fellow",
    "Xiaofeng Liao": "IEEE Fellow",
    "Shixia Liu": "IEEE Fellow",
    "Le Lu": "IEEE Fellow",
    "Jose Martinez": "IEEE Fellow",
    "Sharad Mehrotra": "IEEE Fellow",
    "Prabhat Mishra": "IEEE Fellow",
    "Jose Moreira": "IEEE Fellow",
    "Katherine Morse": "IEEE Fellow",
    "Andreas Moshovos": "IEEE Fellow, ACM Fellow",
    "Alessandro Orso": "IEEE Fellow",
    "John Owens": "IEEE Fellow",
    "Ai-chun Pang": "IEEE Fellow",
    "Pietro Perona": "IEEE Fellow",
    "Louiqa Raschid": "IEEE Fellow, ACM Fellow",
    "Martin Reddy": "IEEE Fellow",
    "Grigore Rosu": "IEEE Fellow",
    "Dieter Schmalstieg": "IEEE Fellow",
    "Julia Schnabel": "IEEE Fellow",
    "Eve Schooler": "IEEE Fellow",
    "Alla Sheffer": "IEEE Fellow, ACM Fellow",
    "Richa Singh": "IEEE Fellow",
    "Willy Susilo": "IEEE Fellow",
    "Jie Tang": "IEEE Fellow, ACM Fellow",
    "My Thai": "IEEE Fellow",
    "Vincent Tseng": "IEEE Fellow",
    "Filip De Turck": "IEEE Fellow",
    "Jaideep Vaidya": "IEEE Fellow",
    "Wil Van Der Aalst": "IEEE Fellow, ACM Fellow",
    "Ganesh Venayagamoorthy": "IEEE Fellow",
    "Lizhe Wan": "IEEE Fellow",
    "Xiaorui Wang": "IEEE Fellow",
    "Yingxu Wang": "IEEE Fellow",
    "Jingyi Yu": "IEEE Fellow, IEEE Fellow, AE of TPAMI, TIP, CVIU",
    "Junsong Yuan": "IEEE Fellow",
    "Lintao Zhang": "IEEE Fellow",
    "Yu Zheng": "IEEE Fellow",
    "Thomas Zimmermann": "IEEE Fellow, ACM Fellow",
    "Murali Annavaram": "IEEE Fellow",
    "Grigoris Antoniou": "IEEE Fellow",
    "Sujata Banerjee": "IEEE Fellow",
    "Cullen Bash": "IEEE Fellow",
    "Boualem Benatallah": "IEEE Fellow",
    "Suparna Bhattacharya": "IEEE Fellow",
    "M Brian Blake": "IEEE Fellow",
    "Anita Carleton": "IEEE Fellow",
    "Samarjit Chakraborty": "IEEE Fellow",
    "Nitesh Chawla": "IEEE Fellow, ACM Fellow",
    "Jinjun Chen": "IEEE Fellow",
    "Minghua Chen": "IEEE Fellow",
    "Chee-yee Chong": "IEEE Fellow",
    "Walter Cleaveland": "IEEE Fellow",
    "Mauro Conti": "IEEE Fellow",
    "Tarek El-Bawab": "IEEE Fellow",
    "Sonia Fahmy": "IEEE Fellow",
    "Martin Farach-Colton": "IEEE Fellow, ACM Fellow",
    "Bonnie Ferri": "IEEE Fellow",
    "Giancarlo Fortino": "IEEE Fellow",
    "Xiaoming Fu": "IEEE Fellow",
    "Gerhard Hancke": "IEEE Fellow",
    "Torsten Hoefler": "IEEE Fellow, ACM Fellow",
    "Sun-yuan Hsieh": "IEEE Fellow",
    "Ihab Ilyas": "IEEE Fellow",
    "Yusheng Ji": "IEEE Fellow",
    "Admela Jukan": "IEEE Fellow",
    "Murat Kantarcioglu": "IEEE Fellow",
    "Latifur Khan": "IEEE Fellow",
    "Yves Letraon": "IEEE Fellow",
    "Keqiu Li": "IEEE Fellow",
    "Feifei Li": "IEEE Fellow, ACM Fellow",
    "Qing Li": "IEEE Fellow",
    "David Lo": "IEEE Fellow",
    "Robyn Lutz": "IEEE Fellow",
    "Sudip Misra": "IEEE Fellow",
    "Hiroyuki Mizuno": "IEEE Fellow",
    "Zhuoqing Morley Mao": "IEEE Fellow",
    "Masato Motomura": "IEEE Fellow",
    "Vittorio Murino": "IEEE Fellow",
    "Isaac Nassi": "IEEE Fellow",
    "Sam Noh": "IEEE Fellow",
    "Adrian Perrig": "IEEE Fellow, ACM Fellow",
    "Gopal Pingali": "IEEE Fellow",
    "Sorel Reisman": "IEEE Fellow",
    "Shiguang Shan": "IEEE Fellow",
    "Heng Tao Shen": "IEEE Fellow, ACM Fellow",
    "Timothy Sherwood": "IEEE Fellow",
    "S Sudarshan": "IEEE Fellow",
    "Rajeev Thakur": "IEEE Fellow",
    "Yonghong Tian": "IEEE Fellow",
    "Hanghang Tong": "IEEE Fellow",
    "Shambhu Upadhyaya": "IEEE Fellow",
    "Marten Van Dijk": "IEEE Fellow",
    "Mayank Vatsa": "IEEE Fellow",
    "Valeriy Vyatkin": "IEEE Fellow",
    "Dongmei Wang": "IEEE Fellow",
    "Haifeng Wang": "IEEE Fellow",
    "Jingdong Wang": "IEEE Fellow, Fellow of IAPR and IEEE, Associate Editor of IEEE TPAMI, IJCV, IEEE TMM, and IEEE TCSVT, area chair of CVPR, ICCV, ECCV, ACM MM, IJCAI, and AAAI",
    "Jun Wang": "IEEE Fellow",
    "Gregory Welch": "IEEE Fellow",
    "Andrew Wolfe": "IEEE Fellow",
    "Cathy Wu": "IEEE Fellow",
    "Xing Xie": "IEEE Fellow",
    "Li Xiong": "IEEE Fellow",
    "Ying Xu": "IEEE Fellow",
    "Ruigang Yang": "IEEE Fellow",
    "Zheng Yang": "IEEE Fellow",
    "Jiguo Yu": "IEEE Fellow",
    "Guoying Zhao": "IEEE Fellow",
    "Jacob Abraham": "IEEE Fellow, ACM Fellow",
    "Miron Abramovici": "IEEE Fellow",
    "Ramesh Agarwal": "IEEE Fellow",
    "Tilak Agerwala": "IEEE Fellow",
    "J Aggarwal": "IEEE Life Fellow",
    "Gul Agha": "IEEE Fellow, ACM Fellow",
    "Vishwani Agrawal": "IEEE Fellow",
    "Dharma Agrawal": "IEEE Fellow",
    "Prathima Agrawal": "IEEE Fellow",
    "A Agrawala": "IEEE Fellow",
    "Ishfaq Ahmad": "IEEE Fellow",
    "Alfred Aho": "IEEE Life Fellow",
    "Narendra Ahuja": "IEEE Fellow, ACM Fellow",
    "Alessandro Alberigi": "IEEE Life Fellow",
    "Robert Alden": "IEEE Life Fellow",
    "Charles Alexander": "IEEE Fellow",
    "Mubarak Ali Shah": "IEEE Fellow, ACM Fellow",
    "Frances Allen": "IEEE Fellow, ACM Fellow",
    "George Almasi": "IEEE Life Fellow",
    "Daniel Alspach": "IEEE Life Fellow",
    "Rajeev Alur": "IEEE Fellow, ACM Fellow",
    "Kitsutaro Amano": "IEEE Life Fellow",
    "Anthony Ambler": "IEEE Fellow",
    "Gene Amdahl": "IEEE Life Fellow",
    "Mostafa Ammar": "IEEE Fellow, ACM Fellow",
    "W.Cleon Anderson": "IEEE Life Fellow",
    "F Andrews": "IEEE Life Fellow",
    "Panayotis Antsaklis": "IEEE Fellow",
    "Ronald Arkin": "IEEE Fellow",
    "Gaston Arredondo": "IEEE Life Fellow",
    "Mr Arvind": "IEEE Fellow",
    "Minoru Asada": "IEEE Fellow",
    "Jaakko Astola": "IEEE Fellow",
    "Karl Johan Astrom": "IEEE Life Fellow",
    "A Avizienis": "IEEE Life Fellow",
    "J Aylor": "IEEE Fellow",
    "Hrvoje Babic": "IEEE Life Fellow",
    "Giorgio Baccarani": "IEEE Fellow",
    "M Bachynski": "IEEE Life Fellow",
    "Jean Bacon": "IEEE Fellow",
    "J Baer": "IEEE Life Fellow",
    "Radhakisan Baheti": "IEEE Fellow",
    "Paramvir Bahl": "IEEE Fellow",
    "Henry Baird": "IEEE Fellow",
    "Ruzena Bajcsy": "IEEE Life Fellow, ACM Fellow",
    "Prithviraj Banerjee": "IEEE Fellow, ACM Fellow",
    "John Baras": "IEEE Fellow",
    "Mario Barbacci": "IEEE Fellow",
    "P Bardell": "IEEE Life Fellow",
    "Victor Basili": "IEEE Fellow, ACM Fellow",
    "C Baugh": "IEEE Fellow",
    "John Bay": "IEEE Fellow",
    "Magdy Bayoumi": "IEEE Fellow",
    "W Beam": "IEEE Life Fellow",
    "Joanne Bechta Dugan": "IEEE Fellow",
    "Bernd Becker": "IEEE Fellow",
    "Miroslav Begovic": "IEEE Fellow",
    "Nicholas Begovich": "IEEE Life Fellow",
    "George Bekey": "IEEE Life Fellow",
    "Richard Belgard": "IEEE Fellow",
    "C Bell": "IEEE Life Fellow",
    "Jon Benediktsson": "IEEE Fellow",
    "Albert Benveniste": "IEEE Fellow",
    "L Beranek": "IEEE Life Fellow",
    "Reinaldo Bergamaschi": "IEEE Fellow",
    "Hal Berghel": "IEEE Fellow, ACM Fellow",
    "Arthur Bernstein": "IEEE Life Fellow",
    "Lawrence Bernstein": "IEEE Life Fellow, ACM Fellow",
    "P Bruce Berra": "IEEE Fellow",
    "Elisa Bertino": "IEEE Fellow, ACM Fellow",
    "Bir Bhanu": "IEEE Fellow",
    "Vijay Bhargava": "IEEE Fellow",
    "Bharat Bhargava": "IEEE Fellow",
    "Vijay Bhatkar": "IEEE Fellow",
    "Prabir Bhattacharya": "IEEE Fellow",
    "Bhargab Bhattacharya": "IEEE Fellow",
    "Laxmin Bhuyan": "IEEE Fellow",
    "Josef Bigun": "IEEE Fellow",
    "Donald Bitzer": "IEEE Life Fellow",
    "Andrew Blake": "IEEE Fellow",
    "E Bloch": "IEEE Life Fellow",
    "Gregor Bochmann": "IEEE Fellow",
    "Barry Boehm": "IEEE Fellow, ACM Fellow",
    "S Bokhari": "IEEE Fellow",
    "Duane Boning": "IEEE Fellow",
    "Piero Bonissone": "IEEE Fellow",
    "T Bonn": "IEEE Life Fellow",
    "Joseph Bordogna": "IEEE Life Fellow",
    "Bella Bose": "IEEE Fellow, ACM Fellow",
    "Pradip Bose": "IEEE Fellow",
    "Nikolaos Bourbakis": "IEEE Fellow",
    "K Bowles": "IEEE Life Fellow",
    "Kevin Bowyer": "IEEE Fellow",
    "Kim Boyer": "IEEE Fellow",
    "Ronald Brachman": "IEEE Fellow",
    "E Bradburd": "IEEE Life Fellow",
    "Jon Bredeson": "IEEE Life Fellow",
    "Richard Brent": "IEEE Fellow",
    "Melvin Breuer": "IEEE Life Fellow",
    "Joe Brewer": "IEEE Life Fellow",
    "Joseph Bronzino": "IEEE Life Fellow",
    "F Brooks": "IEEE Life Fellow",
    "D Brown": "IEEE Life Fellow",
    "Jehoshua Bruck": "IEEE Fellow",
    "C Burckhardt": "IEEE Life Fellow",
    "Walter Burkhard": "IEEE Fellow",
    "A Bush": "IEEE Life Fellow",
    "Michael Bushnell": "IEEE Fellow",
    "Jon Butler": "IEEE Fellow",
    "Luis Cabrera": "IEEE Fellow",
    "J. T. Cain": "IEEE Life Fellow",
    "Virginio Cantoni": "IEEE Fellow",
    "Cyrus Cantrell": "IEEE Fellow",
    "Gerard Capraro": "IEEE Fellow",
    "Larry Carin": "IEEE Fellow",
    "James Carlo": "IEEE Fellow",
    "M Carpentier": "IEEE Life Fellow",
    "Bill Carroll": "IEEE Life Fellow",
    "Doris Carver": "IEEE Fellow",
    "R Case": "IEEE Life Fellow",
    "Manuel Castro": "IEEE Fellow",
    "Francky V Catthoor": "IEEE Fellow",
    "Ralph Cavin": "IEEE Life Fellow",
    "Nicholas Cercone": "IEEE Fellow",
    "J Chadwick": "IEEE Life Fellow",
    "Krishnendu Chakrabarty": "IEEE Fellow, ACM Fellow",
    "Sreejit Chakravarty": "IEEE Fellow",
    "Donald Chamberlin": "IEEE Fellow, ACM Fellow",
    "B Chandrasekaran": "IEEE Life Fellow",
    "Shih Fu Chang": "IEEE Fellow, ACM Fellow",
    "Jin-Fu Chang": "IEEE Fellow",
    "Shi-Kuo Chang": "IEEE Fellow",
    "Carl Chang": "IEEE Fellow",
    "Hou Chaohuan": "IEEE Fellow",
    "Ramalingam Chellappa": "IEEE Fellow, ACM Fellow",
    "Yung-Chang Chen": "IEEE Fellow",
    "Wen-Tsuen Chen": "IEEE Fellow",
    "Di Chen": "IEEE Life Fellow",
    "Kwang-Ting Cheng": "IEEE Fellow",
    "Nim Cheung": "IEEE Fellow",
    "George Chiu": "IEEE Fellow",
    "Imrich Chlamtac": "IEEE Fellow, ACM Fellow",
    "Philip Chou": "IEEE Fellow",
    "Wu Chou": "IEEE Fellow",
    "W Chou": "IEEE Life Fellow",
    "Alok Choudhary": "IEEE Fellow, ACM Fellow",
    "C Chow": "IEEE Life Fellow",
    "D Christiansen": "IEEE Life Fellow",
    "Wesley Chu": "IEEE Life Fellow",
    "Pau Choo Chung": "IEEE Fellow",
    "Jen-Yao Chung": "IEEE Fellow",
    "Mehmet Civanlar": "IEEE Fellow",
    "D Clark": "IEEE Fellow",
    "Grace Clark": "IEEE Fellow",
    "Edmund Clarke": "IEEE Fellow, ACM Fellow",
    "Jean-Louis Coatrieux": "IEEE Fellow",
    "Robert Colwell": "IEEE Fellow",
    "Susan Conry": "IEEE Fellow",
    "Thomas Conte": "IEEE Fellow",
    "David Cooper": "IEEE Life Fellow",
    "John Copeland": "IEEE Life Fellow",
    "F Corbato": "IEEE Life Fellow",
    "Peter Ian Corke": "IEEE Fellow",
    "Ingemar Cox": "IEEE Fellow",
    "Jerome Cox": "IEEE Life Fellow",
    "Melba Crawford": "IEEE Fellow",
    "David Culler": "IEEE Fellow, ACM Fellow",
    "Jane Cullum": "IEEE Fellow",
    "Karl Current": "IEEE Fellow",
    "Bill Curtis": "IEEE Fellow, ACM Fellow",
    "George Cybenko": "IEEE Fellow",
    "Luigi Dadda": "IEEE Life Fellow",
    "Anton Dahbura": "IEEE Fellow",
    "William Dally": "IEEE Fellow, ACM Fellow",
    "Patricia Daniels": "IEEE Fellow",
    "Frederica Darema": "IEEE Fellow",
    "John Darringer": "IEEE Fellow",
    "Chita Das": "IEEE Fellow",
    "Sunil Das": "IEEE Life Fellow",
    "Belur Dasarathy": "IEEE Fellow",
    "Edward Davidson": "IEEE Life Fellow",
    "Anthony Davies": "IEEE Life Fellow",
    "Larry Davis": "IEEE Fellow",
    "Alan Davis": "IEEE Fellow",
    "C Davis": "IEEE Life Fellow",
    "W Kenneth Dawson": "IEEE Life Fellow",
    "Hugo De Man": "IEEE Fellow",
    "Giovanni De Micheli": "IEEE Fellow",
    "Bart De Moor": "IEEE Fellow",
    "Renato De Mori": "IEEE Fellow",
    "Mark Dean": "IEEE Fellow",
    "Maurizio Decina": "IEEE Fellow",
    "Jean-D Decotignie": "IEEE Fellow",
    "Edward Delp": "IEEE Fellow",
    "Thomas Demarco": "IEEE Fellow",
    "Serge Demidenko": "IEEE Fellow",
    "James Demmel": "IEEE Fellow, ACM Fellow",
    "Peter Denning": "IEEE Life Fellow",
    "J Dennis": "IEEE Life Fellow",
    "Narsingh Deo": "IEEE Life Fellow, ACM Fellow",
    "Srinivas Devadas": "IEEE Fellow, ACM Fellow",
    "P Dewilde": "IEEE Fellow",
    "Atam Dhawan": "IEEE Fellow",
    "Sang Hoo Dhong": "IEEE Fellow",
    "D Dietmeyer": "IEEE Life Fellow",
    "Tharam Dillon": "IEEE Fellow",
    "Stephen Director": "IEEE Fellow",
    "Francois Dolivo": "IEEE Fellow",
    "Jack Dongarra": "IEEE Fellow, ACM Fellow",
    "Arwin Dougal": "IEEE Life Fellow",
    "Mihai Draganescu": "IEEE Life Fellow",
    "K Drangeid": "IEEE Life Fellow",
    "Larry Druffel": "IEEE Life Fellow, ACM Fellow",
    "David Du": "IEEE Fellow",
    "Pradeep Dubey": "IEEE Fellow",
    "Michel Dubois": "IEEE Fellow, ACM Fellow",
    "Stephen Dukes": "IEEE Fellow",
    "James Duncan": "IEEE Fellow",
    "Edmund Durfee": "IEEE Fellow",
    "Hugh Durrant-Whyt": "IEEE Fellow",
    "Nikil Dutt": "IEEE Fellow",
    "Charles Dyer": "IEEE Fellow",
    "John Eidson": "IEEE Life Fellow",
    "H Eisner": "IEEE Life Fellow",
    "Mohamed El-Hawary": "IEEE Fellow",
    "L Ellis": "IEEE Life Fellow",
    "Joel Emer": "IEEE Fellow, ACM Fellow",
    "Sverre Eng": "IEEE Life Fellow",
    "Gerald Engel": "IEEE Fellow",
    "Irving Engelson": "IEEE Life Fellow",
    "Jose Epifaniodafranca": "IEEE Fellow",
    "Milos Ercegovac": "IEEE Fellow",
    "Rolf Ernst": "IEEE Fellow",
    "Edward Ernst": "IEEE Life Fellow",
    "Raffaele Esposito": "IEEE Life Fellow",
    "Thelma Estrin": "IEEE Life Fellow",
    "Robert Everett": "IEEE Life Fellow",
    "Yuguang Fang": "IEEE Fellow, ACM Fellow",
    "Robert Fano": "IEEE Life Fellow",
    "K Fegley": "IEEE Life Fellow",
    "Ephraim Feig": "IEEE Fellow",
    "Stuart Feldman": "IEEE Fellow, ACM Fellow",
    "Dagan Feng": "IEEE Fellow",
    "Tse-Yun Feng": "IEEE Life Fellow, ACM Fellow",
    "Domenico Ferrari": "IEEE Life Fellow, ACM Fellow",
    "Wolfgang Fichtner": "IEEE Fellow",
    "M Fischler": "IEEE Life Fellow",
    "Tor Fjeldly": "IEEE Fellow",
    "M Flynn": "IEEE Life Fellow",
    "James Foley": "IEEE Life Fellow",
    "Jose Fortes": "IEEE Fellow",
    "Thomas Fortmann": "IEEE Life Fellow",
    "H Frank": "IEEE Life Fellow",
    "Mark Franklin": "IEEE Life Fellow",
    "Paul Franzon": "IEEE Fellow",
    "Luigi Fratta": "IEEE Fellow",
    "Herbert Freeman": "IEEE Life Fellow, ACM Fellow",
    "P Freeman": "IEEE Life Fellow",
    "Ophir Frieder": "IEEE Fellow, ACM Fellow",
    "B Friedland": "IEEE Life Fellow",
    "Ivan Frisch": "IEEE Life Fellow",
    "Arrigo Frisiani": "IEEE Life Fellow",
    "F Froehlich": "IEEE Life Fellow",
    "Victor Frost": "IEEE Fellow",
    "W Kent Fuchs": "IEEE Fellow",
    "Hiromichi Fujisawa": "IEEE Fellow",
    "Hideo Fujiwara": "IEEE Fellow",
    "Keinosuke Fukunaga": "IEEE Life Fellow",
    "Daniel Gajski": "IEEE Life Fellow",
    "U Galil": "IEEE Life Fellow",
    "R Gallager": "IEEE Life Fellow",
    "Kenneth Galloway": "IEEE Life Fellow",
    "Daniel Gamota": "IEEE Fellow",
    "Karl Ganzhorn": "IEEE Life Fellow",
    "Guang Gao": "IEEE Fellow, ACM Fellow",
    "Oscar Garcia": "IEEE Life Fellow",
    "Vijay Garg": "IEEE Fellow",
    "Harvey Garner": "IEEE Life Fellow",
    "Jean-Luc Gaudiot": "IEEE Fellow",
    "Gerard Gaynor": "IEEE Life Fellow",
    "Charles Gear": "IEEE Life Fellow",
    "Randy Geiger": "IEEE Fellow",
    "Nicholas Georganas": "IEEE Fellow",
    "Mario Gerla": "IEEE Fellow, ACM Fellow",
    "Arif Ghafoor": "IEEE Fellow",
    "Carlo Ghezzi": "IEEE Fellow, ACM Fellow",
    "Joydeep Ghosh": "IEEE Fellow",
    "Sakti Ghosh": "IEEE Life Fellow",
    "Barry Gilbert": "IEEE Fellow",
    "Elmer Gilbert": "IEEE Life Fellow",
    "C Lee Giles": "IEEE Fellow",
    "Wolfgang Giloi": "IEEE Life Fellow",
    "Richard Gitlin": "IEEE Life Fellow",
    "Manfred Glesner": "IEEE Fellow",
    "Amrit Goel": "IEEE Life Fellow",
    "Kenneth Goff": "IEEE Life Fellow",
    "Maya Gokhale": "IEEE Fellow",
    "Jacob Goldberg": "IEEE Life Fellow",
    "Dmitry Goldgof": "IEEE Fellow",
    "S Jamaloddin Golestani": "IEEE Fellow",
    "Forouzan Golshani": "IEEE Fellow",
    "Cesar Gonzales": "IEEE Fellow",
    "Mario Gonzalez": "IEEE Life Fellow",
    "David Goodenough": "IEEE Fellow",
    "James Goodman": "IEEE Fellow, ACM Fellow",
    "David Goodman": "IEEE Life Fellow",
    "Inder Gopal": "IEEE Fellow",
    "B Gopinath": "IEEE Fellow",
    "Steven Gorshe": "IEEE Fellow",
    "Satoshi Goto": "IEEE Fellow",
    "Venu Govindaraju": "IEEE Fellow, ACM Fellow",
    "Ambuj Goyal": "IEEE Fellow, ACM Fellow",
    "Hans Graf": "IEEE Fellow",
    "Martin Graham": "IEEE Life Fellow",
    "James Greenleaf": "IEEE Life Fellow",
    "W Eric Grimson": "IEEE Fellow",
    "Stephen Grossberg": "IEEE Fellow",
    "Michael Gschwind": "IEEE Fellow",
    "Walter Guggenbuehl": "IEEE Life Fellow",
    "Rajesh Gupta": "IEEE Fellow, ACM Fellow",
    "Rajiv Gupta": "IEEE Fellow, ACM Fellow",
    "Dong Ha": "IEEE Fellow",
    "Susan Hackwood": "IEEE Fellow",
    "J Haddad": "IEEE Life Fellow",
    "Gregory Hager": "IEEE Fellow, ACM Fellow",
    "Kazuo Hagimoto": "IEEE Fellow",
    "Yoshiaki Hagiwara": "IEEE Fellow",
    "Brent Hailpern": "IEEE Fellow",
    "Ibrahim Hajj": "IEEE Life Fellow",
    "Lawrence Hall": "IEEE Fellow",
    "Fumio Harashima": "IEEE Life Fellow",
    "David Harel": "IEEE Fellow, ACM Fellow",
    "Robert Harrington": "IEEE Fellow",
    "Michael Harrison": "IEEE Fellow",
    "Peter Hart": "IEEE Life Fellow, ACM Fellow",
    "Reiner Hartenstein": "IEEE Life Fellow",
    "Martin Hasler": "IEEE Fellow",
    "Boudewijn Haverkort": "IEEE Fellow",
    "John Hayes": "IEEE Fellow",
    "Munro Haynes": "IEEE Life Fellow",
    "Philip Heidelberger": "IEEE Fellow, ACM Fellow",
    "George Heilmeier": "IEEE Life Fellow",
    "John Hennessy": "IEEE Fellow",
    "Thomas Henzinger": "IEEE Fellow",
    "Eric Herz": "IEEE Life Fellow",
    "William Higgins": "IEEE Fellow",
    "J Hilibrand": "IEEE Life Fellow",
    "F Hill": "IEEE Fellow",
    "Mark Hill": "IEEE Fellow",
    "Harvard Hinton": "IEEE Fellow",
    "Shigeichi Hirasawa": "IEEE Life Fellow",
    "Tin Ho": "IEEE Fellow",
    "A Hoagland": "IEEE Life Fellow",
    "L Hobbs": "IEEE Life Fellow",
    "Ronald Hoelzeman": "IEEE Life Fellow",
    "M Hoff": "IEEE Life Fellow",
    "Charles Holland": "IEEE Fellow",
    "Se June Hong": "IEEE Fellow",
    "John Hopcroft": "IEEE Life Fellow",
    "R Howe": "IEEE Life Fellow",
    "Yu Hen Hu": "IEEE Fellow",
    "Shing Huang": "IEEE Fellow",
    "T Huang": "IEEE Life Fellow",
    "Johannes Huber": "IEEE Fellow",
    "Donna Hudson": "IEEE Fellow",
    "Joseph Hughes": "IEEE Fellow",
    "Michael Huhns": "IEEE Fellow",
    "Watts Humphrey": "IEEE Life Fellow",
    "Harry Huskey": "IEEE Life Fellow",
    "Jenq-Neng Hwang": "IEEE Fellow",
    "Wei Hwang": "IEEE Fellow",
    "Wen-Mei Hwu": "IEEE Fellow, ACM Fellow",
    "Oscar Ibarra": "IEEE Fellow, ACM Fellow",
    "Tadao Ichikawa": "IEEE Life Fellow",
    "Katsuo Ikeda": "IEEE Life Fellow",
    "Katsushi Ikeuchi": "IEEE Fellow",
    "Hideki Imai": "IEEE Fellow",
    "A Ince": "IEEE Life Fellow",
    "Ronald Indeck": "IEEE Fellow",
    "K Irani": "IEEE Life Fellow",
    "Mary Irwin": "IEEE Fellow",
    "J Irwin": "IEEE Life Fellow",
    "Toru Ishida": "IEEE Fellow",
    "Rokuya Ishii": "IEEE Fellow",
    "Andre Ivanov": "IEEE Fellow",
    "J Iwersen": "IEEE Life Fellow",
    "Sitharama Iyengar": "IEEE Fellow, ACM Fellow",
    "Joseph Ja Ja": "IEEE Fellow",
    "Richard Jaeger": "IEEE Life Fellow",
    "Ravi Jain": "IEEE Fellow",
    "Anil Jain": "IEEE Fellow",
    "Raj Jain": "IEEE Fellow, ACM Fellow",
    "Ramesh Jain": "IEEE Fellow",
    "Pankaj Jalote": "IEEE Fellow",
    "Leah Jamieson": "IEEE Fellow",
    "Raymond Jarvis": "IEEE Fellow",
    "Mehdi Jazayeri": "IEEE Fellow",
    "William Jenkins": "IEEE Fellow",
    "Christian Jensen": "IEEE Fellow",
    "Niraj Jha": "IEEE Fellow, ACM Fellow",
    "Gunnar Johannsen": "IEEE Life Fellow",
    "Barry Johnson": "IEEE Fellow",
    "Sandra Johnson": "IEEE Fellow",
    "Clark Johnson": "IEEE Life Fellow",
    "Anita Jones": "IEEE Fellow",
    "E Jones": "IEEE Life Fellow",
    "A Jordan": "IEEE Life Fellow",
    "A Joshi": "IEEE Life Fellow",
    "Jing-Yang Jou": "IEEE Fellow",
    "Norman Jouppi": "IEEE Fellow, ACM Fellow",
    "William Joyner": "IEEE Fellow",
    "Graham Jullien": "IEEE Fellow",
    "J Jump": "IEEE Life Fellow",
    "Janusz Kacprzyk": "IEEE Fellow",
    "R Kahn": "IEEE Life Fellow",
    "T Kailath": "IEEE Life Fellow",
    "Moshe Kam": "IEEE Fellow",
    "Hisao Kameda": "IEEE Fellow",
    "Mohamed Kamel": "IEEE Fellow",
    "Michitaka Kameyama": "IEEE Fellow",
    "Takeo Kanade": "IEEE Fellow, ACM Fellow",
    "Laveen Kanal": "IEEE Life Fellow",
    "Kenichi Kanatani": "IEEE Fellow",
    "Abraham Kandel": "IEEE Life Fellow, ACM Fellow",
    "Dilip Kandlur": "IEEE Fellow",
    "Sung Mo Kang": "IEEE Fellow",
    "Rohit Kapur": "IEEE Fellow",
    "M Karnaugh": "IEEE Life Fellow",
    "Mark Karpovsky": "IEEE Fellow",
    "Rangachar Kasturi": "IEEE Fellow",
    "Randy Katz": "IEEE Fellow",
    "Joseph Katz": "IEEE Fellow",
    "K Katzeff": "IEEE Life Fellow",
    "Jacob Katzenelson": "IEEE Life Fellow",
    "Arie Kaufman": "IEEE Fellow",
    "Myron Kayton": "IEEE Life Fellow",
    "Zvi Kedem": "IEEE Fellow, ACM Fellow",
    "Samuel Keene": "IEEE Life Fellow",
    "Richard Kemmerer": "IEEE Fellow",
    "W Kerwin": "IEEE Life Fellow",
    "Marion Keyes": "IEEE Fellow",
    "Oussama Khatib": "IEEE Fellow",
    "Masatsuga Kidode": "IEEE Fellow",
    "K Kim": "IEEE Fellow",
    "Yongmin Kim": "IEEE Fellow",
    "C Kime": "IEEE Life Fellow",
    "W King": "IEEE Life Fellow",
    "Kozo Kinoshita": "IEEE Life Fellow",
    "Scott Kirkpatrick": "IEEE Fellow, ACM Fellow",
    "Kenichi Kitayama": "IEEE Fellow",
    "L Kleinrock": "IEEE Life Fellow",
    "Kevin Kloker": "IEEE Fellow",
    "Torleiv Klove": "IEEE Fellow",
    "Cetin Koc": "IEEE Fellow",
    "Dan Koditschek": "IEEE Fellow",
    "Peter Kogge": "IEEE Fellow",
    "F Kohli": "IEEE Life Fellow",
    "Hermann Kopetz": "IEEE Fellow",
    "Israel Koren": "IEEE Fellow",
    "Henry Korth": "IEEE Fellow",
    "S Rao Kosaraju": "IEEE Life Fellow",
    "Mitsumasa Koyanagi": "IEEE Fellow",
    "Donald Kraft": "IEEE Fellow",
    "H Kressel": "IEEE Life Fellow",
    "P Krishnaprasad": "IEEE Fellow",
    "Raghuram Krishnapuram": "IEEE Fellow",
    "Eric Kronstadt": "IEEE Fellow",
    "S Kubina": "IEEE Life Fellow",
    "Paul Kuehn": "IEEE Fellow",
    "Benjamin Kuipers": "IEEE Fellow",
    "Casimir Kulikowski": "IEEE Fellow",
    "Vipin Kumar": "IEEE Fellow, ACM Fellow",
    "Luis Kun": "IEEE Fellow",
    "Sandip Kundu": "IEEE Fellow",
    "Tosiyasu Kunii": "IEEE Life Fellow",
    "Murat Kunt": "IEEE Fellow",
    "Wolfgang Kunz": "IEEE Fellow",
    "Sy-Yen Kuo": "IEEE Fellow",
    "Chung-C Kuo": "IEEE Fellow",
    "Fadi Kurdahi": "IEEE Fellow",
    "K Kurokawa": "IEEE Life Fellow",
    "James Kurose": "IEEE Fellow, ACM Fellow",
    "Kazuo Kyuma": "IEEE Fellow",
    "Thomas La Porta": "IEEE Fellow",
    "Parag Lala": "IEEE Fellow",
    "Jaynarayan Lala": "IEEE Fellow",
    "Theodore Laliotis": "IEEE Life Fellow",
    "Simon Lam": "IEEE Fellow",
    "Phillip Laplante": "IEEE Fellow",
    "Arvid Larson": "IEEE Life Fellow",
    "Alan Laub": "IEEE Fellow",
    "Neal Laurance": "IEEE Life Fellow",
    "Stephen Lavenberg": "IEEE Fellow",
    "Duncan Lawrie": "IEEE Fellow",
    "Harold Lawson": "IEEE Fellow",
    "Edward Lazowska": "IEEE Fellow, ACM Fellow",
    "Franz Leberl": "IEEE Fellow",
    "Insup Lee": "IEEE Fellow, ACM Fellow",
    "Edward Lee": "IEEE Fellow",
    "Der Tsai Lee": "IEEE Fellow",
    "Ruby Lee": "IEEE Fellow",
    "Sukhan Lee": "IEEE Fellow",
    "Will Leland": "IEEE Fellow",
    "George Lendaris": "IEEE Life Fellow",
    "B Leon": "IEEE Life Fellow",
    "C.T. Leondes": "IEEE Life Fellow",
    "Alberto Leon-Garcia": "IEEE Fellow",
    "Victor Leung": "IEEE Fellow",
    "Wu-Hon Leung": "IEEE Fellow",
    "Stefano Levialdi": "IEEE Life Fellow",
    "Martin Levine": "IEEE Life Fellow",
    "Henry Levy": "IEEE Fellow",
    "Peter Lewis": "IEEE Life Fellow",
    "Victor Li": "IEEE Fellow",
    "Chung Li": "IEEE Fellow",
    "Ching Li": "IEEE Life Fellow",
    "Paul Liao": "IEEE Fellow",
    "Jorg Liebeherr": "IEEE Fellow",
    "Michael Lightner": "IEEE Fellow",
    "Leo Ligthart": "IEEE Fellow",
    "Fred Liguori": "IEEE Life Fellow",
    "David Lilja": "IEEE Fellow",
    "Bao-Shuh Lin": "IEEE Fellow",
    "J Lindenlaub": "IEEE Life Fellow",
    "Bruce Lindsay": "IEEE Fellow, ACM Fellow",
    "Stuart Lipoff": "IEEE Fellow",
    "G Lipovski": "IEEE Fellow",
    "John Little": "IEEE Fellow",
    "Jane Liu": "IEEE Fellow",
    "Chen-Ching Liu": "IEEE Fellow",
    "Ming Liu": "IEEE Life Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chao-Ning Liu": "IEEE Life Fellow",
    "Hans-A Loeliger": "IEEE Fellow",
    "Murray Loew": "IEEE Fellow",
    "William Loftus": "IEEE Fellow",
    "Joseph Logue": "IEEE Life Fellow",
    "David Lomet": "IEEE Life Fellow",
    "Darrell Long": "IEEE Fellow",
    "Philip Lopresti": "IEEE Life Fellow",
    "Alexander Loui": "IEEE Fellow",
    "Michael Loui": "IEEE Fellow",
    "Steven Low": "IEEE Fellow",
    "Fabrizio Luccio": "IEEE Life Fellow",
    "Robert Lucky": "IEEE Life Fellow",
    "J Luh": "IEEE Life Fellow",
    "Stephen Lundstrom": "IEEE Life Fellow",
    "Peilin Luo": "IEEE Life Fellow",
    "David Lynch": "IEEE Life Fellow",
    "Michael Rung-Tsong Lyu": "IEEE Fellow, ACM Fellow",
    "John Macdonald": "IEEE Life Fellow",
    "Benoit M Macq": "IEEE Fellow",
    "Vijay Madisetti": "IEEE Fellow",
    "Azad Madni": "IEEE Fellow",
    "John Makhoul": "IEEE Life Fellow",
    "Jitendra Malik": "IEEE Fellow, ACM Fellow, Fellow of the IEEE, ACM, and the American Academy of Arts and Sciences, and a member of the National Academy of Sciences and the National Academy of Engineering",
    "Henrique Malvar": "IEEE Fellow",
    "Ebrahim Mamdani": "IEEE Life Fellow",
    "Laurence Manning": "IEEE Life Fellow",
    "P Mantey": "IEEE Life Fellow",
    "Petros Maragos": "IEEE Fellow",
    "H Marcy": "IEEE Life Fellow",
    "M M Marek-Sadowska": "IEEE Fellow",
    "Akira Masaki": "IEEE Fellow",
    "Gerald Masson": "IEEE Fellow",
    "Takashi Matsumoto": "IEEE Fellow",
    "Yoshihiro Matsumoto": "IEEE Life Fellow",
    "Yasuo Matsuyama": "IEEE Fellow",
    "Ueli Maurer": "IEEE Fellow",
    "Roy Maxion": "IEEE Fellow",
    "Gary May": "IEEE Fellow",
    "George Mcclure": "IEEE Life Fellow",
    "Edward McCluskey": "IEEE Life Fellow, ACM Fellow",
    "Nancy Mead": "IEEE Fellow",
    "Gerard Medioni": "IEEE Fellow",
    "Raman Mehra": "IEEE Fellow",
    "James Meindl": "IEEE Life Fellow",
    "Rami Melhem": "IEEE Fellow",
    "Jaishankar Menon": "IEEE Fellow",
    "P Menon": "IEEE Life Fellow",
    "C Merriam": "IEEE Life Fellow",
    "John Metzner": "IEEE Life Fellow",
    "J Meyer": "IEEE Life Fellow",
    "Marlin Mickle": "IEEE Life Fellow",
    "Tetsuya Miki": "IEEE Fellow",
    "Raymond Miller": "IEEE Life Fellow, ACM Fellow",
    "William Miller": "IEEE Life Fellow",
    "David Mills": "IEEE Fellow",
    "Veljko Milutinovic": "IEEE Fellow",
    "Yinghua Min": "IEEE Fellow",
    "Jack Minker": "IEEE Life Fellow, ACM Fellow",
    "Frederick Mintzer": "IEEE Fellow",
    "J Modestino": "IEEE Life Fellow",
    "K Mohiuddin": "IEEE Fellow",
    "James Moore": "IEEE Fellow",
    "Nelson Morgan": "IEEE Fellow",
    "Kinji Mori": "IEEE Fellow",
    "Lloyd Morley": "IEEE Life Fellow",
    "George Moschytz": "IEEE Life Fellow",
    "Joel Moses": "IEEE Fellow, ACM Fellow",
    "Hussein Mouftah": "IEEE Fellow",
    "James Moulic": "IEEE Fellow",
    "Samiha Mourad": "IEEE Fellow",
    "Trevor Mudge": "IEEE Fellow, ACM Fellow",
    "George Mueller": "IEEE Life Fellow",
    "Amar Mukherjee": "IEEE Fellow",
    "Koso Murakami": "IEEE Fellow",
    "Hiroshi Murase": "IEEE Fellow",
    "Tadao Murata": "IEEE Life Fellow",
    "J Musa": "IEEE Life Fellow",
    "Makoto Nagao": "IEEE Fellow",
    "H Nagel": "IEEE Life Fellow",
    "George Nagy": "IEEE Life Fellow",
    "Klara Nahrstedt": "IEEE Fellow, ACM Fellow",
    "Ravi Nair": "IEEE Fellow",
    "Tsuneo Nakahara": "IEEE Life Fellow",
    "Tadao Nakamura": "IEEE Fellow",
    "Yukihiro Nakamura": "IEEE Fellow",
    "Ryohei Nakatsu": "IEEE Fellow",
    "Wataru Nakayama": "IEEE Fellow",
    "Takashi Nanya": "IEEE Fellow",
    "C Narayanaswami": "IEEE Fellow",
    "Sani Nassif": "IEEE Fellow",
    "Erich Neuhold": "IEEE Fellow",
    "Peter Neumann": "IEEE Life Fellow",
    "Yrjo Neuvo": "IEEE Fellow",
    "Ramakant Nevatia": "IEEE Life Fellow",
    "Lionel Ni": "IEEE Fellow",
    "David Nicol": "IEEE Fellow",
    "Paul Nielsen": "IEEE Fellow",
    "Jurg Nievergelt": "IEEE Life Fellow",
    "Paul Nikolich": "IEEE Fellow",
    "Stig Nilsson": "IEEE Life Fellow",
    "Shogo Nishida": "IEEE Fellow",
    "Takao Nishizeki": "IEEE Fellow, ACM Fellow",
    "Tohei Nitta": "IEEE Life Fellow",
    "Josef Nossek": "IEEE Fellow",
    "David Notkin": "IEEE Fellow",
    "Henri Nussbaumer": "IEEE Life Fellow",
    "Roy Nutter": "IEEE Fellow",
    "Mohammad Obaidat": "IEEE Fellow",
    "Lawrence O'Gorman": "IEEE Fellow",
    "Maciej Ogorzalek": "IEEE Fellow",
    "Tadahiro Ohmi": "IEEE Fellow",
    "Tatsuo Ohtsuki": "IEEE Life Fellow",
    "Erkki Oja": "IEEE Fellow",
    "O Olukotun": "IEEE Fellow",
    "Kinji Ono": "IEEE Life Fellow",
    "Morio Onoe": "IEEE Life Fellow",
    "Daniel Ostapko": "IEEE Life Fellow",
    "Joern Ostermann": "IEEE Fellow",
    "Bjorn Ottersten": "IEEE Fellow",
    "Thomas Overbye": "IEEE Fellow",
    "David Padua": "IEEE Fellow, ACM Fellow",
    "James Palmer": "IEEE Life Fellow",
    "Cherri Pancake": "IEEE Fellow",
    "Sethuraman Panchanathan": "IEEE Fellow, ACM Fellow",
    "D Panda": "IEEE Fellow",
    "Karen Panetta": "IEEE Fellow",
    "Christos Papachristou": "IEEE Life Fellow",
    "N Papanikolopoulos": "IEEE Fellow",
    "Stephen Pardee": "IEEE Life Fellow",
    "Behrooz Parhami": "IEEE Fellow",
    "Alice Parker": "IEEE Fellow",
    "Edward Parrish": "IEEE Life Fellow",
    "Rajnikant Patel": "IEEE Fellow",
    "Lalit Patnaik": "IEEE Fellow",
    "Yale Patt": "IEEE Fellow, ACM Fellow",
    "David Patterson": "IEEE Fellow, ACM Fellow",
    "Sanjoy Paul": "IEEE Fellow",
    "Raymond Paul": "IEEE Fellow",
    "T Pavlidis": "IEEE Life Fellow",
    "Witold Pedrycz": "IEEE Fellow",
    "Soo-Chang Pei": "IEEE Fellow",
    "P Penfield": "IEEE Life Fellow",
    "Ronald Perrott": "IEEE Fellow",
    "W Peterson": "IEEE Life Fellow",
    "Dragutin Petkovic": "IEEE Fellow",
    "S Petrillo": "IEEE Life Fellow",
    "Frederick Petry": "IEEE Fellow",
    "Hoang Pham": "IEEE Fellow",
    "Rosalind Picard": "IEEE Fellow",
    "Michael Picheny": "IEEE Fellow",
    "Raymond Pickholtz": "IEEE Life Fellow",
    "R Piloty": "IEEE Life Fellow",
    "Peter Pirsch": "IEEE Fellow",
    "Ioannis Pitas": "IEEE Fellow",
    "M Pitke": "IEEE Life Fellow",
    "Vincenzo Piuri": "IEEE Fellow",
    "Rejean Plamondon": "IEEE Fellow",
    "A Pohm": "IEEE Life Fellow",
    "Irith Pomeranz": "IEEE Fellow",
    "James Pomerene": "IEEE Life Fellow",
    "Jean Ponce": "IEEE Fellow, ERC advanced grant, Editor-in-Chief of the International Journal of Computer Vision",
    "S Pookaiyaudom": "IEEE Fellow",
    "Douglass Post": "IEEE Fellow",
    "Dhiraj Pradhan": "IEEE Fellow, ACM Fellow",
    "Birendra Prasada": "IEEE Life Fellow",
    "Viktor Prasanna": "IEEE Fellow, ACM Fellow",
    "R Pritchard": "IEEE Life Fellow",
    "Walter Proebster": "IEEE Life Fellow",
    "John Pullen": "IEEE Fellow",
    "C Raghavendra": "IEEE Fellow",
    "S Rajasekaran": "IEEE Fellow",
    "Rochit Rajsuman": "IEEE Fellow",
    "G V S Raju": "IEEE Life Fellow",
    "S Ramadorai": "IEEE Fellow",
    "Raghu Ramakrishnan": "IEEE Fellow, ACM Fellow",
    "C Ramamoorthy": "IEEE Life Fellow",
    "Krithi Ramamritham": "IEEE Fellow",
    "N Ranganathan": "IEEE Fellow",
    "Sanjay Ranka": "IEEE Fellow",
    "Anders Rantzer": "IEEE Fellow",
    "Nageswara Rao": "IEEE Fellow",
    "Robert Rassa": "IEEE Fellow",
    "Nalini Ratha": "IEEE Fellow",
    "R Ray": "IEEE Fellow",
    "Sudhakar Reddy": "IEEE Life Fellow",
    "G Redinbo": "IEEE Life Fellow",
    "Daniel Reed": "IEEE Fellow",
    "Jeffrey Reed": "IEEE Fellow",
    "T Regulinski": "IEEE Life Fellow",
    "Johan Reiber": "IEEE Fellow",
    "Martin Reiser": "IEEE Fellow",
    "Aristides Requicha": "IEEE Life Fellow, ACM Fellow",
    "T V Rhyne": "IEEE Life Fellow",
    "William Riddle": "IEEE Fellow",
    "Robert Rivers": "IEEE Life Fellow",
    "Laura Roa": "IEEE Fellow",
    "Yves Robert": "IEEE Fellow",
    "Thomas Robertazzi": "IEEE Fellow",
    "Arthur Robinson": "IEEE Life Fellow",
    "Mortimer Rogoff": "IEEE Life Fellow",
    "Hans Rombach": "IEEE Fellow",
    "Ronny Ronen": "IEEE Fellow",
    "Arnold Rosenberg": "IEEE Fellow, ACM Fellow",
    "David Rosenblum": "IEEE Fellow",
    "Charles Rosenthal": "IEEE Life Fellow",
    "Tamas Roska": "IEEE Fellow",
    "Christian Roux": "IEEE Fellow",
    "Izhak Rubin": "IEEE Life Fellow",
    "Enrique Ruspini": "IEEE Fellow",
    "Roy Russo": "IEEE Life Fellow",
    "Rob Rutenbar": "IEEE Fellow",
    "C Rypinski": "IEEE Life Fellow",
    "Martin Sachs": "IEEE Fellow",
    "Andrew Sage": "IEEE Life Fellow",
    "C Sah": "IEEE Life Fellow",
    "Sudhakar Sahasrabudhe": "IEEE Fellow",
    "Sartaj Sahni": "IEEE Fellow",
    "George Sai-Halasz": "IEEE Fellow",
    "Nobuji Saito": "IEEE Life Fellow",
    "Tadao Saito": "IEEE Life Fellow",
    "Karem Sakallah": "IEEE Fellow, ACM Fellow",
    "Ken Sakamura": "IEEE Fellow",
    "Hiroshi Sakou": "IEEE Fellow",
    "Jerome Saltzer": "IEEE Life Fellow",
    "Kewal Saluja": "IEEE Fellow",
    "Betty Salzberg": "IEEE Fellow",
    "Ahmed Sameh": "IEEE Fellow, ACM Fellow",
    "Hanan Samet": "IEEE Fellow, ACM Fellow",
    "William Sanders": "IEEE Fellow, ACM Fellow",
    "Arthur Sanderson": "IEEE Fellow",
    "Ravi Sandhu": "IEEE Fellow",
    "Majid Sarrafzadeh": "IEEE Fellow",
    "Tsutomu Sasao": "IEEE Fellow",
    "M Satyanarayanan": "IEEE Fellow",
    "John Savage": "IEEE Life Fellow",
    "Yvon Savaria": "IEEE Fellow",
    "Jacob Savir": "IEEE Fellow",
    "Alexander Sawchuk": "IEEE Fellow",
    "Peter Scheuermann": "IEEE Fellow",
    "Richard Schlichting": "IEEE Fellow",
    "Paul Schneck": "IEEE Fellow, ACM Fellow",
    "Norman Schneidewind": "IEEE Life Fellow",
    "J Schoeffler": "IEEE Life Fellow",
    "Mischa Schwartz": "IEEE Life Fellow",
    "Edmund Schweitzer": "IEEE Fellow",
    "Norman Scott": "IEEE Life Fellow",
    "Carl Sechen": "IEEE Fellow",
    "Zary Segall": "IEEE Fellow",
    "Terrence Sejnowski": "IEEE Fellow",
    "Siegfried Selberherr": "IEEE Fellow",
    "S Sensiper": "IEEE Life Fellow",
    "Carlo Sequin": "IEEE Life Fellow",
    "Ishwar Sethi": "IEEE Fellow",
    "M Sezan": "IEEE Fellow",
    "Lui Sha": "IEEE Fellow, ACM Fellow",
    "Zong Sha": "IEEE Life Fellow",
    "Ghavam Shahidi": "IEEE Fellow",
    "Linda Shapiro": "IEEE Fellow",
    "Gustave Shapiro": "IEEE Life Fellow",
    "Mary Shaw": "IEEE Fellow",
    "Leonard Shaw": "IEEE Life Fellow",
    "Shashi Shekhar": "IEEE Fellow",
    "John Sheppard": "IEEE Fellow",
    "Amit Sheth": "IEEE Fellow, ACM Fellow",
    "Kiyohiro Shikano": "IEEE Fellow",
    "Naohisa Shimomura": "IEEE Life Fellow",
    "Kang Shin": "IEEE Fellow, ACM Fellow",
    "Isao Shirakawa": "IEEE Life Fellow",
    "Norio Shiratori": "IEEE Fellow",
    "S Shiva": "IEEE Fellow",
    "M Shooman": "IEEE Life Fellow",
    "Bruce Shriver": "IEEE Fellow",
    "Heung-Yeung Shum": "IEEE Fellow, ACM Fellow, IEEE/ACM Fellow, International Fellow of the Royal Academy Society of Engineering in the United Kingdom, editorial boards for TPAMI and the IJCV",
    "Jyuo-Min Shyu": "IEEE Fellow",
    "Howard Siegel": "IEEE Fellow, ACM Fellow",
    "Daniel Siewiorek": "IEEE Fellow, ACM Fellow",
    "Abraham Silberschatz": "IEEE Fellow, ACM Fellow",
    "D Simmons": "IEEE Life Fellow",
    "Walter Sincoskie": "IEEE Fellow",
    "Adit Singh": "IEEE Fellow",
    "Mukesh Singhal": "IEEE Fellow",
    "Bhabani Sinha": "IEEE Fellow",
    "J Sklansky": "IEEE Life Fellow",
    "Martha Sloan": "IEEE Life Fellow, ACM Fellow",
    "John Smith": "IEEE Fellow",
    "James Smith": "IEEE Fellow",
    "Alan Smith": "IEEE Fellow, ACM Fellow",
    "T Smith": "IEEE Fellow",
    "Jonathan Smith": "IEEE Fellow",
    "M Smith": "IEEE Life Fellow",
    "K Smith": "IEEE Life Fellow",
    "Burton Smith": "IEEE Life Fellow, ACM Fellow",
    "Marc Snir": "IEEE Fellow, ACM Fellow",
    "Larry Snyder": "IEEE Fellow, ACM Fellow",
    "Gurindar Sohi": "IEEE Fellow",
    "David Soldan": "IEEE Fellow",
    "Mani Soma": "IEEE Fellow",
    "Arun Somani": "IEEE Fellow",
    "Stefano Spaccapietra": "IEEE Fellow",
    "Eugene Spafford": "IEEE Fellow",
    "L Spandorfer": "IEEE Life Fellow",
    "H Spang": "IEEE Life Fellow",
    "Alfred Spector": "IEEE Fellow",
    "John Spragins": "IEEE Life Fellow",
    "Mark S Squillante": "IEEE Fellow, ACM Fellow",
    "Sargur Srihari": "IEEE Fellow",
    "Pradip Srimani": "IEEE Fellow",
    "Mani Srivastava": "IEEE Fellow",
    "Peter Staecker": "IEEE Life Fellow",
    "John Stankovic": "IEEE Fellow",
    "Ralf Steinmetz": "IEEE Fellow, ACM Fellow",
    "Per Stenstrom": "IEEE Fellow",
    "J Stiffler": "IEEE Life Fellow",
    "Guy St-Jean": "IEEE Fellow",
    "James Stoffel": "IEEE Fellow",
    "Ivan Stojmenovic": "IEEE Fellow",
    "Ernest Stokely": "IEEE Life Fellow",
    "W Stone": "IEEE Fellow",
    "Harold Stone": "IEEE Life Fellow, ACM Fellow",
    "Athanasios Stouraitis": "IEEE Fellow",
    "D Strain": "IEEE Life Fellow",
    "Charles Stroud": "IEEE Fellow",
    "Bjarne Stroustrup": "IEEE Fellow, ACM Fellow",
    "Ryszard Struzak": "IEEE Life Fellow",
    "Tatsuya Suda": "IEEE Fellow",
    "Ching Suen": "IEEE Life Fellow",
    "Gary Sullivan": "IEEE Fellow",
    "J Suran": "IEEE Life Fellow",
    "Christer Svensson": "IEEE Fellow",
    "Earl Swartzlander": "IEEE Fellow",
    "Katia Sycara": "IEEE Fellow",
    "Richard Szeliski": "IEEE Fellow, ACM Fellow, \u7f8e\u56fd\u5de5\u7a0b\u9662\u9662\u58eb\uff0cACM Fellow\uff0cIEEE Fellow\uff0c\u66fe\u4efbIJCV\u3001TPAMI\u7b49\u9876\u520a\u7f16\u59d4",
    "Janos Sztipanovits": "IEEE Fellow",
    "Stephen Szygenda": "IEEE Life Fellow",
    "Boleslaw Szymanski": "IEEE Fellow",
    "Yoshitaka Takasaki": "IEEE Life Fellow",
    "Tieniu Tan": "IEEE Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hidehiko Tanaka": "IEEE Fellow",
    "Richard Tanaka": "IEEE Life Fellow",
    "Andrew Tanenbaum": "IEEE Fellow",
    "Yuan Yan Tang": "IEEE Fellow",
    "Steven Tanimoto": "IEEE Fellow",
    "R Tanner": "IEEE Fellow",
    "Tzyh-Jong Tarn": "IEEE Life Fellow",
    "Gabriel Taubin": "IEEE Fellow",
    "Robert Tausworthe": "IEEE Life Fellow",
    "Frederick Taylor": "IEEE Life Fellow",
    "David Tennenhouse": "IEEE Fellow",
    "Lewis Terman": "IEEE Life Fellow",
    "Demetri Terzopoulos": "IEEE Fellow, ACM Fellow",
    "Stuart Tewksbury": "IEEE Fellow",
    "H Thal": "IEEE Life Fellow",
    "R Thayer": "IEEE Life Fellow",
    "Donald Thomas": "IEEE Fellow",
    "Alexander Thomasian": "IEEE Fellow",
    "Craig Thompson": "IEEE Fellow",
    "K Thulasiraman": "IEEE Fellow",
    "M Thuraisingham": "IEEE Fellow",
    "James Tien": "IEEE Fellow",
    "Fouad Tobagi": "IEEE Fellow",
    "Iwao Toda": "IEEE Fellow",
    "Yoshihiro Tohma": "IEEE Life Fellow",
    "Shoji Tominaga": "IEEE Fellow",
    "Willis Tompkins": "IEEE Life Fellow",
    "Jun-Ichiro Toriwaki": "IEEE Fellow",
    "H Torng": "IEEE Life Fellow",
    "Josep Torrellas": "IEEE Fellow, ACM Fellow",
    "Donald Towsley": "IEEE Fellow",
    "J Tracey": "IEEE Life Fellow",
    "Harry Tredennick": "IEEE Fellow",
    "Louise Trevillyan": "IEEE Fellow",
    "Timothy Trick": "IEEE Life Fellow",
    "Satish Tripathi": "IEEE Fellow",
    "Anand Tripathi": "IEEE Fellow",
    "Leonard Tripp": "IEEE Fellow",
    "Kishor Trivedi": "IEEE Fellow",
    "Charles Trowbridge": "IEEE Fellow",
    "W Trybula": "IEEE Life Fellow",
    "Jingpha Tsai": "IEEE Fellow",
    "Hidenori Tsuji": "IEEE Life Fellow",
    "Shigeo Tsujii": "IEEE Life Fellow",
    "Jonathan Turner": "IEEE Fellow, ACM Fellow",
    "Spyridon Tzafestas": "IEEE Life Fellow",
    "Shunsuke Uemura": "IEEE Fellow",
    "S Unger": "IEEE Life Fellow",
    "Andre Vacroux": "IEEE Life Fellow",
    "Vijay Vaishnavi": "IEEE Fellow",
    "Jose Valdez C": "IEEE Life Fellow",
    "Mateo Valero": "IEEE Fellow, ACM Fellow",
    "Tibor Vamos": "IEEE Life Fellow",
    "A Van De Goor": "IEEE Fellow",
    "J Van Ness": "IEEE Life Fellow",
    "A Van Roggen": "IEEE Life Fellow",
    "Andries Vandam": "IEEE Life Fellow",
    "Joos Vandewalle": "IEEE Fellow",
    "Murali Varanasi": "IEEE Life Fellow",
    "Baba Vemuri": "IEEE Fellow",
    "A Venetsanopoulos": "IEEE Life Fellow",
    "Paulo Verissimo": "IEEE Fellow",
    "Martin Vetterli": "IEEE Fellow, ACM Fellow",
    "Mathukumal Vidyasagar": "IEEE Fellow",
    "Max Viergever": "IEEE Fellow",
    "Jeffrey Vitter": "IEEE Fellow",
    "H Voelcker": "IEEE Life Fellow",
    "R Volz": "IEEE Life Fellow",
    "Mladen Vouk": "IEEE Fellow",
    "Benjamin Wah": "IEEE Fellow",
    "Michael Waidner": "IEEE Fellow",
    "Steven Wallach": "IEEE Fellow",
    "Laung Wang": "IEEE Fellow",
    "Jhing Wang": "IEEE Fellow",
    "Roy Want": "IEEE Fellow, ACM Fellow",
    "W Ware": "IEEE Life Fellow",
    "Pramod Warty": "IEEE Fellow",
    "Anthony Wasserman": "IEEE Fellow",
    "Tadashi Watanabe": "IEEE Fellow",
    "Hitoshi Watanabe": "IEEE Life Fellow",
    "Layne Watson": "IEEE Fellow",
    "R Waxman": "IEEE Life Fellow",
    "Alfred Weaver": "IEEE Fellow",
    "Harry Wechsler": "IEEE Fellow",
    "Stuart Wecker": "IEEE Fellow, ACM Fellow",
    "Mark Wegman": "IEEE Fellow, ACM Fellow",
    "Louis Weinberg": "IEEE Life Fellow",
    "Arnold Weinberger": "IEEE Life Fellow",
    "Stephen Weinstein": "IEEE Life Fellow",
    "Uri Weiser": "IEEE Fellow, ACM Fellow",
    "Paul Wesling": "IEEE Fellow",
    "Burnell West": "IEEE Life Fellow",
    "Elaine Weyuker": "IEEE Fellow, ACM Fellow",
    "Kyu Whang": "IEEE Fellow",
    "Jacob White": "IEEE Fellow",
    "Bernard Widrow": "IEEE Life Fellow",
    "Gio Wiederhold": "IEEE Fellow, ACM Fellow",
    "Thomas Williams": "IEEE Fellow",
    "Robin Williams": "IEEE Life Fellow, ACM Fellow",
    "Jeannette Wing": "IEEE Fellow",
    "Omar Wing": "IEEE Life Fellow",
    "O Winn": "IEEE Life Fellow",
    "Arthur Winston": "IEEE Life Fellow",
    "Joel Wolf": "IEEE Fellow",
    "Wayne Wolf": "IEEE Fellow",
    "Jack Wolf": "IEEE Life Fellow",
    "Martin Wong": "IEEE Fellow, ACM Fellow",
    "C Wong": "IEEE Life Fellow",
    "W Wonham": "IEEE Life Fellow",
    "Helen Wood": "IEEE Fellow",
    "David Wood": "IEEE Fellow",
    "C Woodside": "IEEE Life Fellow",
    "Ja-Ling Wu": "IEEE Fellow",
    "Kun-Lung Wu": "IEEE Fellow",
    "Chung-Yu Wu": "IEEE Fellow",
    "Cheng Wen Wu": "IEEE Fellow",
    "Wm Wulf": "IEEE Fellow",
    "Ning Xi": "IEEE Fellow",
    "Shuzo Yajima": "IEEE Life Fellow",
    "Akihiko Yamada": "IEEE Life Fellow",
    "Fuqing Yang": "IEEE Fellow, \u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Kazuo Yano": "IEEE Fellow",
    "Xin Yao": "IEEE Fellow",
    "Hiroshi Yasuda": "IEEE Fellow",
    "Stephen Yau": "IEEE Life Fellow",
    "R Yavatkar": "IEEE Fellow",
    "Raymond Yeh": "IEEE Life Fellow",
    "John Yen": "IEEE Fellow",
    "Daniel Yeung": "IEEE Fellow",
    "Pen-Chung Yew": "IEEE Fellow",
    "Sung-Joo Yoo": "IEEE Fellow",
    "Hoi-Jun Yoo": "IEEE Fellow",
    "Tzay Young": "IEEE Life Fellow",
    "Marshall Yovits": "IEEE Life Fellow",
    "Philip Yu": "IEEE Fellow",
    "H Yu": "IEEE Life Fellow",
    "Lotfi Zadeh": "IEEE Life Fellow",
    "Avideh Zakhor": "IEEE Fellow",
    "Bernard Zeigler": "IEEE Fellow",
    "Alexander Zelinsky": "IEEE Fellow",
    "Marvin Zelkowitz": "IEEE Fellow",
    "Zhengyou Zhang": "IEEE Fellow, ACM Fellow",
    "Hongjiang Zhang": "IEEE Fellow",
    "Ya-Qin Zhang": "IEEE Fellow",
    "Wei Zhao": "IEEE Fellow",
    "Nan-Ning Zheng": "IEEE Fellow",
    "George Zobrist": "IEEE Life Fellow",
    "Albert Zomaya": "IEEE Fellow",
    "Yervant Zorian": "IEEE Fellow",
    "Steven Zucker": "IEEE Fellow",
    "Willy Zwaenepoel": "IEEE Fellow",
    "Maneesh Agrawala": "ACM Fellow",
    "Anima Anandkumar": "ACM Fellow",
    "David Atienza Alonso": "ACM Fellow",
    "Boaz Barak": "ACM Fellow",
    "Michel Beaudouin-Lafon": "ACM Fellow",
    "Peter Boncz": "ACM Fellow",
    "Luis H Ceze": "ACM Fellow",
    "Ranveer Chandra": "ACM Fellow",
    "Ed H. Chi": "ACM Fellow",
    "Corinna Cortes": "ACM Fellow",
    "Constantinos Daskalakis": "ACM Fellow",
    "Bronis R. de Supinski": "ACM Fellow",
    "Kalyanmoy Deb": "ACM Fellow",
    "Kevin Fu": "ACM Fellow",
    "Craig Gotsman": "ACM Fellow",
    "Ahmed E. Hassan": "ACM Fellow",
    "Sumi Helal": "ACM Fellow",
    "Manuel V Hermenegildo": "ACM Fellow",
    "Michael W Hicks": "ACM Fellow",
    "Jason Hong": "ACM Fellow",
    "Sandy Irani": "ACM Fellow",
    "Hiroshi Ishii": "ACM Fellow",
    "Alfons Kemper": "ACM Fellow",
    "Samir Khuller": "ACM Fellow",
    "Farinaz Koushanfar": "ACM Fellow",
    "Chung C Kuo": "ACM Fellow",
    "Hang Li": "ACM Fellow",
    "Jimmy Lin": "ACM Fellow",
    "Radu Marculescu": "ACM Fellow",
    "David M Mount": "ACM Fellow",
    "Gonzalo Navarro": "ACM Fellow",
    "Rafael Pass": "ACM Fellow",
    "Marc Pollefeys": "ACM Fellow, IEEE Fellow\uff0c\u9a6c\u5c14\u5956\u5f97\u4e3b\uff0c\u66fe\u4efbIJCV\u3001TPAMI\u7b49\u9876\u520a\u7f16\u59d4\uff0c\u5fae\u8f6fMR&AI\u82cf\u9ece\u4e16\u5b9e\u9a8c\u5ba4\u4e3b\u4efb",
    "Alex Pothen": "ACM Fellow",
    "Moinuddin Qureshi": "ACM Fellow",
    "Ashutosh Sabharwal": "ACM Fellow",
    "Tim Sherwood": "ACM Fellow",
    "Stefano Soatto": "ACM Fellow, ACM Fellow, IEEE Fellow,  Marr Prize, Associate Editor of IEEE TPAMI, a Member of the Editorial Board of IJCV",
    "John Stasko": "ACM Fellow",
    "Gary J. Sullivan": "ACM Fellow",
    "Jaime Teevan": "ACM Fellow",
    "Kentaro Toyama": "ACM Fellow",
    "Rene Vidal": "ACM Fellow",
    "Eric Xing": "ACM Fellow",
    "Dong Yu": "ACM Fellow",
    "Yizhou Yu": "ACM Fellow",
    "Haitao Zheng": "ACM Fellow",
    "Wenwu Zhu": "ACM Fellow",
    "Denis Zorin": "ACM Fellow",
    "Leonard M. Adleman": "ACM Fellow",
    "David A Bader": "ACM Fellow",
    "Meenakshi Balakrishnan": "ACM Fellow",
    "Mark Braverman": "ACM Fellow",
    "Linda Jean Camp": "ACM Fellow",
    "Edward Y Chang": "ACM Fellow",
    "Tanzeem Choudhury": "ACM Fellow",
    "Daniel Cohen-Or": "ACM Fellow, ACM Fellow\uff0c\u66fe\u4efbTOG\u3001TVCG\u7b49\u9876\u520a\u7f16\u59d4",
    "Gautam Das": "ACM Fellow",
    "Anind Dey": "ACM Fellow",
    "Lieven Eeckhout": "ACM Fellow",
    "Amos Fiat": "ACM Fellow",
    "Hubertus Franke": "ACM Fellow",
    "Batya Friedman": "ACM Fellow",
    "Judith Gal-Ezer": "ACM Fellow",
    "Deepak Ganesan": "ACM Fellow",
    "Anupam Gupta": "ACM Fellow",
    "Zygmunt J. Haas": "ACM Fellow",
    "Elad Hazan": "ACM Fellow",
    "Xiaobo Sharon Hu": "ACM Fellow",
    "Paola Inverardi": "ACM Fellow",
    "Zachary Ives": "ACM Fellow",
    "Sushil Jajodia": "ACM Fellow",
    "Ranjit Jhala": "ACM Fellow",
    "David R Kaeli": "ACM Fellow",
    "Jonathan Katz": "ACM Fellow",
    "Robert Kleinberg": "ACM Fellow",
    "Thomas Lengauer": "ACM Fellow",
    "Hai Li": "ACM Fellow",
    "Tie-Yan Liu": "ACM Fellow",
    "Steve Marschner": "ACM Fellow",
    "Matthew T Mason": "ACM Fellow",
    "Dale A Miller": "ACM Fellow",
    "Elchanan Mossel": "ACM Fellow",
    "Bernhard Nebel": "ACM Fellow",
    "Bjorner Nikolaj": "ACM Fellow",
    "Rafail Ostrovsky": "ACM Fellow",
    "Joel Ouaknine": "ACM Fellow",
    "David Z. Pan": "ACM Fellow",
    "Rosalind Wright Picard": "ACM Fellow",
    "Shaz Qadeer": "ACM Fellow",
    "Glenn Ricart": "ACM Fellow",
    "Tajana Rosing": "ACM Fellow",
    "Robert B Ross": "ACM Fellow",
    "Szymon Rusinkiewicz": "ACM Fellow",
    "Pierangela Samarati": "ACM Fellow",
    "Sunita Sarawagi": "ACM Fellow",
    "Bernt Schiele": "ACM Fellow",
    "Munindar P. Singh": "ACM Fellow",
    "Aravinda P Sistla": "ACM Fellow",
    "Scott Smolka": "ACM Fellow",
    "Mark Tehranipoor": "ACM Fellow",
    "Luca Trevisan": "ACM Fellow",
    "Wenping Wang": "ACM Fellow, IEEE/ACM Fellow",
    "Brent Waters": "ACM Fellow",
    "Ryen W White": "ACM Fellow",
    "Jacob Otto Wobbrock": "ACM Fellow",
    "Tao Xie": "ACM Fellow",
    "Ming-Hsuan Yang": "ACM Fellow",
    "Mohammed Zaki": "ACM Fellow",
    "Ben Y. Zhao": "ACM Fellow",
    "Lin Zhong": "ACM Fellow",
    "Shlomo Zilberstein": "ACM Fellow",
    "Daniel J Abadi": "ACM Fellow",
    "James Allan": "ACM Fellow",
    "Srinivas Aluru": "ACM Fellow",
    "Andrea Arpaci-Dusseau": "ACM Fellow",
    "Remzi Arpaci-Dusseau": "ACM Fellow",
    "Suman Banerjee": "ACM Fellow",
    "Manuel Blum": "ACM Fellow",
    "Lionel Briand": "ACM Fellow",
    "David Brooks": "ACM Fellow",
    "Ran Canetti": "ACM Fellow",
    "John Canny": "ACM Fellow",
    "Anantha Chandrakasan": "ACM Fellow",
    "Yao-Wen Chang": "ACM Fellow",
    "Moses Charikar": "ACM Fellow",
    "Yiran Chen": "ACM Fellow",
    "Graham R. Cormode": "ACM Fellow",
    "Patrick Cousot": "ACM Fellow",
    "Mathieu Desbrun": "ACM Fellow",
    "Whitfield Diffie": "ACM Fellow",
    "Bonnie J Dorr": "ACM Fellow",
    "Nicholas Duffield": "ACM Fellow",
    "Alan Edelman": "ACM Fellow",
    "Thomas Eiter": "ACM Fellow",
    "Cormac Flanagan": "ACM Fellow",
    "Jodi Forlizzi": "ACM Fellow",
    "Dieter Fox": "ACM Fellow, IEEE/AAAI,ACM Fellow, editor of the IEEE Transactions on Robots",
    "Sanjay Ghemawat": "ACM Fellow",
    "Antonio Gonzalez": "ACM Fellow",
    "Andrew D. Gordon": "ACM Fellow",
    "Steven Gribble": "ACM Fellow",
    "Susanne E Hambrusch": "ACM Fellow",
    "Martin Hellman": "ACM Fellow",
    "Nicholas Higham": "ACM Fellow",
    "C. Antony R. Hoare": "ACM Fellow",
    "Holger H. Hoos": "ACM Fellow",
    "Ihab F. Ilyas": "ACM Fellow",
    "Lizy Kurian John": "ACM Fellow",
    "Joost-Pieter Katoen": "ACM Fellow",
    "Nam Sung Kim": "ACM Fellow",
    "Sven Koenig": "ACM Fellow",
    "David Kotz": "ACM Fellow",
    "Arvind Krishnamurthy": "ACM Fellow",
    "Ravi Kumar": "ACM Fellow",
    "Brian Levine": "ACM Fellow",
    "Kevin Leyton-Brown": "ACM Fellow",
    "Xuelong LI": "ACM Fellow",
    "Steven H. Low": "ACM Fellow",
    "Chenyang Lu": "ACM Fellow",
    "Samuel Madden": "ACM Fellow",
    "David Maltz": "ACM Fellow",
    "Volker Markl": "ACM Fellow",
    "Maja Mataric": "ACM Fellow",
    "Filippo Menczer": "ACM Fellow",
    "Jose Meseguer": "ACM Fellow",
    "Meredith Ringel Morris": "ACM Fellow",
    "Nachiappan Nagappan": "ACM Fellow",
    "Radhika Nagpal": "ACM Fellow",
    "Moni Naor": "ACM Fellow",
    "Chandra Narayanaswami": "ACM Fellow",
    "Sam H. Noh": "ACM Fellow",
    "Prakash Panangaden": "ACM Fellow",
    "Manish Parashar": "ACM Fellow",
    "Keshab K. Parhi": "ACM Fellow",
    "Haesun Park": "ACM Fellow",
    "Gordon Plotkin": "ACM Fellow",
    "Michael O. Rabin": "ACM Fellow",
    "Kui Ren": "ACM Fellow",
    "Paul Resnick": "ACM Fellow",
    "Mary Beth Rosson": "ACM Fellow",
    "Steven Salzberg": "ACM Fellow",
    "Sanjit Arunkumar Seshia": "ACM Fellow",
    "Adi Shamir": "ACM Fellow",
    "Adam Smith": "ACM Fellow",
    "Olga Sorkine-Hornung": "ACM Fellow",
    "Rick Stevens": "ACM Fellow",
    "Peter Stone": "ACM Fellow",
    "Yufei Tao": "ACM Fellow",
    "Leandros Tassiulas": "ACM Fellow",
    "Kenneth Lane Thompson": "ACM Fellow",
    "Andrew Tomkins": "ACM Fellow",
    "Olga Troyanskaya": "ACM Fellow",
    "Matthew A Turk": "ACM Fellow",
    "Toby Walsh": "ACM Fellow",
    "Laurie Ann Williams": "ACM Fellow",
    "Cathy H Wu": "ACM Fellow",
    "Shuicheng Yan": "ACM Fellow, AAAI/ACM/SAEng/IEEE/IAPR Fellow",
    "Michael J Zyda": "ACM Fellow",
    "Scott J Aaronson": "ACM Fellow",
    "Saman Amarasinghe": "ACM Fellow",
    "Kavita Bala": "ACM Fellow",
    "Magdalena Balazinska": "ACM Fellow",
    "Paul Beame": "ACM Fellow",
    "Emery David Berger": "ACM Fellow",
    "Ronald F Boisvert": "ACM Fellow",
    "Bradley G Calder": "ACM Fellow",
    "Diego Calvanese": "ACM Fellow",
    "Claire Cardie": "ACM Fellow",
    "Timothy Chan": "ACM Fellow",
    "Kanianthra Mani Chandy": "ACM Fellow",
    "Xilin Chen": "ACM Fellow",
    "Elizabeth Frances Churchill": "ACM Fellow",
    "Philip R Cohen": "ACM Fellow",
    "Vincent Conitzer": "ACM Fellow",
    "Noshir Contractor": "ACM Fellow",
    "Matthew B Dwyer": "ACM Fellow",
    "Elena Ferrari": "ACM Fellow",
    "Michael J. Freedman": "ACM Fellow",
    "Deborah Frincke": "ACM Fellow",
    "Lise Getoor": "ACM Fellow",
    "Maria L Gini": "ACM Fellow",
    "Subbarao Kambhampati": "ACM Fellow",
    "Tamara G Kolda": "ACM Fellow",
    "Xiangyang Li": "ACM Fellow",
    "Songwu Lu": "ACM Fellow",
    "Wendy Elizabeth Mackay": "ACM Fellow",
    "Sheila McIlraith": "ACM Fellow",
    "Rada Mihalcea": "ACM Fellow",
    "Robin R Murphy": "ACM Fellow",
    "Marc Najork": "ACM Fellow",
    "Jason Nieh": "ACM Fellow",
    "Timothy Pinkston": "ACM Fellow",
    "Mihai Pop": "ACM Fellow",
    "Andreas Reuter": "ACM Fellow",
    "Jeffrey S Rosenschein": "ACM Fellow",
    "Srinivasan Seshan": "ACM Fellow",
    "Prashant J Shenoy": "ACM Fellow",
    "Peter W Shor": "ACM Fellow",
    "Mona Singh": "ACM Fellow",
    "Ramesh Kumar Sitaraman": "ACM Fellow",
    "Dawn Song": "ACM Fellow",
    "Salvatore J Stolfo": "ACM Fellow",
    "Dacheng Tao": "ACM Fellow, IEEE Fellow, \u6fb3\u5927\u5229\u4e9a\u79d1\u5b66\u9662\u9662\u58eb\u3001\u65b0\u5357\u5a01\u5c14\u58eb\u7687\u5bb6\u5b66\n\u9662\u9662\u58eb\u3001\u6b27\u6d32\u79d1\u5b66\u9662\u9662\u58eb, ACM Fellow, AAAS Fellow",
    "Moshe Tennenholtz": "ACM Fellow",
    "Giovanni Vigna": "ACM Fellow",
    "Nisheeth Vishnoi": "ACM Fellow",
    "Darrell Whitley": "ACM Fellow",
    "Moustafa A Youssef": "ACM Fellow",
    "Carlo A Zaniolo": "ACM Fellow",
    "Lidong Zhou": "ACM Fellow",
    "Krste Asanovic": "ACM Fellow",
    "N Asokan": "ACM Fellow",
    "Paul Barham": "ACM Fellow",
    "Peter L Bartlett": "ACM Fellow",
    "Elizabeth Belding": "ACM Fellow",
    "Rastislav Bodik": "ACM Fellow",
    "Katy Borner": "ACM Fellow",
    "Amy S Bruckman": "ACM Fellow",
    "Jan Camenisch": "ACM Fellow",
    "Adnan Darwiche": "ACM Fellow",
    "Andre M Dehon": "ACM Fellow",
    "Premkumar T Devanbu": "ACM Fellow",
    "Tamal K Dey": "ACM Fellow",
    "Sandhya Dwarkadas": "ACM Fellow",
    "Steven Feiner": "ACM Fellow",
    "Tim Finin": "ACM Fellow",
    "Thomas Funkhouser": "ACM Fellow, ACM Fellow\uff0cFellow of Alfred P. Sloan Foundation",
    "Minos Garofalakis": "ACM Fellow",
    "Mohammad T. Hajiaghayi": "ACM Fellow",
    "Tian He": "ACM Fellow",
    "Wendi Beth Heinzelman": "ACM Fellow",
    "Aaron Hertzmann": "ACM Fellow",
    "Jessica K Hodgins": "ACM Fellow",
    "John Hughes": "ACM Fellow",
    "Johan H\u00e5stad": "ACM Fellow",
    "Charles Lee Isbell": "ACM Fellow",
    "Sanjeev Khanna": "ACM Fellow",
    "Lillian Lee": "ACM Fellow",
    "Tom Leighton": "ACM Fellow",
    "Fei-Fei Li": "ACM Fellow",
    "Michael Littman": "ACM Fellow",
    "Huan Liu": "ACM Fellow",
    "Jiebo Luo": "ACM Fellow",
    "Bruce M Maggs": "ACM Fellow",
    "Bangalore S Manjunath": "ACM Fellow",
    "Vishal Misra": "ACM Fellow",
    "Frank Mueller": "ACM Fellow",
    "David Parkes": "ACM Fellow",
    "Gurudatta Parulkar": "ACM Fellow",
    "Toniann Pitassi": "ACM Fellow",
    "Lili Qiu": "ACM Fellow",
    "Matthew Roughan": "ACM Fellow",
    "Amit Sahai": "ACM Fellow",
    "Alex C. Snoeren": "ACM Fellow",
    "Gerald Tesauro": "ACM Fellow",
    "Bhavani Thuraisingham": "ACM Fellow",
    "Salil Vadhan": "ACM Fellow",
    "Ellen M Voorhees": "ACM Fellow",
    "Avi Wigderson": "ACM Fellow",
    "Alec Wolman": "ACM Fellow",
    "Lars Birkedal": "ACM Fellow",
    "Edouard Bugnion": "ACM Fellow",
    "Margaret Burnett": "ACM Fellow",
    "Edith Cohen": "ACM Fellow",
    "Dorin Comaniciu": "ACM Fellow",
    "Susan Dray": "ACM Fellow",
    "Edward Alan Fox": "ACM Fellow",
    "Richard M Fujimoto": "ACM Fellow",
    "Shafi Goldwasser": "ACM Fellow",
    "Carla Gomes": "ACM Fellow",
    "Martin Grohe": "ACM Fellow",
    "Aarti Gupta": "ACM Fellow",
    "Venkatesan Guruswami": "ACM Fellow",
    "Steven Michael Hand": "ACM Fellow",
    "Mor Harchol-Balter": "ACM Fellow",
    "Laxmikant Kale": "ACM Fellow",
    "Michael Kass": "ACM Fellow",
    "Angelos Dennis Keromytis": "ACM Fellow",
    "Edward Knightly": "ACM Fellow",
    "Li Erran Li": "ACM Fellow",
    "Gabriel H Loh": "ACM Fellow",
    "Tomas Lozano-Perez": "ACM Fellow",
    "Clifford A Lynch": "ACM Fellow",
    "Yi Ma": "ACM Fellow, associate editor TPAMI, IJCV, IEEE, SIAM; IEEE Fellow, ACM Fellow, SIAM Fellow; ",
    "Andrew K. Mccallum": "ACM Fellow",
    "Silvio Micali": "ACM Fellow",
    "Gail C Murphy": "ACM Fellow",
    "Onur Mutlu": "ACM Fellow",
    "Nuria Oliver": "ACM Fellow",
    "Balaji Prabhakar": "ACM Fellow",
    "Tal Rabin": "ACM Fellow",
    "K. K. Ramakrishnan": "ACM Fellow",
    "Ravi Ramamoorthi": "ACM Fellow, ACM Fellow\uff0cIEEE Fellow\uff0cSIGGRAPH Academy",
    "Yvonne Rogers": "ACM Fellow",
    "Yong Rui": "ACM Fellow",
    "Bernhard Schoelkopf": "ACM Fellow",
    "Steve Seitz": "ACM Fellow",
    "Michael Sipser": "ACM Fellow",
    "Anand Sivasubramaniam": "ACM Fellow",
    "Mani B. Srivastava": "ACM Fellow",
    "Alexander Vardy": "ACM Fellow",
    "Geoffrey Voelker": "ACM Fellow",
    "Qiang Yang": "ACM Fellow",
    "Chengxiang Zhai": "ACM Fellow",
    "Aidong Zhang": "ACM Fellow",
    "Noga Alon": "ACM Fellow",
    "Paul Barford": "ACM Fellow",
    "Luca Benini": "ACM Fellow",
    "Stephen Blackburn": "ACM Fellow",
    "Dan Boneh": "ACM Fellow",
    "Carla Brodley": "ACM Fellow",
    "Justine Cassell": "ACM Fellow",
    "Erik Demaine": "ACM Fellow",
    "Allison Druin": "ACM Fellow",
    "Fredo Durand": "ACM Fellow",
    "Nick Feamster": "ACM Fellow",
    "Jason Flinn": "ACM Fellow",
    "William Freeman": "ACM Fellow",
    "Robert L. Grossman": "ACM Fellow",
    "James Hendler": "ACM Fellow",
    "Monika Henzinger": "ACM Fellow",
    "Anthony Hey": "ACM Fellow",
    "Xuedong Huang": "ACM Fellow",
    "Daniel Jackson": "ACM Fellow",
    "Robert J.K. Jacob": "ACM Fellow",
    "Somesh Jha": "ACM Fellow",
    "Ravi Kannan": "ACM Fellow",
    "Anne-Marie Kermarrec": "ACM Fellow",
    "Martin Kersten": "ACM Fellow",
    "Christos Kozyrakis": "ACM Fellow",
    "Marta Kwiatkowska": "ACM Fellow",
    "James Landay": "ACM Fellow",
    "K. Rustan M. Leino": "ACM Fellow",
    "Joseph Bryan Lyles": "ACM Fellow",
    "Todd C Mowry": "ACM Fellow",
    "Sharon Oviatt": "ACM Fellow",
    "Venkata Padmanabhan": "ACM Fellow",
    "Shwetak N Patel": "ACM Fellow",
    "David Peleg": "ACM Fellow",
    "Radia Perlman": "ACM Fellow",
    "Ganesan Ramalingam": "ACM Fellow",
    "Holly E Rushmeier": "ACM Fellow",
    "Michael E Saks": "ACM Fellow",
    "Sachin S. Sapatnekar": "ACM Fellow",
    "Abigail Sellen": "ACM Fellow",
    "Sudipta Sengupta": "ACM Fellow",
    "Andre Seznec": "ACM Fellow",
    "Valerie Taylor": "ACM Fellow",
    "Carlo Tomasi": "ACM Fellow",
    "Paul Van Oorschot": "ACM Fellow",
    "Manuela Veloso": "ACM Fellow",
    "Zhi-Hua Zhou": "ACM Fellow",
    "Nancy M Amato": "ACM Fellow",
    "David M. Blei": "ACM Fellow",
    "Naehyuck Chang": "ACM Fellow",
    "Hsinchun Chen": "ACM Fellow",
    "Mary P. Czerwinski": "ACM Fellow",
    "Giuseppe De Giacomo": "ACM Fellow",
    "Paul Dourish": "ACM Fellow",
    "Cynthia Dwork": "ACM Fellow",
    "Kevin Fall": "ACM Fellow",
    "Babak Falsafi": "ACM Fellow",
    "Michael Franz": "ACM Fellow",
    "Orna Grumberg": "ACM Fellow",
    "Ramanathan Guha": "ACM Fellow",
    "Jayant R Haritsa": "ACM Fellow",
    "Julia Hirschberg": "ACM Fellow",
    "Piotr Indyk": "ACM Fellow",
    "Tei-Wei Kuo": "ACM Fellow",
    "Xavier Leroy": "ACM Fellow",
    "Chih-Jen Lin": "ACM Fellow",
    "Bing Liu": "ACM Fellow",
    "Michael George Luby": "ACM Fellow",
    "Ueli M Maurer": "ACM Fellow",
    "Victor Miller": "ACM Fellow",
    "Elizabeth D. Mynatt": "ACM Fellow",
    "Judea Pearl": "ACM Fellow",
    "Jian Pei": "ACM Fellow",
    "Frank Pfenning": "ACM Fellow",
    "Dragomir R Radev": "ACM Fellow",
    "Sriram Rajamani": "ACM Fellow",
    "Pablo Rodriguez": "ACM Fellow",
    "Shmuel Sagiv": "ACM Fellow",
    "Peter Schroeder": "ACM Fellow",
    "Assaf Schuster": "ACM Fellow",
    "Kevin Skadron": "ACM Fellow",
    "Wang-Chiew Tan": "ACM Fellow",
    "Santosh Vempala": "ACM Fellow",
    "Tandy Warnow": "ACM Fellow",
    "Michael Wooldridge": "ACM Fellow",
    "Samson Abramsky": "ACM Fellow",
    "Vikram Adve": "ACM Fellow",
    "Foto Afrati": "ACM Fellow",
    "Charles W Bachman": "ACM Fellow",
    "Allan Borodin": "ACM Fellow",
    "Alan Bundy": "ACM Fellow",
    "Lorrie Faith Cranor": "ACM Fellow",
    "Timothy Alden Davis": "ACM Fellow",
    "Inderjit Dhillon": "ACM Fellow",
    "Nikil D. Dutt": "ACM Fellow",
    "Faith Ellen": "ACM Fellow",
    "Michael D Ernst": "ACM Fellow",
    "Adam Finkelstein": "ACM Fellow",
    "Juliana Freire": "ACM Fellow",
    "Johannes Gehrke": "ACM Fellow",
    "Eric Grimson": "ACM Fellow",
    "Mark Guzdial": "ACM Fellow",
    "Gernot Heiser": "ACM Fellow",
    "Eric Horvitz": "ACM Fellow",
    "Thorsten Joachims": "ACM Fellow",
    "Michael Kearns": "ACM Fellow",
    "Valerie King": "ACM Fellow",
    "Sarit Kraus": "ACM Fellow",
    "Leslie Lamport": "ACM Fellow",
    "Sharad Malik": "ACM Fellow",
    "Yishay Mansour": "ACM Fellow",
    "Subhasish Mitra": "ACM Fellow",
    "Michael Mitzenmacher": "ACM Fellow",
    "Robert Morris": "ACM Fellow",
    "Vijaykrishnan Narayanan": "ACM Fellow",
    "Shamkant Navathe": "ACM Fellow",
    "Jignesh M Patel": "ACM Fellow",
    "Parthasarathy Ranganathan": "ACM Fellow",
    "Omer Reingold": "ACM Fellow",
    "Tom Rodden": "ACM Fellow",
    "Ronitt Rubinfeld": "ACM Fellow",
    "Daniela Rus": "ACM Fellow",
    "Alberto Luigi Sangiovanni Vincentelli": "ACM Fellow",
    "Henning Schulzrinne": "ACM Fellow",
    "Stuart Shieber": "ACM Fellow",
    "Ramakrishnan Srikant": "ACM Fellow",
    "Aravind Srinivasan": "ACM Fellow",
    "S. Sudarshan": "ACM Fellow",
    "Paul Syverson": "ACM Fellow",
    "Gene Tsudik": "ACM Fellow",
    "Stephen J Whittaker": "ACM Fellow",
    "Mark S Ackerman": "ACM Fellow",
    "Charu Chandra Aggarwal": "ACM Fellow",
    "James H Anderson": "ACM Fellow",
    "Mihir Bellare": "ACM Fellow",
    "Christine L Borgman": "ACM Fellow",
    "Stefano Ceri": "ACM Fellow",
    "Ingemar J. Cox": "ACM Fellow",
    "Carlos J P De Lucena": "ACM Fellow",
    "Rina Dechter": "ACM Fellow",
    "Chip Elliott": "ACM Fellow",
    "David Forsyth": "ACM Fellow",
    "Wen Gao": "ACM Fellow, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "David Garlan": "ACM Fellow",
    "James Gosling": "ACM Fellow",
    "Peter Haas": "ACM Fellow",
    "Marti Hearst": "ACM Fellow",
    "Matthias Jarke": "ACM Fellow",
    "Sampath Kumar Kannan": "ACM Fellow",
    "David J Kasik": "ACM Fellow",
    "Dina Katabi": "ACM Fellow",
    "Henry A Kautz": "ACM Fellow",
    "Jon Kleinberg": "ACM Fellow",
    "Panganamala Kumar": "ACM Fellow",
    "Douglas Lea": "ACM Fellow",
    "Yoelle Maarek": "ACM Fellow",
    "Christopher Manning": "ACM Fellow",
    "Madhav Marathe": "ACM Fellow",
    "John Mellor-Crummey": "ACM Fellow",
    "Greg Morrisett": "ACM Fellow",
    "Andrew Myers": "ACM Fellow",
    "Dana Nau": "ACM Fellow",
    "Satish Rao": "ACM Fellow",
    "S E Robertson": "ACM Fellow",
    "Timothy Roscoe": "ACM Fellow",
    "Timoleon Sellis": "ACM Fellow",
    "Dennis E Shasha": "ACM Fellow",
    "Nir N Shavit": "ACM Fellow",
    "Kyuseok Shim": "ACM Fellow",
    "Padhraic Smyth": "ACM Fellow",
    "Milind Tambe": "ACM Fellow",
    "Val Tannen": "ACM Fellow",
    "David Williamson": "ACM Fellow",
    "Limsoon Wong": "ACM Fellow",
    "Ellen Zegura": "ACM Fellow",
    "David Zuckerman": "ACM Fellow",
    "Gustavo Alonso": "ACM Fellow",
    "Lars Arge": "ACM Fellow",
    "Pierre Baldi": "ACM Fellow",
    "Hans Boehm": "ACM Fellow",
    "Craig Boutilier": "ACM Fellow",
    "Tracy Camp": "ACM Fellow",
    "Rick Cattell": "ACM Fellow",
    "Larry S Davis": "ACM Fellow",
    "Ahmed Elmagarmid": "ACM Fellow",
    "Wenfei Fan": "ACM Fellow",
    "Lixin Gao": "ACM Fellow",
    "Simson L Garfinkel": "ACM Fellow",
    "Garth A Gibson": "ACM Fellow",
    "Saul Greenberg": "ACM Fellow",
    "Markus Gross": "ACM Fellow",
    "David Paul Grove": "ACM Fellow",
    "Jonathan Grudin": "ACM Fellow",
    "Rachid Guerraoui": "ACM Fellow",
    "Manish Gupta": "ACM Fellow",
    "John Hershberger": "ACM Fellow",
    "Andrew Kahng": "ACM Fellow",
    "Anna Karlin": "ACM Fellow",
    "Srinivasan Keshav": "ACM Fellow",
    "Gregor Kiczales": "ACM Fellow",
    "Masaru Kitsuregawa": "ACM Fellow",
    "Leonid Libkin": "ACM Fellow",
    "Tova Milo": "ACM Fellow",
    "Joseph O'Rourke": "ACM Fellow",
    "Benjamin Pierce": "ACM Fellow",
    "Keshav K Pingali": "ACM Fellow",
    "Andrew M Pitts": "ACM Fellow",
    "Rajeev  Ramnarain Rastogi": "ACM Fellow",
    "Raj Reddy": "ACM Fellow",
    "Keith Ross": "ACM Fellow",
    "Robert Schreiber": "ACM Fellow",
    "Steven Scott": "ACM Fellow",
    "Bart Selman": "ACM Fellow",
    "Ron Shamir": "ACM Fellow",
    "Yoav Shoham": "ACM Fellow",
    "Joseph Sifakis": "ACM Fellow",
    "Alistair Sinclair": "ACM Fellow",
    "Clifford Stein": "ACM Fellow",
    "Ion Stoica": "ACM Fellow",
    "Roberto Tamassia": "ACM Fellow",
    "Walter F. Tichy": "ACM Fellow",
    "Patrick Valduriez": "ACM Fellow",
    "Leslie G Valiant": "ACM Fellow",
    "Katherine Yelick": "ACM Fellow",
    "Ramin Zabih": "ACM Fellow",
    "Xiaodong Zhang": "ACM Fellow",
    "Serge Abiteboul": "ACM Fellow",
    "Divyakant Agrawal": "ACM Fellow",
    "Ronald M Baecker": "ACM Fellow",
    "Thomas Ball": "ACM Fellow",
    "Guy Blelloch": "ACM Fellow",
    "Carl Ebeling": "ACM Fellow",
    "David Eppstein": "ACM Fellow",
    "Geoffrey Fox": "ACM Fellow",
    "George Furnas": "ACM Fellow",
    "David K Gifford": "ACM Fellow",
    "Ramesh Govindan": "ACM Fellow",
    "Baining Guo": "ACM Fellow, IEEE/ACM Fellow, Canadian Academy of  engineering, Editor of Computer Graphics",
    "David Heckerman": "ACM Fellow",
    "Gerard J. Holzmann": "ACM Fellow",
    "Hugues Hoppe": "ACM Fellow",
    "Christian S. Jensen": "ACM Fellow",
    "Howard J Karloff": "ACM Fellow",
    "Stephen Keckler": "ACM Fellow",
    "Peter B. Key": "ACM Fellow",
    "Robert E Kraut": "ACM Fellow",
    "Susan Landau": "ACM Fellow",
    "Ming C Lin": "ACM Fellow",
    "Peter Magnusson": "ACM Fellow",
    "Dahlia Malkhi": "ACM Fellow",
    "Keith Marzullo": "ACM Fellow",
    "SATOSHI MATSUOKA": "ACM Fellow",
    "Nelson Max": "ACM Fellow",
    "Joseph Mitchell": "ACM Fellow",
    "Shubu Mukherjee": "ACM Fellow",
    "Beng Chin Ooi": "ACM Fellow",
    "Zehra Ozsoyoglu": "ACM Fellow",
    "Janos Pach": "ACM Fellow",
    "Linda Petzold": "ACM Fellow",
    "Martha Pollack": "ACM Fellow",
    "Dan Roth": "ACM Fellow",
    "John Sanguinetti": "ACM Fellow",
    "Margo Seltzer": "ACM Fellow",
    "Amit Singhal": "ACM Fellow",
    "Diane L Souvaine": "ACM Fellow",
    "Divesh Srivastava": "ACM Fellow",
    "Dan Suciu": "ACM Fellow",
    "Dean Tullsen": "ACM Fellow",
    "Amin Vahdat": "ACM Fellow",
    "David Wetherall": "ACM Fellow",
    "Frank Kenneth Zadeck": "ACM Fellow",
    "David A Abramson": "ACM Fellow",
    "Sarita Adve": "ACM Fellow",
    "Lorenzo Alvisi": "ACM Fellow",
    "Luiz Andre Barroso": "ACM Fellow",
    "Douglas C Burger": "ACM Fellow",
    "Jennifer Chayes": "ACM Fellow",
    "Peter Chen": "ACM Fellow",
    "Anne Condon": "ACM Fellow",
    "Mark Crovella": "ACM Fellow",
    "Ron Cytron": "ACM Fellow",
    "Michael D Dahlin": "ACM Fellow",
    "Amr El Abbadi": "ACM Fellow",
    "Carla S. Ellis": "ACM Fellow",
    "Christos Faloutsos": "ACM Fellow",
    "Kathleen S Fisher": "ACM Fellow",
    "Wendy Hall": "ACM Fellow",
    "Jean-Pierre Hubaux": "ACM Fellow",
    "Michael  I. Jordan": "ACM Fellow",
    "Lydia Kavraki": "ACM Fellow",
    "Sara Kiesler": "ACM Fellow",
    "Philip N Klein": "ACM Fellow",
    "Donald Kossmann": "ACM Fellow",
    "John Launchbury": "ACM Fellow",
    "Richard F Lyon": "ACM Fellow",
    "Raymond Mooney": "ACM Fellow",
    "S. Muthukrishnan": "ACM Fellow",
    "Fernando Pereira": "ACM Fellow",
    "Pavel Pevzner": "ACM Fellow",
    "Dieter Rombach": "ACM Fellow",
    "David S. Rosenblum": "ACM Fellow",
    "Stefan Savage": "ACM Fellow",
    "Robert B Schnabel": "ACM Fellow",
    "Daniel A Spielman": "ACM Fellow",
    "Subhash Suri": "ACM Fellow",
    "Frank Wm Tompa": "ACM Fellow",
    "Stephen Trimberger": "ACM Fellow",
    "David M Ungar": "ACM Fellow",
    "Andreas Zeller": "ACM Fellow",
    "Shumin Zhai": "ACM Fellow",
    "Hagit Attiya": "ACM Fellow",
    "David F Bacon": "ACM Fellow",
    "Ricardo A Baeza-Yates": "ACM Fellow",
    "Chandrajit L. Bajaj": "ACM Fellow",
    "Vijay P Bhatkar": "ACM Fellow",
    "Jose A Blakeley": "ACM Fellow",
    "Gaetano Borriello": "ACM Fellow",
    "Nell B. Dale": "ACM Fellow",
    "Bruce Davie": "ACM Fellow",
    "Jeffrey A Dean": "ACM Fellow",
    "Thomas L Dean": "ACM Fellow",
    "Bruce R. Donald": "ACM Fellow",
    "Thomas D Erickson": "ACM Fellow",
    "Paulo Esteves-Verissimo": "ACM Fellow",
    "Gerhard Fischer": "ACM Fellow",
    "Ian T Foster": "ACM Fellow",
    "Andrew V Goldberg": "ACM Fellow",
    "Michael T Goodrich": "ACM Fellow",
    "Joseph M Hellerstein": "ACM Fellow",
    "Laurie J Hendren": "ACM Fellow",
    "Urs Hoelzle": "ACM Fellow",
    "Farnam Jahanian": "ACM Fellow",
    "Erich L Kaltofen": "ACM Fellow",
    "David Karger": "ACM Fellow",
    "Arie E Kaufman": "ACM Fellow",
    "Hans-Peter Kriegel": "ACM Fellow",
    "Maurizio Lenzerini": "ACM Fellow",
    "John Chi-Shing Lui": "ACM Fellow",
    "Dinesh Manocha": "ACM Fellow",
    "Margaret Martonosi": "ACM Fellow",
    "Yossi Matias": "ACM Fellow",
    "RJ Miller": "ACM Fellow",
    "John T Riedl": "ACM Fellow",
    "Martin Rinard": "ACM Fellow",
    "Patricia G. Selinger": "ACM Fellow",
    "Rudrapatna K Shyamasundar": "ACM Fellow",
    "Shang-Hua Teng": "ACM Fellow",
    "Chandramohan A Thekkath": "ACM Fellow",
    "Robbert Van Renesse": "ACM Fellow",
    "Baba C Vemuri": "ACM Fellow",
    "Kyu-Young Whang": "ACM Fellow",
    "Yorick Wilks": "ACM Fellow",
    "Terry Winograd": "ACM Fellow",
    "Martin Abadi": "ACM Fellow",
    "Gregory D. Abowd": "ACM Fellow",
    "Alex Aiken": "ACM Fellow",
    "Sanjeev Arora": "ACM Fellow",
    "Hari Balakrishnan": "ACM Fellow",
    "William A.S. Buxton": "ACM Fellow",
    "Kenneth Clarkson": "ACM Fellow",
    "Jason Cong": "ACM Fellow",
    "Perry Cook": "ACM Fellow",
    "Stephen A Cook": "ACM Fellow",
    "Jack Davidson": "ACM Fellow",
    "Umeshwar Dayal": "ACM Fellow",
    "Xiaotie Deng": "ACM Fellow",
    "J.J. Garcia-Luna-Aceves": "ACM Fellow",
    "Michel X Goemans": "ACM Fellow",
    "Pat Hanrahan": "ACM Fellow",
    "Charles H House": "ACM Fellow",
    "Watts S Humphrey": "ACM Fellow",
    "Alan Kay": "ACM Fellow",
    "Joseph A Konstan": "ACM Fellow",
    "Roy Levin": "ACM Fellow",
    "Paul G Lowney": "ACM Fellow",
    "Kathryn S McKinley": "ACM Fellow",
    "Bertrand Meyer": "ACM Fellow",
    "John C. Mitchell": "ACM Fellow",
    "Ian Munro": "ACM Fellow",
    "Judith S Olson": "ACM Fellow",
    "Lawrence C Paulson": "ACM Fellow",
    "Hamid Pirahesh": "ACM Fellow",
    "Brian Randell": "ACM Fellow",
    "Michael Reiter": "ACM Fellow",
    "Jennifer Rexford": "ACM Fellow",
    "Jonathan Rose": "ACM Fellow",
    "Mendel Rosenblum": "ACM Fellow",
    "Rob A Rutenbar": "ACM Fellow",
    "Tuomas Sandholm": "ACM Fellow",
    "Vivek Sarkar": "ACM Fellow",
    "Per O Stenstrom": "ACM Fellow",
    "Madhu Sudan": "ACM Fellow",
    "Douglas B Terry": "ACM Fellow",
    "Anant Agarwal": "ACM Fellow",
    "Utpal Banerjee": "ACM Fellow",
    "Catriel Beeri": "ACM Fellow",
    "Avrim Blum": "ACM Fellow",
    "Eric A. Brewer": "ACM Fellow",
    "Andrei Broder": "ACM Fellow",
    "Michael F. Cohen": "ACM Fellow",
    "Larry Constantine": "ACM Fellow",
    "Danny Dolev": "ACM Fellow",
    "Rodney Downey": "ACM Fellow",
    "Edward A Feigenbaum": "ACM Fellow",
    "Edward Felten": "ACM Fellow",
    "Lance Fortnow": "ACM Fellow",
    "Georg Gottlob": "ACM Fellow",
    "Richard Hull": "ACM Fellow",
    "Daniel Huttenlocher": "ACM Fellow",
    "Tao Jiang": "ACM Fellow",
    "John C Klensin": "ACM Fellow",
    "Monica Lam": "ACM Fellow",
    "Marc Levoy": "ACM Fellow",
    "Bud Mishra": "ACM Fellow",
    "Eliot Moss": "ACM Fellow",
    "Rajeev Motwani": "ACM Fellow",
    "Martin Odersky": "ACM Fellow",
    "Gary M Olson": "ACM Fellow",
    "Randy Pausch": "ACM Fellow",
    "Amir Pnueli": "ACM Fellow",
    "Eric S Roberts": "ACM Fellow",
    "Donald E Thomas": "ACM Fellow",
    "Philip Wadler": "ACM Fellow",
    "Mitchell Wand": "ACM Fellow",
    "HongJiang Zhang": "ACM Fellow",
    "Eric Allender": "ACM Fellow",
    "Arvind Arvind": "ACM Fellow",
    "Mikhail Atallah": "ACM Fellow",
    "Ming-Syan Chen": "ACM Fellow",
    "Susan T Dumais": "ACM Fellow",
    "Usama M Fayyad": "ACM Fellow",
    "Matthias Felleisen": "ACM Fellow",
    "Kenneth Forbus": "ACM Fellow",
    "Phillip B Gibbons": "ACM Fellow",
    "Lee Giles": "ACM Fellow",
    "Albert G Greenberg": "ACM Fellow",
    "William D Gropp": "ACM Fellow",
    "Roch Guerin": "ACM Fellow",
    "John Guttag": "ACM Fellow",
    "Laura M Haas": "ACM Fellow",
    "Alon Yitzchak Halevy": "ACM Fellow",
    "Anthony C Hearn": "ACM Fellow",
    "Thomas A Henzinger": "ACM Fellow",
    "John E. Laird": "ACM Fellow",
    "James Larus": "ACM Fellow",
    "Charles E Leiserson": "ACM Fellow",
    "Ming Li": "ACM Fellow",
    "Nick McKeown": "ACM Fellow",
    "J Strother Moore": "ACM Fellow",
    "Alan Newell": "ACM Fellow",
    "Peter Norvig": "ACM Fellow",
    "Dianne Prost OLeary": "ACM Fellow",
    "Dan R Olsen": "ACM Fellow",
    "Oyekunle Olukotun": "ACM Fellow",
    "M. Tamer Ozsu": "ACM Fellow",
    "Vern Paxson": "ACM Fellow",
    "Michael Scott": "ACM Fellow",
    "Alfred Z Spector": "ACM Fellow",
    "Victor Vianu": "ACM Fellow",
    "Marianne Winslett": "ACM Fellow",
    "Alexander L Wolf": "ACM Fellow",
    "Bryant W York": "ACM Fellow",
    "Stanley Zdonik": "ACM Fellow",
    "Lixia Zhang": "ACM Fellow",
    "Thomas Anderson": "ACM Fellow",
    "Dines Bjorner": "ACM Fellow",
    "Stephen Bourne": "ACM Fellow",
    "Rodney A Brooks": "ACM Fellow",
    "Surajit Chaudhuri": "ACM Fellow",
    "Keith D Cooper": "ACM Fellow",
    "David Dill": "ACM Fellow",
    "Christophe Diot": "ACM Fellow",
    "Michael J Franklin": "ACM Fellow",
    "Robert Harper": "ACM Fellow",
    "Maurice Herlihy": "ACM Fellow",
    "Phokion Kolaitis": "ACM Fellow",
    "T V Lakshman": "ACM Fellow",
    "Brad A Myers": "ACM Fellow",
    "David M Nicol": "ACM Fellow",
    "Krishna Palem": "ACM Fellow",
    "Thomas Reps": "ACM Fellow",
    "Mikkel Thorup": "ACM Fellow",
    "Eli Upfal": "ACM Fellow",
    "Umesh Vazirani": "ACM Fellow",
    "Vijay V Vazirani": "ACM Fellow",
    "Gerhard Weikum": "ACM Fellow",
    "Daniel Weld": "ACM Fellow",
    "Michael Wellman": "ACM Fellow",
    "Jennifer Widom": "ACM Fellow",
    "Walter Willinger": "ACM Fellow",
    "David A Wood": "ACM Fellow",
    "Hui Zhang": "ACM Fellow",
    "Janis A Bubenko": "ACM Fellow",
    "Luca Cardelli": "ACM Fellow",
    "Andrew A Chien": "ACM Fellow",
    "George E Collins": "ACM Fellow",
    "Allan Gottlieb": "ACM Fellow",
    "Vicki Hanson": "ACM Fellow",
    "Mark D. Hill": "ACM Fellow",
    "Yannis Ioannidis": "ACM Fellow",
    "Frans Kaashoek": "ACM Fellow",
    "Per-Ake Larson": "ACM Fellow",
    "Peter Lee": "ACM Fellow",
    "Paul Mockapetris": "ACM Fellow",
    "Simon L Peyton-Jones": "ACM Fellow",
    "Richard Schantz": "ACM Fellow",
    "Michael D Schroeder": "ACM Fellow",
    "Stamatis Vassiliadis": "ACM Fellow",
    "Benjamin W. Wah": "ACM Fellow",
    "David S Wise": "ACM Fellow",
    "Rakesh Agrawal": "ACM Fellow",
    "Victor Bahl": "ACM Fellow",
    "Bonnie Berger": "ACM Fellow",
    "John Carroll": "ACM Fellow",
    "Richard Demillo": "ACM Fellow",
    "Barbara J Grosz": "ACM Fellow",
    "Brent T Hailpern": "ACM Fellow",
    "Jiawei Han": "ACM Fellow",
    "Mary Harrold": "ACM Fellow",
    "Mark A Horowitz": "ACM Fellow",
    "Paul Hudak": "ACM Fellow",
    "H V Jagadish": "ACM Fellow",
    "Anil K Jain": "ACM Fellow",
    "Ramesh C Jain": "ACM Fellow",
    "Dexter Kozen": "ACM Fellow",
    "Yi-Bing Lin": "ACM Fellow",
    "Kathleen McKeown": "ACM Fellow",
    "Thomas Moran": "ACM Fellow",
    "Eugene Myers": "ACM Fellow",
    "Craig Partridge": "ACM Fellow",
    "Daniel A Reed": "ACM Fellow",
    "Stuart Russell": "ACM Fellow",
    "Scott J Shenker": "ACM Fellow",
    "Gurindar S Sohi": "ACM Fellow",
    "C J Van Rijsbergen": "ACM Fellow",
    "Pankaj Agarwal": "ACM Fellow",
    "Vishwani D Agrawal": "ACM Fellow",
    "Ozalp Babaoglu": "ACM Fellow",
    "Jon Crowcroft": "ACM Fellow",
    "Thomas G Dietterich": "ACM Fellow",
    "Susan Eggers": "ACM Fellow",
    "Harold N Gabow": "ACM Fellow",
    "Adolfo Guzman": "ACM Fellow",
    "Joseph Halpern": "ACM Fellow",
    "Neil Immerman": "ACM Fellow",
    "Sidney Karin": "ACM Fellow",
    "Wendy A Kellogg": "ACM Fellow",
    "David B Lomet": "ACM Fellow",
    "Gary L Miller": "ACM Fellow",
    "C. Mohan": "ACM Fellow",
    "Jeffrey F Naughton": "ACM Fellow",
    "Bantwal R Rau": "ACM Fellow",
    "David H Salesin": "ACM Fellow",
    "Mahadev Satyanarayanan": "ACM Fellow",
    "George Varghese": "ACM Fellow",
    "John Wilkes": "ACM Fellow",
    "Robert Aiken": "ACM Fellow",
    "Tetsuo Asano": "ACM Fellow",
    "Philip A Bernstein": "ACM Fellow",
    "Joel S Birnbaum": "ACM Fellow",
    "Alan H Borning": "ACM Fellow",
    "Yuri Breitbart": "ACM Fellow",
    "Jin-Yi Cai": "ACM Fellow",
    "David D Clark": "ACM Fellow",
    "Susan B Davidson": "ACM Fellow",
    "Johan DeKleer": "ACM Fellow",
    "Giovanni DeMicheli": "ACM Fellow",
    "David J. Farber": "ACM Fellow",
    "Joan Feigenbaum": "ACM Fellow",
    "Sally J Floyd": "ACM Fellow",
    "Erol Gelenbe": "ACM Fellow",
    "John P Hayes": "ACM Fellow",
    "Joseph F Jaja": "ACM Fellow",
    "Robert E Kahn": "ACM Fellow",
    "Sung Mo Kang ": "ACM Fellow",
    "Richard B Kieburtz": "ACM Fellow",
    "Robert A Kowalski": "ACM Fellow",
    "Jeffrey Kramer": "ACM Fellow",
    "Ruby B Lee": "ACM Fellow",
    "Witold Litwin": "ACM Fellow",
    "Barton P Miller": "ACM Fellow",
    "Jeffrey C Mogul": "ACM Fellow",
    "Donald A. Norman": "ACM Fellow",
    "Cherri M Pancake": "ACM Fellow",
    "Christos Papadimitriou": "ACM Fellow",
    "Donn B Parker": "ACM Fellow",
    "Janak H Patel": "ACM Fellow",
    "Ira Pohl": "ACM Fellow",
    "John Mark Pullen": "ACM Fellow",
    "Prabhakar Raghavan": "ACM Fellow",
    "Krithivasan Ramamritham": "ACM Fellow",
    "John C Reynolds": "ACM Fellow",
    "George Robertson": "ACM Fellow",
    "Nick Roussopoulos": "ACM Fellow",
    "Krishan K. Sabnani": "ACM Fellow",
    "Ravinderpal S Sandhu": "ACM Fellow",
    "Hans-Joerg Schek": "ACM Fellow",
    "Richard D Schlichting": "ACM Fellow",
    "David Bernard Shmoys": "ACM Fellow",
    "Marilyn Claire Wolf": "ACM Fellow",
    "Ouri Wolfson": "ACM Fellow",
    "Pamela Zave": "ACM Fellow",
    "Francine Berman": "ACM Fellow",
    "Laxminarayan Bhuyan Bhuyan": "ACM Fellow",
    "Alan W Biermann": "ACM Fellow",
    "Shahid H Bokhari": "ACM Fellow",
    "Randal E Bryant": "ACM Fellow",
    "Peter Buneman": "ACM Fellow",
    "Stuart K. Card": "ACM Fellow",
    "Michael J Carey": "ACM Fellow",
    "Douglas E Comer": "ACM Fellow",
    "Karen Duncan": "ACM Fellow",
    "Deborah Estrin": "ACM Fellow",
    "Ronald Fagin": "ACM Fellow",
    "Peter A Freeman": "ACM Fellow",
    "Wesley Kent Fuchs": "ACM Fellow",
    "Donald J Haderle": "ACM Fellow",
    "Michael Heath": "ACM Fellow",
    "Leonard Kleinrock": "ACM Fellow",
    "Henry F Korth": "ACM Fellow",
    "Axel Van Lamsweerde": "ACM Fellow",
    "Raymond A Lorie": "ACM Fellow",
    "Donald W Loveland": "ACM Fellow",
    "Albert R Meyer": "ACM Fellow",
    "James H Morris": "ACM Fellow",
    "Larry L Peterson": "ACM Fellow",
    "Moshe Y Vardi": "ACM Fellow",
    "David S Warren": "ACM Fellow",
    "Reinhard Wilhelm": "ACM Fellow",
    "Willy E Zwaenepoel": "ACM Fellow",
    "Marc Auslander": "ACM Fellow",
    "Ken Birman": "ACM Fellow",
    "Ronald J. Brachman": "ACM Fellow",
    "Robert T Braden": "ACM Fellow",
    "Rob Cook": "ACM Fellow",
    "Joseph S DeBlasi": "ACM Fellow",
    "Richard J Fateman": "ACM Fellow",
    "James D Foley": "ACM Fellow",
    "John D Gannon": "ACM Fellow",
    "Charles M Geschke": "ACM Fellow",
    "Robert L Glass": "ACM Fellow",
    "Ronald L. Graham": "ACM Fellow",
    "Leonidas Guibas": "ACM Fellow, ACM Fellow, IEEE Fellow, National Academy of Engineering Member, National Academy of Arts and Sciences Member",
    "Toshihide Ibaraki": "ACM Fellow",
    "Philip M Lewis": "ACM Fellow",
    "David MacQueen": "ACM Fellow",
    "C. Dianne Martin": "ACM Fellow",
    "Larry M Masinter": "ACM Fellow",
    "Kurt Mehlhorn": "ACM Fellow",
    "David L Mills": "ACM Fellow",
    "Pamela Samuelson": "ACM Fellow",
    "Richard Snodgrass": "ACM Fellow",
    "Mary Lou Soffa": "ACM Fellow",
    "Chung Jen Tan": "ACM Fellow",
    "Koji Torii": "ACM Fellow",
    "David L Waltz": "ACM Fellow",
    "John Warnock": "ACM Fellow",
    "Akinori Yonezawa": "ACM Fellow",
    "Dharma P Agrawal": "ACM Fellow",
    "Gregory R Andrews": "ACM Fellow",
    "Andrew W Appel": "ACM Fellow",
    "James C Browne": "ACM Fellow",
    "Robert S Cartwright": "ACM Fellow",
    "Peter P Chen": "ACM Fellow",
    "Lori Clarke": "ACM Fellow",
    "Richard J Cole": "ACM Fellow",
    "Clarence A Ellis": "ACM Fellow",
    "Richard Gabriel": "ACM Fellow",
    "Gopal Krishna Gupta": "ACM Fellow",
    "James Jay Horning": "ACM Fellow",
    "Neil Jones": "ACM Fellow",
    "Aravind K Joshi": "ACM Fellow",
    "Stephen T Kent": "ACM Fellow",
    "Simon S Lam": "ACM Fellow",
    "Kai Li": "ACM Fellow",
    "David Maier": "ACM Fellow",
    "David S Notkin": "ACM Fellow",
    "Susan H Nycum": "ACM Fellow",
    "Leon J Osterweil": "ACM Fellow",
    "Venkat Rangan": "ACM Fellow",
    "John T Richards": "ACM Fellow",
    "Lawrence A Rowe": "ACM Fellow",
    "Barbara Gershon Ryder": "ACM Fellow",
    "Alan Selman": "ACM Fellow",
    "Carlo H Sequin": "ACM Fellow",
    "Eugene H. Spafford": "ACM Fellow",
    "Eva Tardos": "ACM Fellow",
    "Richard N Taylor": "ACM Fellow",
    "Albert J Turner": "ACM Fellow",
    "Emmerich Welzl": "ACM Fellow",
    "Jeannette M. Wing": "ACM Fellow",
    "Mihalis Yannakakis": "ACM Fellow",
    "Stuart Zweben": "ACM Fellow",
    "Ian F Akyildiz": "ACM Fellow",
    "Jean-Loup E Baer": "ACM Fellow",
    "Roger R Bate": "ACM Fellow",
    "J D Couger": "ACM Fellow",
    "Gordon B Davis": "ACM Fellow",
    "David P Dobkin": "ACM Fellow",
    "Hector Garcia-Molina": "ACM Fellow",
    "Irene Greif": "ACM Fellow",
    "Yuri Gurevich": "ACM Fellow",
    "John L Hennessy": "ACM Fellow",
    "Richard A. Kemmerer": "ACM Fellow",
    "H W Lawson": "ACM Fellow",
    "Der-Tsai Lee": "ACM Fellow",
    "Richard Lipton": "ACM Fellow",
    "Nancy Lynch": "ACM Fellow",
    "Daniel A. Menasce": "ACM Fellow",
    "Ron Perrott": "ACM Fellow",
    "Nicholas Pippenger": "ACM Fellow",
    "Vaughan Ronald Pratt": "ACM Fellow",
    "John H Reif": "ACM Fellow",
    "Raymond Reiter": "ACM Fellow",
    "Robert Sedgewick": "ACM Fellow",
    "Kenneth C Sevcik": "ACM Fellow",
    "Micha Sharir": "ACM Fellow",
    "Alan C Shaw": "ACM Fellow",
    "Ben Shneiderman": "ACM Fellow",
    "Kenneth Steiglitz": "ACM Fellow",
    "Donald F Towsley": "ACM Fellow",
    "Peter Widmayer": "ACM Fellow",
    "Robert Wilensky": "ACM Fellow",
    "Philip S Yu": "ACM Fellow",
    "Paolo Zanella": "ACM Fellow",
    "William Richards Adrion": "ACM Fellow",
    "Alfred V Aho": "ACM Fellow",
    "Kurt B Akeley": "ACM Fellow",
    "Gregor V Bochmann": "ACM Fellow",
    "Anita Borg": "ACM Fellow",
    "B. Chandrasekaran": "ACM Fellow",
    "Bernard Chazelle": "ACM Fellow",
    "George Dodd": "ACM Fellow",
    "Jose L. Encarnacao": "ACM Fellow",
    "Jeanne Ferrante": "ACM Fellow",
    "Michael J Fischer": "ACM Fellow",
    "Dennis Frailey": "ACM Fellow",
    "Robert M Graham": "ACM Fellow",
    "Michael A Harrison": "ACM Fellow",
    "Mary Jane Irwin": "ACM Fellow",
    "Jeffrey Jaffe": "ACM Fellow",
    "Anita K Jones": "ACM Fellow",
    "Randy H. Katz": "ACM Fellow",
    "Maria Klawe": "ACM Fellow",
    "Lawrence H Landweber": "ACM Fellow",
    "Michael E Lesk": "ACM Fellow",
    "Henry M Levy": "ACM Fellow",
    "Barbara Liskov": "ACM Fellow",
    "Richard R Muntz": "ACM Fellow",
    "Richard E Nance": "ACM Fellow",
    "Bryan Preas": "ACM Fellow",
    "TRN Rao": "ACM Fellow",
    "Edward M Reingold": "ACM Fellow",
    "John Rice": "ACM Fellow",
    "Sartaj K Sahni": "ACM Fellow",
    "John E Savage": "ACM Fellow",
    "Ravi Sethi": "ACM Fellow",
    "Mary M Shaw": "ACM Fellow",
    "John A Stankovic": "ACM Fellow",
    "Larry Stockmeyer": "ACM Fellow",
    "Andrew S Tanenbaum": "ACM Fellow",
    "Mary K Vernon": "ACM Fellow",
    "Uzi Vishkin": "ACM Fellow",
    "Jeffrey S. Vitter": "ACM Fellow",
    "Anthony I Wasserman": "ACM Fellow",
    "Fred W Weingarten": "ACM Fellow",
    "Ian Witten": "ACM Fellow",
    "Marshall C Yovits": "ACM Fellow",
    "Paul W Abrahams": "ACM Fellow",
    "R L Ashenhurst": "ACM Fellow",
    "Alan H Barr": "ACM Fellow",
    "Grady Booch": "ACM Fellow",
    "David H Brandin": "ACM Fellow",
    "Richard P Brent": "ACM Fellow",
    "Loren C Carpenter": "ACM Fellow",
    "Edwin Catmull": "ACM Fellow",
    "Robert Constable": "ACM Fellow",
    "Dorothy E Denning": "ACM Fellow",
    "David DeWitt": "ACM Fellow",
    "Erwin Engeler": "ACM Fellow",
    "Zvi Galil": "ACM Fellow",
    "Michael R Garey": "ACM Fellow",
    "Myron Ginsberg": "ACM Fellow",
    "John B Goodenough": "ACM Fellow",
    "Donald Greenberg": "ACM Fellow",
    "Herbert R J Grosch": "ACM Fellow",
    "Bertram Herzog": "ACM Fellow",
    "Harold J Highland": "ACM Fellow",
    "Lance Hoffman": "ACM Fellow",
    "David S Johnson": "ACM Fellow",
    "Cliff B Jones": "ACM Fellow",
    "Kenneth W Kennedy": "ACM Fellow",
    "Won Kim": "ACM Fellow",
    "Sambasiva Kosaraju": "ACM Fellow",
    "Richard E. Ladner": "ACM Fellow",
    "S Lakshmivarahan": "ACM Fellow",
    "Nancy Leveson": "ACM Fellow",
    "Jayadev Misra": "ACM Fellow",
    "J Nievergelt": "ACM Fellow",
    "Anthony Oettinger": "ACM Fellow",
    "Franco P Preparata": "ACM Fellow",
    "Roy F Rada": "ACM Fellow",
    "Daniel J Rosenkrantz": "ACM Fellow",
    "Gerard Salton": "ACM Fellow",
    "Fred B Schneider": "ACM Fellow",
    "Norihisa Suzuki": "ACM Fellow",
    "Jeffrey D Ullman": "ACM Fellow",
    "Chris S Wallace": "ACM Fellow",
    "Peter Wegner": "ACM Fellow",
    "John R. White": "ACM Fellow",
    "J Turner Whitted": "ACM Fellow",
    "Chak-Kuen Wong": "ACM Fellow",
    "Andrew C Yao": "ACM Fellow",
    "Paul Young": "ACM Fellow",
    "James M Adams": "ACM Fellow",
    "Franz L Alt": "ACM Fellow",
    "William F. Atchison": "ACM Fellow",
    "Richard H Austing": "ACM Fellow",
    "Kenneth E Batcher": "ACM Fellow",
    "C Gordon Bell": "ACM Fellow",
    "Michael W Blasgen": "ACM Fellow",
    "Daniel Bobrow": "ACM Fellow",
    "David R. Boggs": "ACM Fellow",
    "Lorraine Borman": "ACM Fellow",
    "Charles L Bradshaw": "ACM Fellow",
    "Daniel S Bricklin": "ACM Fellow",
    "Frederick Brooks": "ACM Fellow",
    "Douglas K Brotz": "ACM Fellow",
    "Richard R. Burton": "ACM Fellow",
    "Richard G Canning": "ACM Fellow",
    "Walter Carlson": "ACM Fellow",
    "Vinton Cerf": "ACM Fellow",
    "Edgar F Codd": "ACM Fellow",
    "Ed Coffman": "ACM Fellow",
    "Fernando J Corbato": "ACM Fellow",
    "Harvey G Cragon": "ACM Fellow",
    "Thomas A D'Auria": "ACM Fellow",
    "Thomas DeFanti": "ACM Fellow",
    "Peter J Denning": "ACM Fellow",
    "Jack Dennis": "ACM Fellow",
    "L Peter Deutsch": "ACM Fellow",
    "Edsger W Dijkstra": "ACM Fellow",
    "Stephen Dunwell": "ACM Fellow",
    "J Presper Eckert": "ACM Fellow",
    "Peter Elias": "ACM Fellow",
    "Gerald L Engel": "ACM Fellow",
    "John H Esbin": "ACM Fellow",
    "Bob O Evans": "ACM Fellow",
    "Aaron Finerman": "ACM Fellow",
    "Robert W. Floyd": "ACM Fellow",
    "Michael J Flynn": "ACM Fellow",
    "Robert M Frankston": "ACM Fellow",
    "Frank Friedman": "ACM Fellow",
    "Bernard A Galler": "ACM Fellow",
    "Charles W Gear": "ACM Fellow",
    "Adele Goldberg": "ACM Fellow",
    "Calvin C. Gotlieb": "ACM Fellow",
    "Susan L Graham": "ACM Fellow",
    "Jim Gray": "ACM Fellow",
    "Cordell Green": "ACM Fellow",
    "David Joseph Gries": "ACM Fellow",
    "Carl Hammer": "ACM Fellow",
    "Richard W Hamming": "ACM Fellow",
    "Fred H Harris": "ACM Fellow",
    "Juris Hartmanis": "ACM Fellow",
    "William Daniel Hillis": "ACM Fellow",
    "John E Hopcroft": "ACM Fellow",
    "Tom Hull": "ACM Fellow",
    "J N Hume": "ACM Fellow",
    "Harry D Huskey": "ACM Fellow",
    "William Kahan": "ACM Fellow",
    "Ronald M Kaplan": "ACM Fellow",
    "Richard Karp": "ACM Fellow",
    "Donald E Knuth": "ACM Fellow",
    "David J Kuck": "ACM Fellow",
    "Thomas E Kurtz": "ACM Fellow",
    "Ray Kurzweil": "ACM Fellow",
    "Butler W Lampson": "ACM Fellow",
    "Stephen S Lavenberg": "ACM Fellow",
    "Joshua Lederberg": "ACM Fellow",
    "John A Lee": "ACM Fellow",
    "Meir Lehman": "ACM Fellow",
    "Joyce Currie Little": "ACM Fellow",
    "C.L. Liu": "ACM Fellow",
    "M Stuart Lynn": "ACM Fellow",
    "Herbert Maisel": "ACM Fellow",
    "Zohar Manna": "ACM Fellow",
    "John McCarthy": "ACM Fellow",
    "Daniel D McCracken": "ACM Fellow",
    "Paul R McJones": "ACM Fellow",
    "A J Milner": "ACM Fellow",
    "Roger M Needham": "ACM Fellow",
    "Peter G Neumann": "ACM Fellow",
    "Monroe Newborn": "ACM Fellow",
    "John Ousterhout": "ACM Fellow",
    "Susan S Owicki": "ACM Fellow",
    "David Lorge Parnas": "ACM Fellow",
    "William B Poucher": "ACM Fellow",
    "Anthony Ralston": "ACM Fellow",
    "Ronald L Rivest": "ACM Fellow",
    "Azriel Rosenfeld": "ACM Fellow",
    "Jeff Rulifson": "ACM Fellow",
    "Jean E Sammet": "ACM Fellow",
    "Dana S Scott": "ACM Fellow",
    "Herbert A Simon": "ACM Fellow",
    "Barbara B Simons": "ACM Fellow",
    "Donald R Slutz": "ACM Fellow",
    "Richard E Stearns": "ACM Fellow",
    "Thomas B Steel": "ACM Fellow",
    "Guy L Steele": "ACM Fellow",
    "Michael Stonebraker": "ACM Fellow",
    "William Strecker": "ACM Fellow",
    "Patrick Suppes": "ACM Fellow",
    "Gerald Sussman": "ACM Fellow",
    "Ivan Sutherland": "ACM Fellow",
    "Edward A Taft": "ACM Fellow",
    "Robert E Tarjan": "ACM Fellow",
    "Robert W Taylor": "ACM Fellow",
    "Charles P Thacker": "ACM Fellow",
    "Irv Traiger": "ACM Fellow",
    "Joseph Traub": "ACM Fellow",
    "Allen Tucker": "ACM Fellow",
    "Andries van Dam": "ACM Fellow",
    "Willis H Ware": "ACM Fellow",
    "Ben Wegbreit": "ACM Fellow",
    "Eric A Weiss": "ACM Fellow",
    "David John Wheeler": "ACM Fellow",
    "Maurice V. Wilkes": "ACM Fellow",
    "Shmuel Winograd": "ACM Fellow",
    "Niklaus E Wirth": "ACM Fellow",
    "Seymour J Wolfson": "ACM Fellow",
    "William A Wulf": "ACM Fellow",
    "L A Zadeh": "ACM Fellow",
    "Luc Van Gool": "Marr\u5956\u5f97\u4e3b\uff0cETH\u6559\u6388\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u5b9e\u9a8c\u5ba4\u4e3b\u4efb",
    "Alan Yuille": "IEEE Fellow",
    "Richard Hartley": "IEEE Fellow",
    "Philip H. S. Torr": "FREng Fellow, FRS Fellow, Royal Academy of Engineering Research Chair in Computer Vision and Machine Learning",
    "Song-Chun Zhu": "IEEE Fellow",
    "Andreas Geiger": "ELLIS fellow",
    "Trevor Darrell": "associate editor for the Artificial Intelligence Journal and the IEEE Transactions on Pattern Analysis and Machine Intelligence.",
    "Xiaoou Tang": "IEEE Fellow, Associate Editor of IEEE TPAMI and editor-in-chief of IJCV, founder of SenseTime",
    "Kostas Daniilidis": "IEEE Fellow, Associate Editor of IEEE TPAMI, Ruth Yalom Stone Professor of Computer Vision at the University of Pennsylvania",
    "Max Welling": "CIFAR Fellow, ELLIS Fellow, Associate editor in chief of IEEE TPAMI (2011-2015), advisory board of the Neurips foundation",
    "Christian Theobalt": "Fellow of EUROGRAPHICS, Associate editor of IEEE TPAMI and ACM ToG, Director of Visual Computing and AI Department at MPI for Informatics",
    "Steven M. Seitz": "Alfred P. Sloan Fellowship, Twice awarded the David Marr Prize, NSF Career Award, ONR Young Investigator Award",
    "William T. Freeman": "ACM Fellow, IEEE Fellow, AAAI Fellow, National Academy of Engineering Member",
    "Joshua B. Tenenbaum": "MacArthur Fellow, Fellow of Society of Experimental Psychologists, Fellow of Cognitive Science Society, associate editor of the journal Cognitive Science",
    "Silvio Savarese": "Executive Vice President and Chief Scientist of salesforce, Faculty co-director of Standford Vision and Learning Lab, NSF Career Award",
    "Li Fei-Fei": "IEEE Fellow, National Academy of Engineering Member, National Academy of Medicine Member, American Academy of Arts and Sciences Member, J.K. Aggarwal Prize of IAPR, Co-Director of the Stanford Vision and Learning Lab",
    "Thomas Brox": "ELLIS fellow, Associate Editor of IJCV, IEEE TPAMI",
    "Davide Scaramuzza": "Associate Editor for the IEEE Transactions on Robotics, IEEE Robotics and Automation Society Early Career Award",
    "Alexei A. Efros": "Journal Editorial Board of IJCV, PAMI Thomas S. Huang Memorial Prize",
    "Phillip Isola": "Google Faculty Research Award, PAMI Young Researcher Award",
    "Tae-Kyun Kim": "Associate Editor of Pattern Recognition Journal, Image and Vision Computing Journal, and IET Computer Vision",
    "Nassir Navab": "IEEE Fellow",
    "Michael J. Black": "foreign member of the Royal Swedish Academy of Sciences",
    "David A. Forsyth": "IEEE Fellow, ACM Fellow, editor of PAMI",
    "Andrew Zisserman": "Marr\u5956\u5f97\u4e3b\u3001\u82f1\u56fd\u7687\u5bb6\u5b66\u4f1a\u4f1a\u58eb\u3001BMVA Distinguished Fellowship",
    "Stephen Lin": "Editorial Board of IJCV",
    "Geoffrey Hinton": " fellow of the Royal Society, the Royal Society of Canada, and the Association for the Advancement of Artificial Intelligence, foreign member of the American Academy of Arts and Sciences and the National Academy of Engineering, and a former president of the Cognitive Science Society.",
    "Eric P. Xing": "Board member of The international machine learning society, associate editor JASA, AOAS, JMLR, MLJ and PAMI",
    "Niloy J. Mitra": "associate editor of CGF, CAGD, TOG",
    "Raquel Urtasun": "Editorial Board of IJCV",
    "Daniel Cremers": "member of the Bavarian Academy of Sciences and Humanitites. associate for ICCV, ECCV, CVPR, ACCV, IROS",
    "Ping Tan": "Director of Alibaba DAMO academy",
    "Baoquan Chen": "IEEE Fellow\uff0c\u66fe\u4efbTOG\u3001 TVCG\u7b49\u9876\u520a\u7f16\u59d4\uff0cVIS Academy",
    "Paul Debevec": "\u827e\u7f8e\u5956\u7ec8\u8eab\u6210\u5c31\u5956\u5f97\u4e3b",
    "Xin Tong": "\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u9996\u5e2d\u7814\u7a76\u5458",
    "Andrew Davison": "Fellow of the Royal Academy of Engineering",
    "Geoffrey E. Hinton": "\u56fe\u7075\u5956\uff0cFellow of the Royal Society, \u7f8e\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Gordon Wetzstein": "Associate editor of TOG",
    "Ali Farhadi": "Sloan Research Fellowships",
    "Qionghai Dai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "D.A. Forsyth": "IEEE/ACM Fellow, editor in chief, the TPAMI",
    "Antonio Torralba": "AAAI Fellow, associate editor of IJCV, program chair for CVPR",
    "Dietor Fox": "IEEE/ACM/AAAI Fellow, editor of the IEEE Transactions on Robotics, program co-chair of the 2008 AAAI Conference on Artificial Intelligence, and program chair of the 2013 Robotics: Science and Systems conference.",
    "Charlie C.L. Wang": "\u7f8e\u56fd\u673a\u68b0\u5de5\u7a0b\u5b66\u4f1aFellow",
    "Gang Hua": "IEEE & IAPR Fellow",
    "Jiri Matas": "Associate Editor of TPAMI",
    "Pascal Fua": "IEEE Fellow",
    "Erik Blasch": "IEEE Fellow, AIAA Associate Fellow",
    "Long Quan": "IEEE Fellow",
    "Andrea Vedaldi": "Associate Editor of TPAMI",
    "Bing Zeng": "IEEE Fellow",
    "Roberto Cipolla": "\u82f1\u56fd\u7687\u5bb6\u5b66\u4f1a\u9662\u58eb\uff0c\u82f1\u56fd\u7687\u5bb6\u5de5\u7a0b\u5b66\u9662\u9662\u58eb",
    "Weimin Bao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guilin Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guoliang Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hanfu Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Junliang Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xingdan Chen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Junhao Chu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Tiejun Cui": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ruwei Dai": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chibiao Ding": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guangren Duan": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yunmei Dong": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jiancheng Fang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dengguo Feng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Fuxi Gan": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qihuang Gong": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ying Gu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xiaohong Guan": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guangcan Guo": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Lei Guo": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yue Hao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jifeng He": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chaohuan Hou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xun Hou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jinpeng Huai": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Lin Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Minqiang Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ru Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Wei Huang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Shuisheng Jian": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Fengyi Jiang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jie Jiang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yaqiu Jin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dingbo Kuang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xiaolin Lei": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qihu Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Shushen Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Wei Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xiang Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yanda Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhi Li": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Huimin Lin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guozhi Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Shenggang Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Songhao Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yichun Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yongtan Liu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianhua Lu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ruqian Lu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jian Lv": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Junfa Mao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Kunchi Peng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Depei Qian": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hong Qiao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Guogang Qin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xubang Shen": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jian Song": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Huaimin Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jiaqi Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jinlong Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianyu Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Lijun Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qiming Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yangyuan Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yongliang Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yuzhu Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yue Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb, \u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhanguo Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhijiang Wang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhaohui Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dexin Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hongxin Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Peiheng Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yirong Wu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianbai Xia": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Libin Xiang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zongben Xu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ningsheng Xu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yongqi Xue": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Deren Yang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xuejun Yang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianquan Yao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Qizhi Yao": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Hao Yin": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Dengyun Yu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Bo Zhang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jingzhong Zhang": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Jianhua Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Wanhua Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Yaozong Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Youliao Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhiming Zheng": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Bingkun Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Chaochen Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Xingming Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhixin Zhou": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Luhua Zhu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Ninghua Zhu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Zhongliang Zhu": "\u4e2d\u56fd\u79d1\u5b66\u9662\u9662\u58eb",
    "Tianyou Chai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Chun Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jie Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhijie Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zuoning Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hao Dai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhonghan Deng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Wenhua Ding": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Baoyan Duan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Bangkui Fan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Binxing Fang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Aiguo Fei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Weihua Gui": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "You He": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Bitao Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Changjun Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Huilin Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhiyin Kong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yushi Lan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Detian Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Deyi Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xiangke Liao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jie Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yongjian Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zejin Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Teng Long": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jun Lu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xicheng Lu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xiangang Luo": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yi Luo": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yueguang Lv": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yunhe Pan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Donglin Su": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiaguang Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Ninghui Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiubin Tan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Endong Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shafei Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yaonan Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yiyin Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hanming Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiangxing Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianping Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianqi Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Manqing Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Weiren Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yangsheng Xu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xiaoniu Yang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Fuqiang Yao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Quan Yu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shaohua Yu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Baodong Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guangjun Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hongke Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Ping Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yaoxue Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Qinping Zhao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Nanning Zheng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Weimin Zheng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "De Ben": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hegao Cai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiren Cai": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jing Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Lianghui Chen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Dianyuan Fan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jiaxiong Fang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xisheng Feng": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jie Gao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Huixing Gong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xianyi Gong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhiben Gong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guirong Guo": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Dequan He": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xingui He": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Qiheng Hu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Peikang Huang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Wenhan Jiang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guofan Jin": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yilian Jin": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Bohu Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Deren Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guojie Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Lemin Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Tongbao Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Youping Li": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yongnian Lin": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shanghe Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yunjie Liu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Jianxun Lu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yuanliang Ma": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Erke Mao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guangnan Ni": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Junhua Pan": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Changxiang Shen": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Junhong Su": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Youxian Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yu Sun": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Chengwei Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Renxiang Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Tianran Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zicai Wang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Yu Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhengyao Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Ziqing Wei": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Cheng Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Hequan Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shouer\u00b7Silamu Wu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Juyan Xu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zuyan Xu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shizhong Yang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Junen Yao": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Minghan Ye": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shangfu Ye": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shenghua Ye": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Guangyi Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Lvqian Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Minggao Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Xixiang Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhonghua Zhang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shan Zhong": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Liwei Zhou": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Shouhuan Zhou": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Zhongyi Zhou": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Gaofeng Zhu": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb",
    "Songlin Zhuang": "\u4e2d\u56fd\u5de5\u7a0b\u9662\u9662\u58eb"
}
  var papers =[
  {
    "title": "vox-surf: voxel-based implicit surface representation",
    "id": 1,
    "valid_pdf_number": "27/28",
    "matched_pdf_number": "22/27",
    "matched_rate": 0.8148148148148148,
    "citations": {
      "Neuralangelo: High-fidelity neural surface reconstruction": {
        "authors": [
          "Z Li",
          "T M\u00fcller",
          "A Evans",
          "RH Taylor"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.pdf",
        "ref_texts": "[18] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "18"
        ],
        "1": "Follow-up works extend the above approaches to realtime at the cost of surface fidelity [18, 37], while others [3, 5, 44] use auxiliary information to enhance the reconstruction results."
      },
      "Sine: Semantic-driven image-based nerf editing with prior-guided editing field": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "B Yang",
          "T Fan"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf",
        "ref_texts": "[31] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "31"
        ],
        "1": "Recently, NeRF [42] achieves photo-realistic rendering with volume rendering and inspires many works, including surface reconstruction [31,65, 73], scene editing [4, 18, 67, 70, 71] and generation [22, 51], inverse rendering [5, 76], SLAM [72, 77], etc."
      },
      "Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam": {
        "authors": [
          "H Wang",
          "J Wang",
          "L Agapito"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf",
        "ref_texts": "[12] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-Surf: V oxel-based Implicit SurfaceRepresentation. arXiv preprint arXiv:2208.10925 , 2022. 2, 3",
        "ref_ids": [
          "12"
        ],
        "1": "Recent efforts focus on sparse alternatives to these parametric embeddings such as octrees [28], tri-plane [2], hash-grid [15] or sparse voxel grid [12, 13] to improve the memory efficiency of dense grids.",
        "2": "To improve the memory efficiency of parametric encoding-based methods, sparse parametric encodings, such as Octree [28], Tri-plane [2], or sparse voxel grid [12, 13, 15], have been proposed."
      },
      "Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation": {
        "authors": [
          "X Yang",
          "H Li",
          "H Zhai",
          "Y Ming",
          "Y Liu"
        ],
        "url": "https://arxiv.org/pdf/2210.15858",
        "ref_texts": "[13] H. Li, X. Yang, H. Zhai, Y . Liu, H. Bao, and G. Zhang. V ox-surf: V oxelbased implicit surface representation. arXiv preprint arXiv:2208.10925 , 2022.",
        "ref_ids": [
          "13"
        ],
        "1": "Our hybrid scene representation is inspired by recent works that use voxel-based neural implicit representations [13, 14] .",
        "2": "More specifically, we share a similar structure with V ox-Surf [13] which encodes 3D scenes with neural networks and local embeddings.",
        "3": "Unlike prior works where the limit is heuristically specified [13, 14], we dynamically change it according to the specified maximum sampling distance Dmax.",
        "4": "[13] H."
      },
      "Neat: Learning neural implicit surfaces with arbitrary topologies from multi-view images": {
        "authors": [
          "X Meng",
          "W Chen",
          "B Yang"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_NeAT_Learning_Neural_Implicit_Surfaces_With_Arbitrary_Topologies_From_Multi-View_CVPR_2023_paper.pdf",
        "ref_texts": "[23] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , pages 1\u201312, 2022. 2",
        "ref_ids": [
          "23"
        ],
        "1": "Related Work 3D Geometric Representation A 3D surface can be represented explicitly with voxels [5, 11, 23, 32, 44], pointclouds [1, 12, 24, 31, 53], and meshes [16, 47, 50], or can be represented implicitly with neural implicit functions, which have gained popularity for their continuity and the arbitraryresolution property."
      },
      "Gridpull: Towards scalability in learning implicit representations from 3d point clouds": {
        "authors": [
          "C Chen",
          "YS Liu",
          "Z Han"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_GridPull_Towards_Scalability_in_Learning_Implicit_Representations_from_3D_Point_ICCV_2023_paper.pdf",
        "ref_texts": "[38] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-Surf: V oxel-based implicit surface representation. CoRR, abs/2208.10925, 2022. 2",
        "ref_ids": [
          "38"
        ],
        "1": "Some methods [70, 75,96,38] inferred a discrete radiance field defined on grids to speed up the learning of a radiance field, while they cared more about the quality of synthesized views than the underlying geometry."
      },
      "Towards unbiased volume rendering of neural implicit surfaces with geometry priors": {
        "authors": [
          "Y Zhang",
          "Z Hu",
          "H Wu",
          "M Zhao",
          "L Li"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.pdf",
        "ref_texts": "[10] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 3",
        "ref_ids": [
          "10"
        ],
        "1": "Others aim to enhance training efficiency via voxel-based representation, such as V oxurf [26] and V ox-Surf [10]\n3."
      },
      "Cp-slam: Collaborative neural point-based slam system": {
        "authors": [
          "J Hu",
          "M Mao",
          "H Bao",
          "G Zhang"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7c10e259c7e56fa218ee03d9ae7d728e-Paper-Conference.pdf",
        "ref_texts": "[21] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "21"
        ],
        "1": "Inspired by different representations of neural field including voxel grid [21] and point cloud [43], NICE-SLAM [50] and V ox-Fusion [44] chose voxel grid to perform tracking and mapping instead of a single neural network which is limited by expression ability and forgetting problem."
      },
      "Streetsurf: Extending multi-view implicit surface reconstruction to street views": {
        "authors": [
          "J Guo",
          "N Deng",
          "X Li",
          "Y Bai",
          "B Shi",
          "C Wang"
        ],
        "url": "https://arxiv.org/pdf/2306.04988",
        "ref_texts": "[18] H. Li, X. Yang, H. Zhai, Y . Liu, H. Bao, and G. Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "18"
        ],
        "1": "To improve spatial efficiency, voxel-pruning [10,18,24,48,66], hash-indexing [32], tensordecomposition [4,5], or multi-scale voxels [27] are introduced.",
        "2": "Follow-up works replace the MLP network with local implicit grids [41,56], sparse voxels [18], MLP blocks [8] or displacement fields [58] for better efficiency or local details.",
        "3": "This occupancy information is periodically updated from the close-range SDF network in a bootstrap manner that does not require reconstructing the scene in advance with COLMAP [42] as [46] or feeding LiDAR pointclouds to pre-compute occupied voxels as V oxSurf [18].",
        "4": "[18] H.",
        "5": "2 Optional sky masks Previous research studying multi-view neural reconstruction [18,46] or novel view synthesis [40,49] on outdoor scenes typically require mask annotation or segmentation that marks sky pixels in order to penalize non-occupied areas.",
        "6": "Previous approach studying multi-view reconstruction of outdoor scenes require either reconstructing the scene in advance [46] with COLMAP [42] or feeding LiDAR pointclouds to pre-compute occupied voxels [18]."
      },
      "Mips-fusion: Multi-implicit-submaps for scalable and robust online neural rgb-d reconstruction": {
        "authors": [
          "Y Tang",
          "J Zhang",
          "Z Yu",
          "H Wang",
          "K Xu"
        ],
        "url": "https://arxiv.org/pdf/2308.08741",
        "ref_texts": ""
      },
      "Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images": {
        "authors": [
          "Y Liu",
          "P Wang",
          "C Lin",
          "X Long",
          "J Wang",
          "L Liu"
        ],
        "url": "https://arxiv.org/pdf/2305.17398",
        "ref_texts": ""
      },
      "Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance": {
        "authors": [
          "H Chen",
          "C Li",
          "GH Lee"
        ],
        "url": "https://arxiv.org/pdf/2312.00846",
        "ref_texts": "[22] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 3",
        "ref_ids": [
          "22"
        ],
        "1": "Further research has aimed to adapt these methods for real-time applications, but compromising on surface accuracy as reported in recent studies [22, 49]."
      },
      "Swift-Mapping: Online Neural Implicit Dense Mapping in Urban Scenes": {
        "authors": [
          "K Wu",
          "K Zhang",
          "M Gao",
          "J Zhao",
          "Z Gan"
        ],
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/28420/28820",
        "ref_texts": "2022. Vox-surf: Voxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics.Li, Z.; Li, L.; and Zhu, J. 2023. Read: Large-scale neural scene rendering for autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 1522\u20131529. Lin, J.; and Zhang, F. 2022. R 3 LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package. In 2022 International Conference on Robotics and Automation (ICRA), 10672\u2013",
        "ref_ids": [
          "2022"
        ]
      },
      "Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing": {
        "authors": [
          "J Zeng",
          "C Bao",
          "R Chen",
          "Z Dong",
          "G Zhang"
        ],
        "url": "https://arxiv.org/pdf/2308.03280",
        "ref_texts": ""
      },
      "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview": {
        "authors": [
          "Y Ming",
          "X Yang",
          "W Wang",
          "Z Chen",
          "J Feng"
        ],
        "url": "https://arxiv.org/pdf/2405.05526",
        "ref_texts": "[48] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, G. Zhang, Vox-surf: Voxel-based implicit surface representation, IEEE Transactions on Visualization and Computer Graphics (2022) 1\u201312.",
        "ref_ids": [
          "48"
        ],
        "1": "InsteadofencodingtheentirescenewiththeMLPs, Vox-Surf [48] employs a hybrid architecture that consists of an explicit dense voxel grid with the neural implicit surface representation.",
        "2": "55 Vox-Surf[48] 0.",
        "3": "[48] H."
      },
      "PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction": {
        "authors": [
          "W Su",
          "C Zhang",
          "Q Xu",
          "W Tao"
        ],
        "url": "https://arxiv.org/pdf/2401.12751",
        "ref_texts": "[1] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, and G. Zhang, \u201cVox-surf: Voxel-based implicit surface representation,\u201d IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "1"
        ],
        "1": "Index Terms \u2014Surface reconstruction, volume rendering, surface rendering, multi-view stereo \u2726\n1 I NTRODUCTION SURFACE reconstruction from posed multi-view images is one of the fundamental problems in 3D computer vision [1], [2].",
        "2": "[1] H."
      },
      "Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering": {
        "authors": [
          "C Zhang",
          "W Su",
          "W Tao"
        ],
        "url": "https://arxiv.org/pdf/2310.07997",
        "ref_texts": "[16] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "16"
        ],
        "1": "Impressively, some methods use voxelbased representation to enhance training efficiency, such as V oxurf [32] and V ox-Surf [16], making significant strides in achieving this equilibrium."
      },
      "Sur2f: A Hybrid Representation for High-Quality and Efficient Surface Reconstruction from Multi-view Images": {
        "authors": [
          "Z Huang",
          "Z Liang",
          "H Zhang",
          "Y Lin",
          "K Jia"
        ],
        "url": "https://arxiv.org/pdf/2401.03704",
        "ref_texts": "[20] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 3",
        "ref_ids": [
          "20"
        ],
        "1": "And their follow-up works [3, 9, 10, 20, 22, 46\u201348, 50] make efforts to improve the quality and/or efficiency."
      },
      "LGSDF: Continual Global Learning of Signed Distance Fields Aided by Local Updating": {
        "authors": [
          "Y Yue",
          "Y Deng",
          "J Wang",
          "Y Yang"
        ],
        "url": "https://arxiv.org/pdf/2404.05187"
      },
      "InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering": {
        "authors": [
          "A Canela",
          "P Caselles",
          "I Malik",
          "GT Garces"
        ],
        "url": "https://arxiv.org/pdf/2308.04868",
        "ref_texts": "[20] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. arXiv preprint arXiv:2208.10925 , 2022. 2",
        "ref_ids": [
          "20"
        ],
        "1": "Still, the convergence time for these approaches remains in the tens of minutes [20], which is prohibitively high for many applications.",
        "2": "Methods based on multi-resolution feature grids have been successfully used in combination with differentiable volume rendering to obtain 3D reconstructions [20, 61]."
      },
      "3QFP: Efficient neural implicit surface reconstruction using Tri-Quadtrees and Fourier feature Positional encoding": {
        "authors": [
          "S Sun",
          "M Mielle",
          "AJ Lilienthal",
          "M Magnusson"
        ],
        "url": "https://arxiv.org/pdf/2401.07164",
        "ref_texts": "[24] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. \u201cV ox-Surf: V oxelBased Implicit Surface Representation\u201d. In: IEEE Transactions on Visualization and Computer Graphics (2022), pp. 1\u201312. DOI:10.1109/TVCG.2022.",
        "ref_ids": [
          "24"
        ],
        "1": "Instead of storing features in 3D voxel grids [20, 23, 24] or dense feature planes [21], we use three planar quadtrees to represent surfaces.",
        "2": "Accounting for the large memory footprint when applying dense feature voxel grids, several techniques have been proposed to reduce memory usage, such as hash-tables [35], octree-trees [16]; these compact data structures have been leveraged in recent robotic applications [20, 18, 19, 26, 24, 23, 36].",
        "3": "To avoid storing unnecessary features in free space, prior work [20, 23, 16, 24] employs octree to store features only within voxel grids where surface points are located."
      },
      "OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees": {
        "authors": [
          "H Kim",
          "A Meuleman",
          "H Jang",
          "J Tompkin"
        ],
        "url": "https://arxiv.org/pdf/2404.00678",
        "ref_texts": "[12] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "12"
        ],
        "1": "Neural implicit representations using SDFs show potential for image-based 3D reconstruction through several works [12, 19, 28, 30, 31]."
      },
      "ImTooth: Neural Implicit Tooth for Dental Augmented Reality": {
        "authors": [
          "H Li",
          "H Zhai",
          "X Yang",
          "Z Wu",
          "Y Zheng"
        ],
        "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/VR-TVCG-2023-ImTooth/ImTooth.pdf",
        "ref_texts": "[27] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, and G. Zhang. Vox-Surf: Voxelbased implicit surface representation. IEEE Transactions on Visualization and Computer Graphics, pp. 1\u201312, 2022.",
        "ref_ids": [
          "27"
        ],
        "1": "Different from other methods, we use the plaster teeth models as our reconstruction target and learn a voxelsbased neural implicit representation [27] which is editable and flexible.",
        "2": "To reconstruct highquality results of the large-scale scene, V ox-Surf [27] adopts a sparse voxel structure to divide the spatial regions and store the geometry features in the nodes of the voxel.",
        "3": "To reconstruct the detail of the teeth model, we adopt the idea of voxel-based neural implicit representation proposed in Vox-Surf[27], which is flexible and lightweight compared to other methods [36, 58, 62, 63].",
        "4": "Although V ox-Surf [27] relieves the latter step, an accurate 6 DoF pose is still necessary.",
        "5": "From left to right, we show the RGB image of the plaster model, scanned surface (ground truth), and the surface reconstructed by the COLMAP [46,47], NeuS [58], Vox-Surf [27], our proposed ImTooth, respectively.",
        "6": "We adopt the surfaceaware voxel resampling strategy [27] after 30,000 iterations.",
        "7": "We compare our method with the traditional reconstruction method, COLMAP [46, 47], and the recent implicit surface reconstruction method, NeuS [58], V ox-Surf [27].",
        "8": "[27] H."
      },
      "ShapeMed-Knee: A Dataset and Neural Shape Model Benchmark for Modeling 3D Femurs": {
        "authors": [
          "AA Gatti",
          "L Blankemeier",
          "D Van Veen",
          "B Hargreaves"
        ],
        "url": "https://www.medrxiv.org/content/medrxiv/early/2024/05/07/2024.05.06.24306965.full.pdf",
        "ref_texts": "[23] H. Li, X. Yang, H. Zhai, Y . Liu, H. Bao and G. Zhang, \u201cV ox-Surf: V oxel-Based Implicit Surface Representation,\u201d IEEE Transactions on Visualization and Computer Graphics, pp. 1\u201312, 2022.",
        "ref_ids": [
          "23"
        ],
        "1": "To improve reconstruction of large scenes or fine details, instead of a single global z,a spatially localized zis input into the MLP [23], [24].",
        "2": "[23] H."
      },
      "DF-SLAM: Neural Feature Rendering Based on Dictionary Factors Representation for High-Fidelity Dense Visual SLAM System": {
        "authors": [
          "W Wei",
          "J Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.17876",
        "ref_texts": "[24] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, and G. Zhang, \u201cVox-surf: Voxelbased implicit surface representation,\u201d IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "24"
        ],
        "1": "Subsequent works have adopted hybrid scene representations, encoding scene information into features and anchoring these features onto specific data structures such as octrees [21, 22], voxel grids [23, 24], tri-planes [25], and hash grids [26].",
        "2": "[24] H."
      },
      "TOWARDS VISION-GUIDED SKULL BASE SURGERY": {
        "authors": [
          "Z Li"
        ],
        "url": "https://jscholarship.library.jhu.edu/bitstreams/775f39cc-1280-445b-b371-26f380818de1/download",
        "ref_texts": "132.Li, H., Yang, X., Zhai, H., Liu, Y., Bao, H. & Zhang, G. Vox-Surf: Voxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics (2022).",
        "ref_ids": [
          "132"
        ],
        "1": ", within minutes) performancebutsacrificethesurfacequality[132, 133]."
      },
      "Universal structural map for indoor navigation in university campus": {
        "authors": [
          "P \u015awitalski",
          "A Salamo\u0144czyk"
        ],
        "url": "https://czasopisma.uws.edu.pl/studiainformatica/article/download/3619/3409",
        "ref_texts": "14. Li, H., Yang, X., Zhai, H., Liu, Y., Bao, H.,and Zhang G.: Vox-Surf: Voxel-Based Implicit Surface Representation. In IEEE Transactions on Visualization and Computer Graphics (2022), doi: 10.1109/TVCG.2022.3225844.",
        "ref_ids": [
          "14"
        ],
        "1": "[14] propose a Vox-Surf representation which divides the implicit surface into finite bounded voxels."
      }
    }
  },
  {
    "title": "vox-fusion: dense tracking and mapping with voxel-based neural implicit representation",
    "id": 4,
    "valid_pdf_number": "71/79",
    "matched_pdf_number": "64/71",
    "matched_rate": 0.9014084507042254,
    "citations": {
      "Sine: Semantic-driven image-based nerf editing with prior-guided editing field": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "B Yang",
          "T Fan"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf",
        "ref_texts": "[72] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 2",
        "ref_ids": [
          "72"
        ],
        "1": "Recently, NeRF [42] achieves photo-realistic rendering with volume rendering and inspires many works, including surface reconstruction [31,65, 73], scene editing [4, 18, 67, 70, 71] and generation [22, 51], inverse rendering [5, 76], SLAM [72, 77], etc."
      },
      "Progressively optimized local radiance fields for robust view synthesis": {
        "authors": [
          "A Meuleman",
          "YL Liu",
          "C Gao"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Meuleman_Progressively_Optimized_Local_Radiance_Fields_for_Robust_View_Synthesis_CVPR_2023_paper.pdf",
        "ref_texts": "[45] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. arXiv:2210.15858, 2022. 3",
        "ref_ids": [
          "45"
        ],
        "1": "V ox-Fusion [45] and Nice-SLAM [53] achieve good pose estimation but are de-signed for RGB-D inputs and require accurate depth: V oxFusion to allocate a sparse voxel grid and Nice-SLAM to determine where to sample along the ray."
      },
      "vmap: Vectorised object mapping for neural field slam": {
        "authors": [
          "X Kong",
          "S Liu",
          "M Taher"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Kong_vMAP_Vectorised_Object_Mapping_for_Neural_Field_SLAM_CVPR_2023_paper.pdf",
        "ref_texts": "[40] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-Fusion: Dense tracking andmapping with voxel-based neural implicit representation. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR) , 2022. 2",
        "ref_ids": [
          "40"
        ],
        "1": "To make implicit representation more scalable and efficient, a group of implicit SLAM systems [25, 35, 40, 45, 48] fused neural fields with conventional volumetric representations."
      },
      "Nicer-slam: Neural implicit scene encoding for rgb slam": {
        "authors": [
          "Z Zhu",
          "S Peng",
          "V Larsson",
          "Z Cui",
          "MR Oswald"
        ],
        "url": "https://arxiv.org/pdf/2302.03594",
        "ref_texts": "[67] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "67"
        ],
        "1": "Although follow-up works [67, 30, 21, 18, 25, 39] try to improve upon NICE-SLAM and iMAP from different perspectives, all of these works still rely on the reliable depth input from RGB-D sensors.",
        "2": "Many follow-up works improve upon these two works from various perspectives, including efficient scene representation [18, 21], fast optimziation [67], add IMU measurements [25], or different shape representations [39, 30].",
        "3": "We compare to (a) SOTA neural implicitbased RGB-D SLAM system NICE-SLAM [76] and V oxFusion [67], (b) classic MVS method COLMAP [46], and (c) SOTA dense monocular SLAM system DROIDSLAM [57].",
        "4": "Unlike recent implicit-based dense SLAM systems [51, 76, 67] which use occupancy to implicitly represent scene geometry, we instead use SDFs."
      },
      "Point-slam: Dense neural point cloud-based slam": {
        "authors": [
          "E Sandstr\u00f6m",
          "Y Li",
          "L Van Gool"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.pdf",
        "ref_texts": "[69] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 1, 2, 4, 5, 6, 7, 8",
        "ref_ids": [
          "69"
        ],
        "1": "To eliminate the potential domain gap between train and test time, recent SLAM methods rely on test time optimization via volume rendering [53, 69, 79].",
        "2": "18433\n tional approaches, neural scene representations have attractive properties for mapping like improved noise and outlier handling [64], better hole filling and inpainting capabilities for unobserved scene parts [69, 79], and data compression [42, 58].",
        "3": "Like DTAM [37] or BAD-SLAM [48] recent neural SLAM methods [79, 69, 53] only use a single scene representation for both tracking and mapping but they rely either on a regular grid structure [79, 69] or a single MLP [53].",
        "4": "These works have led to full dense SLAM pipelines [69, 79, 53, 28], which represent the current most promising trend towards accurate and robust visual SLAM.",
        "5": "The grid-based representation is perhaps the most explored one and can be further split into methods using dense grids [79, 36, 63, 64, 13, 54, 3, 24, 11, 77, 76, 66, 81], hierarchical octrees [69, 49, 29, 6, 26] and voxel hashing [38, 21, 15, 60, 33] to save memory.",
        "6": "This is in contrast to voxel-based frameworks [79, 69] which need to carve the empty space between the camera and the surface, thus requiring significantly more samples.",
        "7": "We primarily compare our method to existing state-of-the-art dense neural RGBD SLAM methods such as NICE-SLAM [79], V ox-Fusion [69] and ESLAM [28].",
        "8": "We reproduce the results from [69] using the open source code and report the results as V ox-Fusion\u2217.",
        "9": "86 V oxFusion\u2217[69]Depth L1 [cm] \u21931.",
        "10": "77\n(a) Office 0\n Office 3\n Room 0 NICE-SLAM [79] V ox-Fusion\u2217[69] Point-SLAM (ours) Ground Truth (b) Figure 3: Reconstruction Performance on Replica [51].",
        "11": "Office 0\n Room 1\n Room 2 NICE-SLAM [79] V ox-Fusion\u2217[69] Point-SLAM (ours) Ground Truth Figure 4: Rendering Performance on Replica [51] .",
        "12": "06 V ox-Fusion [69] 0.",
        "13": "54 V ox-Fusion\u2217[69] 1.",
        "14": "The grayed numbers of [69] are from the paper that come from a single run which we could not reproduce.",
        "15": "3a compares our method to NICE-SLAM [79], V oxFusion [69] and ESLAM [28] in terms of the geomet-ric reconstruction accuracy.",
        "16": "3b compares the mesh reconstructions of NICE-SLAM [79], V oxFusion [69] and our method to the ground truth mesh.",
        "17": "233 V ox-Fusion\u2217[69]PSNR [dB] \u2191 22.",
        "18": "For NICE-SLAM [79] and V ox-Fusion [69] we take the numbers from [78].",
        "19": "76) V ox-Fusion\u2217[69] 3.",
        "20": "70 V ox-Fusion [69] 8.",
        "21": "57 N/A V ox-Fusion\u2217[69] 68.",
        "22": "NICE-SLAM [79] and V ox-Fusion [69] which employ a large voxel size that leads to more averaging and a reduced sensitivity to specularities.",
        "23": "86 MB V ox-Fusion [69] 12 ms 55 ms 0."
      },
      "Nerf-loam: Neural implicit representation for large-scale incremental lidar odometry and mapping": {
        "authors": [
          "J Deng",
          "Q Wu",
          "X Chen",
          "S Xia",
          "Z Sun"
        ],
        "url": "http://openaccess.thecvf.com/content/ICCV2023/papers/Deng_NeRF-LOAM_Neural_Implicit_Representation_for_Large-Scale_Incremental_LiDAR_Odometry_and_ICCV_2023_paper.pdf",
        "ref_texts": "[45] Xingrui Y ang, Hai Li, Hongjia Zhai, Y uhang Ming, Y uqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking andmapping with voxel-based neural implicit representation. In2022 IEEE International Symposium on Mixed and Aug-mented Reality (ISMAR) , pages 499\u2013507, 2022.",
        "ref_ids": [
          "45"
        ],
        "1": "Recently, neural radiance fields (NeRF) [32] has shown promising potentials in representing 3D scenes implicitlyusing a neural network and parallelly pose tracking meth-ods [33, 51, 45].",
        "2": "Compared to the existing 3D representations, the success of neural implicit representation [1, 18, 32, 40, 50] for novelview synthesis attach great attention, and many research in-vestigates the possibility to use this concept realizing simul-taneous localization and mapping (SLAM) [42, 46, 27, 33,51, 45].",
        "3": "Although [45, 50] adoptan octree-based sparse grid with voxel embeddings and canbe applied in larger areas, the pre-allocated embeddings ortime-consuming loop to search the voxels is not available inoutdoor for both odometry and mapping.",
        "4": "Different from existing methods [45, 50], we treat the environments dif-ferently when optimizing the SDF values, e.",
        "5": "Although utilizing the code, the pre-allocate em-beddings [34, 45] or time-consuming one by one search inhash table [50] is not suitable for our task, especially whenit needs to retrieve hundreds of thousands of embeddingsfrom a hash table containing millions of entries."
      },
      "Pats: Patch area transportation with subdivision for local feature matching": {
        "authors": [
          "J Ni",
          "Y Li",
          "Z Huang",
          "H Li",
          "H Bao"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_PATS_Patch_Area_Transportation_With_Subdivision_for_Local_Feature_Matching_CVPR_2023_paper.pdf",
        "ref_texts": "[62] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "62"
        ],
        "1": "In the past decades, local feature matching [3, 40] has been widely used in a large number of applications such as structure from motion (SfM) [44, 64], simultaneous localization and mapping (SLAM) [30,36,62], visual localization [19,41], object pose estimation [22, 61], etc."
      },
      "Recent advances in 3d gaussian splatting": {
        "authors": [
          "T Wu",
          "YJ Yuan",
          "LX Zhang",
          "J Yang",
          "YP Cao"
        ],
        "url": "https://arxiv.org/pdf/2403.11134",
        "ref_texts": "[201] Yang X, Li H, Zhai H, Ming Y, Liu Y, Zhang G. Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, 499\u2013",
        "ref_ids": [
          "201"
        ],
        "1": "23 Vox-Fusion [201] 24.",
        "2": "[201] Yang X, Li H, Zhai H, Ming Y, Liu Y, Zhang G."
      },
      "Cp-slam: Collaborative neural point-based slam system": {
        "authors": [
          "J Hu",
          "M Mao",
          "H Bao",
          "G Zhang"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7c10e259c7e56fa218ee03d9ae7d728e-Paper-Conference.pdf",
        "ref_texts": "[44] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "44"
        ],
        "1": "Very recently, some methods [36,50,44] exploit the Neural Radiance Fields (NeRF) for dense visual SLAM in a rendering-based optimization framework showing appealing rendering quality in novel view.",
        "2": "Inspired by different representations of neural field including voxel grid [21] and point cloud [43], NICE-SLAM [50] and V ox-Fusion [44] chose voxel grid to perform tracking and mapping instead of a single neural network which is limited by expression ability and forgetting problem.",
        "3": "In the single-agent experiment, because we use the rendered loop-closure data, we primarily choose the state-of-the-art neural SLAM systems such as NICE-SLAM [50], V ox-Fusion [44] and ORB-SLAM3 [4] for comparison on the loop-closure dataset.",
        "4": "71 V ox-Fusion [44]RMSE [cm]\u2193 0.",
        "5": "V ox-Fusion [44] has incorporated an important modification, implementing a sparse grid that is tailored to the specific scene instead of a dense grid.",
        "6": "4 present a quantitative analysis of the geometric reconstruction produced by our proposed system in comparison to NICE-SLAM [50] and V ox-Fusion [44].",
        "7": "33 V ox-Fusion [44]Depth L1 [cm]\u2193 0.",
        "8": "88MB V ox-Fusion [44] 0."
      },
      "Mips-fusion: Multi-implicit-submaps for scalable and robust online neural rgb-d reconstruction": {
        "authors": [
          "Y Tang",
          "J Zhang",
          "Z Yu",
          "H Wang",
          "K Xu"
        ],
        "url": "https://arxiv.org/pdf/2308.08741",
        "ref_texts": "2022. Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation. arXiv preprint arXiv:2210.15858 (2022). Yijun Yuan and Andreas N\u00fcchter. 2022. An algorithm for the SE (3)-transformation on neural implicit maps for remapping functions. IEEE Robotics and Automation Letters 7, 3 (2022), 7763\u20137770. Jiazhao Zhang, Yijie Tang, He Wang, and Kai Xu. 2022. ASRO-DIO: Active Subspace Random Optimization Based Depth Inertial Odometry. IEEE Transactions on Robotics (2022). Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, and Kai Xu. 2020. Fusion-aware point convolution for online semantic 3d scene segmentation. In Proc. CVPR . 4534\u20134543. Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, and Kai Xu. 2021. ROSEFusion: random optimization for online dense reconstruction under fast camera motion. ACM Trans. on Graph. (SIGGRAPH) 40, 4 (2021), 1\u201317. Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Pollefeys. 2022. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 12786\u201312796. ACM Trans. Graph., Vol. 40, No. 4, Article 56. Publication date: August 2023.",
        "ref_ids": [
          "2022"
        ],
        "1": "[2022] propose a unique mapping scheme based on on-the-fly implicits of Hermite Radial Basis Functions (HRBFs) demonstrating good accuracy and robustness of RGB-D reconstruction.",
        "2": "[2022] propose to represent scene surface using an implicit TSDF and incorporate this representation in the NeRF framework for rendering-based learning.",
        "3": "[2022] propose a geometric and photometric 3D mapping pipeline from monocular images based on hierarchical volumetric neural radiance fields.",
        "4": "Yuan and N\u00fcchter [2022] propose an algorithm for the \ud835\udc46\ud835\udc38(3)-transformation of neural implicit maps for remapping in loop closure."
      },
      "Compact 3d gaussian splatting for dense visual slam": {
        "authors": [
          "T Deng",
          "Y Chen",
          "L Zhang",
          "J Yang",
          "S Yuan"
        ],
        "url": "https://arxiv.org/pdf/2403.11247",
        "ref_texts": "40. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507",
        "ref_ids": [
          "40"
        ],
        "1": "Vox-Fusion [40] employs octree architecture for dynamic map scalability.",
        "2": "R0 R1 R2 Of0 Of1 Of2 Of3 Of4 Vox-Fusion [40] 3.",
        "3": "0000 0059 0106 0169 0181 0207 Vox-Fusion [40] 26.",
        "4": "We also compared to other NeRF-based SLAM methods, such as NICE-SLAM [45], Co-SLAM [34], ESLAM [11], Vox-Fusion [40].",
        "5": "87 Vox-Fusion [40] 11.",
        "6": "as Vox-Fusion [40], NICE-SLAM [45], Co-SLAM [34], and ESLAM [11].",
        "7": "R0 R1 R2 Of0 Of1 Of2 Of3 Of4 Vox-Fusion [40]PSNR\u2191 SSIM\u2191 LPIPS \u219324."
      },
      "Learning neural implicit through volume rendering with attentive depth fusion priors": {
        "authors": [
          "P Hu",
          "Z Han"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/68637ee6b30276f900bc67320466b69f-Paper-Conference.pdf",
        "ref_texts": "[83] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , Dec 2022.",
        "ref_ids": [
          "83"
        ]
      },
      "UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM": {
        "authors": [
          "E Sandstr\u00f6m",
          "K Ta",
          "L Van Gool"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/papers/Sandstrom_UncLe-SLAM_Uncertainty_Learning_for_Dense_Neural_SLAM_ICCVW_2023_paper.pdf",
        "ref_texts": "[78] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking andmapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Aug-mented Reality (ISMAR), pages 499\u2013507. IEEE, 2022. 1, 6,7,8",
        "ref_ids": [
          "78"
        ],
        "1": "Introduction Neural scene representations have taken over the 3D reconstruction field by storm [47, 41,12,42] and have recently also been built into SLAM systems [67,81,78] with excellent results for geometric reconstruction, hole filling,and novel view synthesis.",
        "2": "However, their camera track-ing performance is typically inferior to the one of tradi-tional sparse methods [9] that rely on feature point matching [81, 78].",
        "3": "Currently, the majority of dense neural SLAM approaches employ a uniformweighting for all pixels during mapping [81, 78,37,80] and tracking [81, 78,67,80].",
        "4": "In the multi-sensor setting, we also compare to V oxFusion [78] by weighting all depth readings equally.",
        "5": "We compare to V ox-Fusion [78], a dense neural SLAM system and SenFuNet [58], which is a mapping only frame-work.",
        "6": "In Table 5we show for SGM+PSMNet fusion that we are able to consistentlyimprove over the single-sensor reconstructions in isolationand over SenFuNet [58] and V oxFusion [78].",
        "7": "22 V ox-Fusion [78] 6."
      },
      "H-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid Representation": {
        "authors": [
          "C Jiang",
          "H Zhang",
          "P Liu",
          "Z Yu",
          "H Cheng"
        ],
        "url": "https://arxiv.org/pdf/2306.03207",
        "ref_texts": "[15] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "15"
        ],
        "1": "Several works [13]\u2013[15] employ NeRF to overcome limitations associated with explicit representations and achieve better mapping results in various aspects.",
        "2": "Our method avoids redundant sample calculations across all keyframes [13] and ensures quality in marginal areas, without increasing the number of training samples [15].",
        "3": "Numerous studies [13]\u2013[15, 19] have been inspired by NeRF [12] and utilize implicit representation for incremental dense mapping.",
        "4": "V ox-Fusion [15], instead, only allocates voxels to the area containing the surface, forcing the network to learn more details in those regions.",
        "5": "V ox-Fusion [15] adds a new keyframe based on the ratio of newly allocated voxels to the currently observed voxels.",
        "6": "SDF-based Volume rendering Like V ox-Fusion [15], we only sample points along the ray that intersects with any voxel.",
        "7": "Optimization Process 1) Loss Function: We apply loss functions like V ox-Fusion [15]: RGB Loss (Lrgb), Depth Loss (Ld), Free Space Loss (Lfs) and SDF Loss (Lsd f) on a batch of rays R.",
        "8": "C OMPARED WITH NICE-SLAM [14] AND VOX-FUSION [15], OUR APPROACH YIELDS BETTER RESULTS IN ALL THE METRICS .",
        "9": "2) Baselines: We select two advanced NeRF-based dense RGB-D SLAM methods currently open-source, NICE-SLAM\n[14] and V ox-Fusion [15] for comparison.",
        "10": "[15] X."
      },
      "Neural implicit dense semantic slam": {
        "authors": [
          "Y Haghighi",
          "S Kumar",
          "JP Thiran",
          "L Van Gool"
        ],
        "url": "https://arxiv.org/pdf/2304.14560",
        "ref_texts": "[21] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.[22] J. L. Schonberger and J.-M. Frahm, \u201cStructure-from-motion revisited,\u201d inProceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4104\u20134113.",
        "ref_ids": [
          "21",
          "22"
        ],
        "1": "216V ox-Fusion [21]PSNR [dB]\u2191 23.",
        "2": "247COLMAP [22]PSNR [dB]\u2191 20.",
        "3": "[21] X.",
        "4": "[22] J."
      },
      "Rgb-d mapping and tracking in a plenoxel radiance field": {
        "authors": [
          "AL Teigen",
          "Y Park",
          "A Stahl"
        ],
        "url": "https://openaccess.thecvf.com/content/WACV2024/papers/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.pdf",
        "ref_texts": "[35] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 3, 6, 7, 8",
        "ref_ids": [
          "35"
        ],
        "1": "Due to NeRF\u2019s simple formulation of dense mapping and its small storage size, several authors have attempted to use NeRF as a map representation in dense SLAM algorithms [31, 35, 40].",
        "2": "V ox-Fusion [35] Figure 2.",
        "3": "Both of these subsets have been the standard for comparison in previous works [31, 35, 40].",
        "4": "Therefore, to draw comparisons with existing approaches, we select NICE-SLAM [40], and V ox-Fusion [35] as competing methods due to their status as state-of-the-art algorithms for radiance field-based simultaneous localization and mapping.",
        "5": ")(ms) V ox-Fusion [35]ATE [m]\u2193 0.",
        "6": "(m/pixel) \u2193 V ox-Fusion [35] 19.",
        "7": ")(ms) V ox-Fusion [35]ATE [m]\u2193 0."
      },
      "Gaussian splatting slam": {
        "authors": [
          "H Matsuki",
          "R Murai",
          "PHJ Kelly",
          "AJ Davison"
        ],
        "url": "https://arxiv.org/pdf/2312.06741",
        "ref_texts": "[45] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR) , 2022.",
        "ref_ids": [
          "45"
        ],
        "1": "In the RGBD case, we compare against neural-implicit SLAM methods [8, 9, 29, 35, 41, 45, 48] which are also map-centric, rendering-based and do not perform loop closure.",
        "2": "07 V ox-Fusion [45] 3.",
        "3": "07 V ox-Fusion [45] 1.",
        "4": "54 V ox-Fusion[45] 24.",
        "5": "For the RGB-D case, numbers for NICE-SLAM [48], DI-Fusion [8], V ox-Fusion [45], PointSLAM [29] are taken from Point-SLAM [29], and numbers for iMAP [35], BAD-SLAM [31], Kintinous [42], ORBSLAM [21] are from iMAP [35], and ald all the other baselines: ESLAM [9], Co-SLAM [41] are from each individual papers.",
        "6": "233 V ox-Fusion [45]PSNR[dB] \u2191 22.",
        "7": "85 V ox-Fusion [45] 2."
      },
      "Ro-map: Real-time multi-object mapping with neural radiance fields": {
        "authors": [
          "X Han",
          "H Liu",
          "Y Ding",
          "L Yang"
        ],
        "url": "https://arxiv.org/pdf/2304.05735",
        "ref_texts": "[31] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "31"
        ],
        "1": "Subsequent works [29], [30] have made further improvements, including the integration with traditional voxel grids [31] and different shape representations [32].",
        "2": "[31] X."
      },
      "Loner: Lidar only neural representations for real-time slam": {
        "authors": [
          "S Isaacson",
          "PC Kung",
          "M Ramanagopal"
        ],
        "url": "https://arxiv.org/pdf/2309.04937",
        "ref_texts": "[22] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "22"
        ],
        "1": "Recently, several more papers have introduced architectures and encodings to improve neuralimplicit SLAM\u2019s memory efficiency, computation speed, and accuracy [19, 20, 21, 22].",
        "2": "[22] X."
      },
      "PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency": {
        "authors": [
          "Y Pan",
          "X Zhong",
          "L Wiesmann",
          "T Posewsky"
        ],
        "url": "https://arxiv.org/pdf/2401.09101",
        "ref_texts": "[101] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proc. of the Intl. Symposium on Mixed and Augmented Reality (ISMAR) , 2022.",
        "ref_ids": [
          "101"
        ],
        "1": "Consequently, several mapping and SLAM systems based on neural implicit representation have been proposed, mainly for RGB-D cameras operating indoor [29], [71], [79], [91], [101], [112] but also for LiDAR sensors operating outdoor [12], [111].",
        "2": "In the realm of mapping and SLAM from a stream of RGB-D data, several works propose to use a single MLP [2], [55], [79] and a scalable hybrid representations combining dense or sparse local latent features and a shallow MLP [26], [29], [71], [91], [92], [101], [112] to model the geometry or radiance field of the scene and optionally track the camera within the scene.",
        "3": "They use an octree-based feature grid [101], [111].",
        "4": "The de facto optimization target in previous RGB-D based neural implicit SLAM approaches [71], [79], [101], [112] is the depth rendering loss, which can be seen as related to the point-to-point metric with projection-based data association.",
        "5": "05 V ox-Fusion [101] 1.",
        "6": "[101] X."
      },
      "Gs-slam: Dense visual slam with 3d gaussian splatting": {
        "authors": [
          "C Yan",
          "D Qu",
          "D Wang",
          "D Xu",
          "Z Wang",
          "B Zhao"
        ],
        "url": "https://arxiv.org/pdf/2311.11700",
        "ref_texts": "[48] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. ISMAR , pages 499\u2013507, 2022. 2, 5, 6, 7, 8",
        "ref_ids": [
          "48"
        ],
        "1": "For example, NICE-SLAM [55] integrates MLPs with multiresolution voxel grids, enabling large scene reconstruction, and V ox-Fusion [48] employs octree expansion for dynamic map scalability, while ESLAM [11] and Point-SLAM [27] utilize tri-planes and neural point clouds respectively to improve the mapping capability.",
        "2": "Following [11, 27, 41, 48, 55], we use 8 scenes from the Replica dataset for localization, mesh reconstruction, and rendering quality comparison.",
        "3": "We compare our method with existing SOTA NeRF-based dense visual SLAM: NICE-SLAM [55], V oxFusion [48], CoSLAM [41], ESLAM [11] and PointSLAM [27].",
        "4": "Our method surpasses iMAP [35], NICE-SLAM [55] and V oxfusion [48], and achieves a comparable performance, average 3.",
        "5": "06 V ox-Fusion\u2217[48] 1.",
        "6": "3 V ox-Fusion\u2217[48] 3.",
        "7": "86 V oxFus ion [48]Depth L1 \u21931.",
        "8": "It is noticeable that GS-SLAM achieves 386 FPS rendering speed on average, which is 100\u00d7faster than the second-best method V ox-Fusion [48].",
        "9": "Room 0 Room 1 Room 2 Office 3NICE-SLAM [55]\n V ox-Fusion [48]\n CoSLAM [41]\n ESLAM [11]\n Ours Ground Truth Figure 4.",
        "10": "48 MB V ox-Fusion [48] 0.",
        "11": "233 V ox-Fusion\u2217[48]PSNR [dB] \u219122.",
        "12": "front-to-back order rather than the volume rendering technique used by current NeRF-Based SLAM [11, 27, 35, 41, 48, 53, 55]."
      },
      "NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields": {
        "authors": [
          "J Naumann",
          "B Xu",
          "S Leutenegger",
          "X Zuo"
        ],
        "url": "https://arxiv.org/pdf/2312.13471",
        "ref_texts": "[51] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and 10 mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507, 2022. 2, 3",
        "ref_ids": [
          "51"
        ],
        "1": "Lately, numerous works have aimed at integrating SLAM with neural implicit mapping [24, 34, 38, 45, 51, 57].",
        "2": "Subsequent works aimed at enhancing the scene representation [34, 45, 51], introducing implicit semantic encoding [24], and integrating inertial measurements [22]."
      },
      "Sni-slam: Semantic neural implicit slam": {
        "authors": [
          "S Zhu",
          "G Wang",
          "H Blum",
          "J Liu",
          "L Song"
        ],
        "url": "https://arxiv.org/pdf/2311.11016",
        "ref_texts": "[50] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 2, 3, 4, 6, 7",
        "ref_ids": [
          "50"
        ],
        "1": "Following the advantages of implicit representation, NeRF-based SLAM [16, 40, 46, 50, 54] methods have been developed.",
        "2": "V ox-Fusion [50] is based on octree management for incremental mapping.",
        "3": "We utilize feature planes [16] to store features, which saves storage space compared with voxel grid [50, 54].",
        "4": "Another approach [50] utilizes the decoder network to obtain geometric and color information from a single feature.",
        "5": "503 V ox-Fusion [50] 2.",
        "6": "For SLAM accuracy, we compare our method with state-of-the-art NeRF-based dense visual SLAM methods [16, 37, 40, 46, 50, 54].",
        "7": "V ox-Fusion [50] achieves the highest Accuracy (cm) because it only reconstructs observed areas and ignores errors in predicted unseen regions, but this strategy results in nearly worst Completion (cm) andCompletion ratio (%) metrics compared with other NeRF-SLAM methods.",
        "8": "69 V ox-Fusion [50] 8.",
        "9": "87 V ox-Fusion [50] 3.",
        "10": "2M V ox-Fusion [50] 2.",
        "11": "Following previous methods [16, 46, 50, 54], we evaluate tracking accuracy on the ScanNet dataset [5]."
      },
      "A review of visual SLAM for robotics: evolution, properties, and future applications": {
        "authors": [
          "B Al-Tawil",
          "T Hempel",
          "A Abdelrahman"
        ],
        "url": "https://www.frontiersin.org/articles/10.3389/frobt.2024.1347985/pdf",
        "ref_texts": "24, 7048\u20137060. doi: 10.1109/tits.2023.3258526 Wu, W., Guo, L., Gao, H., You, Z., Liu, Y., and Chen, Z. (2022). Yolo-slam: a semantic slam system towards dynamic environment with geometric constraint. NeuralComput. Appl.34, 6011\u20136026. doi: 10.1007/s00521-021-06764-3 Xiao, L., Wang, J., Qiu, X., Rong, Z., and Zou, X. (2019). Dynamic-slam: semantic monocular visual localization and mapping based on deep learning in dynamic environment. RoboticsAut.Syst. 117, 1\u201316. doi: 10.1016/j.robot.2019.03.012 Xu, C., Liu, Z., and Li, Z. (2021). Robust visual-inertial navigation system for low precision sensors under indoor and outdoor environments. Remote Sens. 13, 772. doi:10.3390/rs13040772 Yan, L., Hu, X., Zhao, L., Chen, Y., Wei, P., and Xie, H. (2022). Dgs-slam: a fast and robust rgbd slam in dynamic environments combined by geometric and semantic information. RemoteSens. 14, 795. doi: 10.3390/rs14030795 Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., and Zhang, G. (2022). \u201cVox-fusion: dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (IEEE), 499\u2013507. Yousif, K., Bab-Hadiashar, A., and Hoseinnezhad, R. (2015). An overview to visual odometry and visual slam: applications to mobile robotics. Intell.Ind.Syst. 1, 289\u2013311. doi:10.1007/s40903-015-0032-7 Zang, Q., Zhang, K., Wang, L., and Wu, L. (2023). An adaptive orb-slam3 system for outdoor dynamic environments. Sensors 23, 1359. doi: 10.3390/s23031359 Zhang, J., Zhu, C., Zheng, L., and Xu, K. (2021a). Rosefusion: random optimization for online dense reconstruction under fast camera motion. ACMTrans.Graph.(TOG)"
      },
      "Semgauss-slam: Dense semantic gaussian splatting slam": {
        "authors": [
          "S Zhu",
          "R Qin",
          "G Wang",
          "J Liu",
          "H Wang"
        ],
        "url": "https://arxiv.org/pdf/2403.07494",
        "ref_texts": "[31] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "31"
        ],
        "1": "Following this, several works [31,32,33,34,35,36,37] introduce more efficient scene representation, such as hash-based feature grid and feature plane, to achieve more accurate SLAM performance.",
        "2": "We compare our method with the existing state of-the-art dense visual SLAM, including NeRF-based SLAM [30,33,32,34,31] and 3DGS-based SLAM [17].",
        "3": "233 V ox-Fusion [31] 2.",
        "4": "30 V ox-Fusion [31] 68."
      },
      "Gaussian-slam: Photo-realistic dense slam with gaussian splatting": {
        "authors": [
          "V Yugay",
          "Y Li",
          "T Gevers",
          "MR Oswald"
        ],
        "url": "https://arxiv.org/pdf/2312.10070",
        "ref_texts": "78. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE",
        "ref_ids": [
          "78"
        ],
        "1": "These efforts have led to the development of comprehensive dense SLAM systems [34,54,63,78,85,88,89], showing a trend in the pursuit of precise and reliable visual SLAM.",
        "2": "They further divide into methods using dense grids [3,9,11,28,44,64,70\u201373,86,87,89], hierarchical octrees [6,30,31,36,57,78] and voxel hashing [13,20,41,46,67] for efficient memory management.",
        "3": "We primarily compare our method to existing state-ofthe-art dense neural RGBD SLAM methods such as NICE-SLAM [89], VoxFusion [78], ESLAM [34], and Point-SLAM [53].",
        "4": "233 Vox-Fusion [78]PSNR\u219122.",
        "5": "441 Vox-Fusion [78]PSNR\u2191 15.",
        "6": "548 Vox-Fusion [78]PSNR\u219119.",
        "7": "7 we compare our method to NICESLAM [89], Vox-Fusion [78], ESLAM [34], Point-SLAM [53], and concurrent SplaTAM [23] in terms of the geometric reconstruction accuracy on the Replica dataset [59].",
        "8": "95 Vox-Fusion [78] 0.",
        "9": "3 Vox-Fusion [78] 3.",
        "10": "70 Vox-Fusion [78] 68.",
        "11": "9 Vox-Fusion [78]Depth L1 [cm] \u21931.",
        "12": "64 Vox-Fusion [78] 98 1."
      },
      "A survey on 3d gaussian splatting": {
        "authors": [
          "G Chen",
          "W Wang"
        ],
        "url": "https://arxiv.org/pdf/2401.03890",
        "ref_texts": "[196] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, \u201cVoxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "196"
        ],
        "1": "58 Vox-Fusion [196] [ISMAR22] 1.",
        "2": "23 Vox-Fusion [196] [ISMAR22]PSNR \u2191 22.",
        "3": "\u2022Benchmarking Algorithms: For performance comparison, we involve four recent papers which introduce 3D Gaussians into their systems [116]\u2013[119], as well as three dense SLAM methods [196], [197], [199].",
        "4": "[196] X."
      },
      "Swift-Mapping: Online Neural Implicit Dense Mapping in Urban Scenes": {
        "authors": [
          "K Wu",
          "K Zhang",
          "M Gao",
          "J Zhao",
          "Z Gan"
        ],
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/28420/28820",
        "ref_texts": "6055 Roessle, B.; Barron, J. T.; Mildenhall, B.; Srinivasan, P. P.; and Nie\u00dfner, M. 2022. Dense depth priors for neural radiance fields from sparse input views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12892\u201312901. Rudnev, V.; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V.; and Theobalt, C. 2022. NeRF for Outdoor Scene Relighting. InEuropean Conference on Computer Vision (ECCV). Straub, J.; Whelan, T.; Ma, L.; Chen, Y.; Wijmans, E.; Green, S.; Engel, J. J.; Mur-Artal, R.; Ren, C.; Verma, S.; Clarkson, A.; Yan, M.; Budge, B.; Yan, Y.; Pan, X.; Yon, J.; Zou, Y.; Leon, K.; Carter, N.; Briales, J.; Gillingham, T.; Mueggler, E.; Pesqueira, L.; Savva, M.; Batra, D.; Strasdat, H. M.; Nardi, R. D.; Goesele, M.; Lovegrove, S.; and Newcombe, R. 2019. The Replica Dataset: A Digital Replica of Indoor Spaces. arXiv preprint arXiv:1906.05797. Sucar, E.; Liu, S.; Ortiz, J.; and Davison, A. J. 2021. iMAP: Implicit mapping and positioning in real-time. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 6229\u20136238. Sun, C.; Sun, M.; and Chen, H.-T. 2022. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5459\u20135469. Takikawa, T.; Litalien, J.; Yin, K.; Kreis, K.; Loop, C.; Nowrouzezahrai, D.; Jacobson, A.; McGuire, M.; and Fidler, S. 2021. Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. Tancik, M.; Casser, V.; Yan, X.; Pradhan, S.; Mildenhall, B.; Srinivasan, P. P.; Barron, J. T.; and Kretzschmar, H. 2022. Block-nerf: Scalable large scene neural view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8248\u20138258. Xie, Z.; Zhang, J.; Li, W.; Zhang, F.; and Zhang, L. 2023. Snerf: Neural radiance fields for street views. arXiv preprint arXiv:2303.00749. Yang, X.; Li, H.; Zhai, H.; Ming, Y.; Liu, Y.; and Zhang, G. 2022. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 499\u2013507. IEEE. Yu, A.; Li, R.; Tancik, M.; Li, H.; Ng, R.; and Kanazawa, A. 2021. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 5752\u20135761. Zeng, C.; Chen, G.; Dong, Y.; Peers, P.; Wu, H.; and Tong, X. 2023. Relighting Neural Radiance Fields with Shadow and Highlight Hints. In ACM SIGGRAPH 2023 Conference Proceedings, 1\u201311. Zhang, X.; Bi, S.; Sunkavalli, K.; Su, H.; and Xu, Z. 2022. Nerfusion: Fusing radiance fields for large-scale scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5449\u20135458. Zhang, X.; Srinivasan, P. P.; Deng, B.; Debevec, P.; Freeman, W. T.; and Barron, J. T. 2021. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 40(6): 1\u201318.Zhang, Y.; Guo, X.; Poggi, M.; Zhu, Z.; Huang, G.; and Mattoccia, S. 2023. Completionformer: Depth completion with convolutions and vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18527\u201318536. Zhu, Z.; Peng, S.; Larsson, V.; Xu, W.; Bao, H.; Cui, Z.; Oswald, M. R.; and Pollefeys, M. 2022. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12786\u201312796. The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)"
      },
      "Ngel-slam: Neural implicit representation-based global consistent low-latency slam system": {
        "authors": [
          "Y Mao",
          "X Yu",
          "K Wang",
          "Y Wang",
          "R Xiong"
        ],
        "url": "https://arxiv.org/pdf/2311.09525",
        "ref_texts": "[29] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "29"
        ],
        "1": "In robotics,neural implicit representations have been used for object tracking and SLAM, enabling the construction of environment maps and estimation of robot or camera positions [5], [4], [27], [28], [29], [30], [31], which are closely related to our work.",
        "2": "[29] X."
      },
      "NeRF in Robotics: A Survey": {
        "authors": [
          "G Wang",
          "L Pan",
          "S Peng",
          "S Liu",
          "C Xu",
          "Y Miao"
        ],
        "url": "https://arxiv.org/pdf/2405.01333",
        "ref_texts": "[22] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in ISMAR , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "22"
        ],
        "1": "V ox-Fusion [22] uses a treelike structure to store grid embeddings, allowing the dynamic allocation of new spatial voxels as the scene expands.",
        "2": "V oxFusion [22] employs voxel feature embedding as input, generating RGB and SDF values as output.",
        "3": "[22] X."
      },
      "Towards open world nerf-based slam": {
        "authors": [
          "D Lisus",
          "C Holmes",
          "S Waslander"
        ],
        "url": "https://arxiv.org/pdf/2301.03102",
        "ref_texts": "[18] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV ox-Fusion: Dense Tracking and Mapping with V oxel-based Neural Implicit Representation,\u201d in IEEE Int. Symp. Mixed Augmented Reality (ISMAR) , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "18"
        ],
        "1": "One future avenue of interest is to leverage ideas in [18] to avoid using a predefined grid.",
        "2": "[18] X."
      },
      "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "Y Li",
          "X Zhang",
          "B Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2404.02152",
        "ref_texts": "[62] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 2",
        "ref_ids": [
          "62"
        ],
        "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
      },
      "Neural Radiance Field in Autonomous Driving: A Survey": {
        "authors": [
          "L He",
          "L Li",
          "W Sun",
          "Z Han",
          "Y Liu",
          "S Zheng"
        ],
        "url": "https://arxiv.org/pdf/2404.13816",
        "ref_texts": "[60] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV ox-fusion: Dense tracking and mapping with voxelbased neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "60"
        ],
        "1": "Furthermore, V ox-Fusion[60] incrementally allocates voxels by an octree-based structure without a pre-trained geometry decoder and proposes a keyframe selection strategy suitable for sparse voxels, resulting in better performance than NICE-SLAM on the Replica dataset in both tracking and mapping.",
        "2": "[60] X."
      },
      "Towards real-time scalable dense mapping using robot-centric implicit representation": {
        "authors": [
          "J Liu",
          "H Chen"
        ],
        "url": "https://arxiv.org/pdf/2306.10472",
        "ref_texts": ""
      },
      "Q-slam: Quadric representations for monocular slam": {
        "authors": [
          "C Peng",
          "C Xu",
          "Y Wang",
          "M Ding",
          "H Yang"
        ],
        "url": "https://arxiv.org/pdf/2403.08125",
        "ref_texts": "44. Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022)",
        "ref_ids": [
          "44"
        ],
        "1": "87 Vox-Fusion [44]\n(GT depth)PSNR \u2191 22.",
        "2": "92s V ox-Fusion [44] 12.",
        "3": "It can be observed that our method can achieve comparable speed with V ox-Fusion [44], but providing much higher rendering quality as shown in Tab 1."
      },
      "NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM": {
        "authors": [
          "W Guo",
          "B Wang",
          "L Chen"
        ],
        "url": "https://arxiv.org/pdf/2402.02020",
        "ref_texts": "[17] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "17"
        ],
        "1": "Recent research, as indicated in references [17], [18], has experimented with the use of voxel representations for scene depiction and dynamic management through octree structures to achieve real-time expansion and reconstruction of unknownarXiv:2402.",
        "2": "For instance, the V ox-Fusion [17] method attempts to store neural features directly in voxel vertices to accelerate the optimization process of the scene, but this approach still relies on larger-scale neural networks for accurate scene representation.",
        "3": "V ox-Fusion [17] integrates neural implicit representations with traditional volume fusion methods, utilizing voxel-based neural implicit surface representations to encode and optimize scenes within each voxel.",
        "4": "Multiresolution Voxel Generation and Management 1) Generation: Contrary to the [14], [17] approach, which employs a single-resolution voxel, we utilize multiresolution voxels based on scene details to represent the scene.",
        "5": "2) Neural Multiresolution Voxel Representation: Diverging from V ox-Fusion [17] and DVGO [74] methodologies, our voxel grid representation employs trilinear interpolation for simultaneous modeling of SDF and color features within voxel cells, which enhances precision in querying any space position, significantly boosting scene convergence efficiency: interp (x,V(D),V(S)) :x\u2208R3,V(D),V(S)\u2208RA\u00d7N.",
        "6": "(5) In contrast to existing methods like Point-SLAM [19] and V ox-Fusion [17], which rely on neural networks to concurrently regress both identity and RGB values during their processing phases, this study introduces a more rapid and streamlined implicit representation approach, named VDF .",
        "7": "(8) Then, we adopt a volumetric rendering technique similar to the V ox-Fusion [17] to compute and render depth Dand color JOURNAL OF L ATEX CLASS FILES, VOL.",
        "8": "2) Baseline: For our comparative analysis, we employed iMAP [13], V ox-Fusion [17], NICE-SLAM [14], and DIFusion [76] as our baseline methods.",
        "9": "In addition, The codes for tracking and mapping evaluation are both from V ox-Fusion [17].",
        "10": "THE DATA OF I MAP, V OX-FUSION ARE FROM [17], V OX-FUSION *AND NICE-SLAM ARE IMPLEMENTED BY THE OPEN -SOURCE CODE .",
        "11": "We conducted a quantitative comparison of our system with NICE-SLAM [14] and V ox-Fusion [17].",
        "12": "THE DATA OF I MAP, NICE-SLAM, AND VOX-FUSION ARE FROM [17].",
        "13": "Mapping We performed a qualitative comparison of our system with NICE-SLAM [14] and V ox-Fusion [17].",
        "14": "THE DATA OF VOX-FUSION ARE FROM [17].",
        "15": "THE DATA OF NICE-SLAM AND VOX-FUSION ARE FROM [17].",
        "16": "[17] X."
      },
      "Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping": {
        "authors": [
          "T Hua",
          "H Bai",
          "Z Cao",
          "M Liu",
          "D Tao",
          "L Wang"
        ],
        "url": "https://arxiv.org/pdf/2401.03203",
        "ref_texts": "[14] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "14"
        ],
        "1": "With the advent of Neural Radiance Fields (NeRF) [11], several research attempts [12], [13], [14], [15], [16] leverage neural field to better represent the scene by encoding the appearance and geometry in a compact and learnable way, benefiting both memory consumption and mapping quality.",
        "2": "The Neural Radiance Field [11], a novel approach rooted in Implicit Neural Representation (INR) combined with volume rendering techniques, has inspired substantial implicit dense mapping [12], [13], [14], [15], [16], [22], [23], [24], [25], [26], resulting in higher reconstruction quality with more compact representation.",
        "3": ", scalability and computational efficiency [13], [14], [15], [16].",
        "4": "01245\n[14] X."
      },
      "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM": {
        "authors": [
          "M Li",
          "S Liu",
          "H Zhou"
        ],
        "url": "https://arxiv.org/pdf/2402.03246",
        "ref_texts": "41. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022)",
        "ref_ids": [
          "41"
        ],
        "1": "Theseexperimentscompareour method against both implicit NeRF-based approaches [15,37,41,47], and novel 3D-Gaussian-based methods [16], evaluating performance in mapping, tracking, and semantic segmentation.",
        "2": "We compared our method with Vox-Fusion [41], NICE-SLAM [47], Co-SLAM [37], ESLAM [15], and Point-SLAM [33] for ATE RMSE evaluation."
      },
      "Implicit Event-RGBD Neural SLAM": {
        "authors": [
          "D Qu",
          "C Yan",
          "D Wang",
          "J Yin",
          "D Xu",
          "B Zhao"
        ],
        "url": "https://arxiv.org/pdf/2311.11013",
        "ref_texts": "[72] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In ISMAR , pages 499\u2013507. IEEE, 2022. 2",
        "ref_ids": [
          "72"
        ],
        "1": "V ox-Fusion [72] utilizes an octreebased structure to expand the scene dynamically."
      },
      "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap": {
        "authors": [
          "M Li",
          "J Huang",
          "L Sun",
          "AX Tian",
          "T Deng"
        ],
        "url": "https://arxiv.org/pdf/2405.05702",
        "ref_texts": ""
      },
      "DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding": {
        "authors": [
          "M Li",
          "J He",
          "G Jiang",
          "H Wang"
        ],
        "url": "https://arxiv.org/pdf/2401.01545",
        "ref_texts": ""
      },
      "Nid-slam: Neural implicit representation-based rgb-d slam in dynamic environments": {
        "authors": [
          "Z Xu",
          "J Niu",
          "Q Li",
          "T Ren",
          "C Chen"
        ],
        "url": "https://arxiv.org/pdf/2401.01189",
        "ref_texts": "[13] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022.",
        "ref_ids": [
          "13"
        ],
        "1": "Previous methods [2], [5], [13]\u2013\n[16] have demonstrated the feasibility of using neural networks to model the color and geometric information of static scenes; however, they have not fully exploited the potential of neural implicit representations in dynamic environments.",
        "2": "[13] X."
      },
      "Just flip: Flipped observation generation and optimization for neural radiance fields to cover unobserved view": {
        "authors": [
          "M Lee",
          "K Kang",
          "H Yu"
        ],
        "url": "https://arxiv.org/pdf/2303.06335",
        "ref_texts": "[9] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "9"
        ],
        "1": "Due to these features, many recent works [6], [7], [8], [9], [10] have applied NeRF to SLAM and 3D mapping.",
        "2": "To address these limitations, recent research [6], [7], [8], [9], [10] has applied NeRF to SLAM for 3D mapping.",
        "3": "[9] X."
      },
      "SimpleMapping: Real-time visual-inertial dense mapping with deep multi-view stereo": {
        "authors": [
          "Y Xin",
          "X Zuo",
          "D Lu"
        ],
        "url": "https://arxiv.org/pdf/2306.08648",
        "ref_texts": "[60] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang. V ox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pp. 499\u2013507, 2022.",
        "ref_ids": [
          "60"
        ],
        "1": "Furthermore, we showcase the comparable reconstruction performance of SimpleMapping utilizing only a monocular camera setup without IMU, against a state-of-theart RGB-D dense SLAM method with neural implicit representation, V ox-Fusion [60].",
        "2": "Furthermore, we evaluate our approach against V ox-Fusion [60], a RGB-D based dense tracking and mapping system using a voxel based neural implicit representation, on ScanNet [6] test set.",
        "3": "V oxFusion [60] optimizes feature embeddings in voxels and the weights of a MLP decoder on-the-fly with intensive computation, while ours relying on offline training of the MVS network is much more efficient at inference stage.",
        "4": "As shown in Table 6 and Figure 6, our approach consistently outperforms TANDEM [20] and exhibits competitive performance compared to the RGB-D method, V ox-Fusion [60].",
        "5": "Note that Vox-Fusion [60] takes RGB-D inputs.",
        "6": "10 V ox-Fusion [60] 2.",
        "7": "84 V ox-Fusion [60] 18.",
        "8": "60 V ox-Fusion [60] 47.",
        "9": "30 V ox-Fusion [60] 60.",
        "10": "Vox-Fusion [60] tends to produce over-smoothed geometries and experience drift during long-time tracking, resulting in inconsistent reconstruction, as observed in Scene0787.",
        "11": "For example, when applying V ox-Fusion [60] to the ScanNet dataset [6], each frame\u2019s average processing time for tracking amounts to 2.",
        "12": "[60] X."
      },
      "SLAM Meets NeRF: A Survey of Implicit SLAM Methods": {
        "authors": [
          "K Yang",
          "Y Cheng",
          "Z Chen",
          "J Wang"
        ],
        "url": "https://www.mdpi.com/2032-6653/15/3/85/pdf",
        "ref_texts": "35. Yang, X.; Li, H.; Zhai, H.; Ming, Y.; Liu, Y.; Zhang, G. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proceedings of the 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Singapore, 17\u201321 October 2022; pp. 499\u2013507.",
        "ref_ids": [
          "35"
        ],
        "1": "To address this problem, Vox-Fusion [35] dynamically allocates new voxels by using an explicit octree structure and encodes the voxel coordinates by Morton coding to improve the voxel retrieval speed.",
        "2": "Method Name YearUtilized Sensors Decoded Parameters RGB-D RGB LiDAR SDF Density Color NICE-SLAM [11] 2022 \u2713 \u2713 \u2713 Vox-Fusion [35] 2022 \u2713 \u2713 \u2713 NICER-SLAM [34] 2023 \u2713 \u2713 \u2713 Co-SLAM [12] 2023 \u2713 \u2713 \u2713 LONER [38] 2023 \u2713 \u2713 \u2713 Shine-mapping [39] 2023 \u2713 \u2713 \u2713 NF-Atlas [41] 2023 \u2713 \u2713 \u2713 LODE [42] 2023 \u2713 \u2713 \u2713 NeRF-LOAM [45] 2023 \u2713 \u2713 \u2713 LocNDF [11] 2023 \u2713 \u2713 \u2713 C."
      },
      "Continuous Pose for Monocular Cameras in Neural Implicit Representation": {
        "authors": [
          "Q Ma",
          "DP Paudel",
          "A Chhatkuli",
          "L Van Gool"
        ],
        "url": "https://arxiv.org/pdf/2311.17119",
        "ref_texts": "[61] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 8",
        "ref_ids": [
          "61"
        ],
        "1": "7 Method Rm 0 Rm 1 Rm 2 Off 0 Off 1 Off 2 Off 3 Off 4 Avg V ox-Fusion* [61] 1.",
        "2": "89 V ox-Fusion* [61] 68.",
        "3": "1 V ox-Fusion* [61] 3."
      },
      "Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians": {
        "authors": [
          "E Sandstr\u00f6m",
          "K Tateno",
          "M Oechsle",
          "M Niemeyer"
        ],
        "url": "https://arxiv.org/pdf/2405.16544",
        "ref_texts": "[72] Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022)",
        "ref_ids": [
          "72"
        ],
        "1": "1 Introduction A common factor within the recent trend of dense SLAM is that the majority of works reconstruct a dense map by optimizing a neural implicit encoding of the scene, either as weights of an MLP [1, 57,39,45], as features anchored in dense grids [82,42,66,67,58,3,29,83,51], using hierarchical octrees [72], via voxel hashing [79,78,8,49,40], point clouds [18,50,30,75] or axis-aligned feature planes [33,47].",
        "2": "Enhancements like voxel hashing [43,23,44,11,40] and octrees [53,72,37,5,31] improved scalability, while point-based SLAM [68,52,4,23,25,6,76,50,30,75] has also been effective.",
        "3": "10891 (2024)\n[72] Yang, X."
      },
      "NeRF-Guided Unsupervised Learning of RGB-D Registration": {
        "authors": [
          "Z Yu",
          "Z Qin",
          "Y Tang",
          "Y Wang",
          "R Yi",
          "C Zhu"
        ],
        "url": "https://arxiv.org/pdf/2405.00507",
        "ref_texts": "33. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 5",
        "ref_ids": [
          "33"
        ],
        "1": "3 Pose Optimization in Neural SLAM Existing Neural SLAM methods [19,26,28,30,33,40,41] incorporate neural implicit representations into RGB-D SLAM systems, allowing tracking and mapping from scratch.",
        "2": "In the subsequent works, NICESLAM [41] and Vox-Fusion [33] introduce a hybrid representation that combines learnable grid-based features with a neural decoder, enabling the utilization of local scene color and geometry to guide pose optimization."
      },
      "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview": {
        "authors": [
          "Y Ming",
          "X Yang",
          "W Wang",
          "Z Chen",
          "J Feng"
        ],
        "url": "https://arxiv.org/pdf/2405.05526",
        "ref_texts": "[196] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, G. Zhang, Voxfusion:Densetrackingandmappingwithvoxel-basedneuralimplicit Ming et al.: Preprint submitted to Elsevier Page 29 of 32 Benchmarking NeRF for Autonomous Robots representation, in: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "196"
        ],
        "1": "Vox-Fusion [196] proposes a hybrid SLAM system that blends voxel-based mapping with neural implicit networks for efficient, detailed environment reconstruction using SDF representation.",
        "2": "66 Vox-Fusion [196] 2.",
        "3": "52 Vox-Fusion [196] 0.",
        "4": "97 Vox-Fusion [196] 8.",
        "5": "[196] X."
      },
      "CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field": {
        "authors": [
          "J Hu",
          "X Chen",
          "B Feng",
          "G Li",
          "L Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2403.16095",
        "ref_texts": "53. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 4, 10, 11",
        "ref_ids": [
          "53"
        ],
        "1": "NICE-SLAM [58] chose a fully covered voxel grid to store neural features, while Vox-Fusion [53] further improved this grid to an adaptive size.",
        "2": "We primarily consider state-of-the-art NeRF-SLAM works, includingNICE-SLAM[58],Co-SLAM[47],Point-SLAM[34],andVox-Fusion[53], as baselines.",
        "3": "\"-\" indicates failure results in Vox-Fusion [53].",
        "4": "4, we quantitatively measure the mapping performance of our proposed system, in comparison to NICE-SLAM [58], Co-SLAM [47], Point-SLAM [34], and Vox-Fusion [53]."
      },
      "Enhancing Vehicle Aerodynamics with Deep Reinforcement Learning in Voxelised Models": {
        "authors": [
          "J Patel",
          "Y Spyridis",
          "V Argyriou"
        ],
        "url": "https://arxiv.org/pdf/2405.11492",
        "ref_texts": "[12] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "12"
        ],
        "1": "Beyond CFD, voxelisation has found applications in 3D city modeling [11], virtual/augmented reality [12], and 3D printing [13].",
        "2": "[12] X."
      },
      "TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting for Enhanced SLAM": {
        "authors": [
          "P Jiang",
          "H Liu",
          "X Li",
          "T Wang",
          "F Zhang"
        ],
        "url": "https://arxiv.org/pdf/2405.19614",
        "ref_texts": "[37] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In ISMAR , pages 499\u2013507, 2022.",
        "ref_ids": [
          "37"
        ],
        "1": "39 V ox-Fusion [37] 3."
      },
      "Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural Radiance Fields": {
        "authors": [
          "S Lee",
          "K Kang",
          "H Yu"
        ],
        "url": "https://arxiv.org/pdf/2404.06727",
        "ref_texts": "42. Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 3",
        "ref_ids": [
          "42"
        ],
        "1": "NeRF\u2019s application scope has also expanded, encompassing areas such as scene editing [35,44], converting text to 3D models [15,23], and enhancing visual scene-based SLAM technologies [12,24,42,48], demonstrating its versatility and potential in various domains."
      },
      "Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting": {
        "authors": [
          "X Lang",
          "L Li",
          "H Zhang",
          "F Xiong",
          "M Xu",
          "Y Liu"
        ],
        "url": "https://arxiv.org/pdf/2404.06926",
        "ref_texts": "[8] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang. \u201cV ox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation\u201d. In:2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE. 2022, pp. 499\u2013507.",
        "ref_ids": [
          "8"
        ],
        "1": "Further, V ox-Fusion [8] utilizes octree to dynamically expand the volumetric neural implicit map, eliminating the need for pre-allocated grids.",
        "2": "[8] X."
      },
      "Rgbd gs-icp slam": {
        "authors": [
          "S Ha",
          "J Yeon",
          "H Yu"
        ],
        "url": "https://arxiv.org/pdf/2403.12550",
        "ref_texts": "45. Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 1",
        "ref_ids": [
          "45"
        ],
        "1": "Various approaches [8, 18, 32, 39, 45, 47] have been attempted to utilize INR for the real-time SLAM mapping process."
      },
      "3QFP: Efficient neural implicit surface reconstruction using Tri-Quadtrees and Fourier feature Positional encoding": {
        "authors": [
          "S Sun",
          "M Mielle",
          "AJ Lilienthal",
          "M Magnusson"
        ],
        "url": "https://arxiv.org/pdf/2401.07164",
        "ref_texts": "[23] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. \u201cV ox-Fusion: Dense Tracking and Mapping with V oxel-based Neural Implicit Representation\u201d. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . 2022, pp. 499\u2013507. DOI:10 . 1109 / ISMAR55827.2022.00066 .",
        "ref_ids": [
          "23"
        ],
        "1": "Instead of storing features in 3D voxel grids [20, 23, 24] or dense feature planes [21], we use three planar quadtrees to represent surfaces.",
        "2": "Accounting for the large memory footprint when applying dense feature voxel grids, several techniques have been proposed to reduce memory usage, such as hash-tables [35], octree-trees [16]; these compact data structures have been leveraged in recent robotic applications [20, 18, 19, 26, 24, 23, 36].",
        "3": "To avoid storing unnecessary features in free space, prior work [20, 23, 16, 24] employs octree to store features only within voxel grids where surface points are located."
      },
      "H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense Mapping Using Hierarchical Hybrid Representation": {
        "authors": [
          "C Jiang",
          "Y Luo",
          "B Zhou",
          "S Shen"
        ],
        "url": "https://arxiv.org/pdf/2403.10821",
        "ref_texts": "[4] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE Int. Symp. Mixed Augmented Reality . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "4"
        ],
        "1": "To enhance mapping speed and expand representation capacity, various grid representations have been introduced, including dense 3D grids [3], sparse octree grids [4], multiresolution hash grids [6], and factored grids [5].",
        "2": "SDF-based Volume rendering Like V ox-Fusion [4], we only sample points along the ray that intersects with any leaf node voxel of octree.",
        "3": "[4] X."
      },
      "A*\u2013Ant Colony Optimization Algorithm for Multi-Branch Wire Harness Layout Planning": {
        "authors": [
          "F Yang",
          "P Wang",
          "R Zhang",
          "S Xing",
          "Z Wang",
          "M Li"
        ],
        "url": "https://www.mdpi.com/2079-9292/13/3/529/pdf",
        "ref_texts": "24. Yang, X.; Li, H.; Zhai, H.; Ming, Y.; Liu, Y.; Zhang, G. Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation. In Proceedings of the 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Singapore, 17\u201321 October 2022; pp. 499\u2013507.",
        "ref_ids": [
          "24"
        ],
        "1": "Here, considering that path planning does not have a strong requirement for strict distance, in order to meet the requirements of subsequent interference detection and other aspects, we consider adopting a method based on implicit representation [24]."
      },
      "DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding": {
        "authors": [
          "W Wu",
          "G Wang",
          "T Deng",
          "S Aegidius",
          "S Shanks"
        ],
        "url": "https://arxiv.org/pdf/2403.11776",
        "ref_texts": "[3]Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "3"
        ],
        "1": "Compared to current NeRF-based SLAM methods, such as iMAP [1], NICE-SLAM\n[2], V ox-Fusion [3], ESLAM[4] and Co-SLAM [5], DVN-SLAM not only achieves competitive performance in static scenes, but also remains effective in high-dynamic scenes.",
        "2": "05 V ox-Fusion [3] 2.",
        "3": "Due to the adoption of an Octree for scene representation in V ox-Fusion [3], the reconstructed results tend to have more holes, resulting in a high accuracy (Acc) but a low completeness ratio.",
        "4": "V ox-Fusion [3] utilizes an octree structure for scene representation, resulting in significant holes in the reconstruction.",
        "5": "4M V ox-Fusion [3] 2.",
        "6": "69 V ox-Fusion [3] 1.",
        "7": "13 V ox-Fusion [3] 2.",
        "8": "87 V ox-Fusion [3] 3.",
        "9": "26 V ox-Fusion [3] 3.",
        "10": "84 V ox-Fusion [3] 1.",
        "11": "71 V ox-Fusion [3] 3.",
        "12": "98 V ox-Fusion [3] 1.",
        "13": "82 V ox-Fusion [3] 4."
      },
      "Monocular Gaussian SLAM with Language Extended Loop Closure": {
        "authors": [
          "T Lan",
          "Q Lin",
          "H Wang"
        ],
        "url": "https://arxiv.org/pdf/2405.13748",
        "ref_texts": "41. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: IEEE Int. Symp. Mix. Augment. Real. pp. 499\u2013507. IEEE (2022) 3, 9, 10, 11, 12, 13 Monocular Gaussian SLAM with Language Extended Loop Closure 17",
        "ref_ids": [
          "41"
        ],
        "1": "Differentiable Rendering SLAM As the emergence of Neural Radiance Field [23] (NeRF), methods such as [5,14,20,29,30,34,39,41,43,45] have achieved excellent improvement in high-fidelity reconstruction with NeRF-based representation.",
        "2": "In addition, some previous NeRF-based methods [30,41,45] are also involved in the comparison.",
        "3": "Furthermore, though based on monocular input, our method shows competitive performance with RGBD-based Gaussian SLAM [17] and outperforms previous NeRF-based methods [40,41,45].",
        "4": "The results show that our RGB-based method outperforms early RGB-D based NeRF SLAM methods [41,45] in almost all the metrics, and only slightly lower in PSNR than SOTA RGB-D based methods [17,30].",
        "5": "Results of [17,40,41,45] are taken from [17] and results of [36] and [43] are obtained by running their codes.",
        "6": "Method Office0 Office01 Office02 Office03 Office04 Room0 Room1 Room2 AvgRGB-DVox-Fusion [41] 0.",
        "7": "Results of [17,30,41,45] are taken from [17].",
        "8": "Method Metric Office0 Office01 Office02 Office03 Office04 Room0 Room1 Room2 Avg Vox-Fusion [41]PSNR \u219127.",
        "9": "Results of [17,30,41,45] are taken from [17] and results of [36,43] are taken from [43].",
        "10": "Method 0000 0059 0106 0169 0181 AvgRGB-DVox-Fusion [41] 0.",
        "11": "Results of [17,25,30, 38,41,45] are taken from [17].",
        "12": "0198 Vox-Fusion [41] 0."
      },
      "N-Mapping: Normal Guided Neural Non-Projective Signed Distance Fields for Large-scale 3D Mapping": {
        "authors": [
          "S Song",
          "J Zhao",
          "K Huang",
          "J Lin",
          "C Ye"
        ],
        "url": "https://arxiv.org/pdf/2401.03412",
        "ref_texts": "[10] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxFusion: Dense Tracking and Mapping with V oxel-based Neural Implicit Representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "10"
        ],
        "1": "Subsequent works tackle this issue by employing various data structures such as sparse octree [10], [11], hash encoding [26], and neural points [27].",
        "2": "Most of these approaches [8], [10], [23]\u2013[25] mitigate this issue by replaying historical keyframes.",
        "3": "Voxel-oriented Training 1) Voxel-oriented Sliding Window: Current feature gridbased methods [9], [10], [25] often select recently observed keyframes from the global set for efficient local optimization, similar to the sliding window method employed in traditional SLAM systems.",
        "4": "[10] X."
      },
      "Blending Distributed NeRFs with Tri-stage Robust Pose Optimization": {
        "authors": [
          "B Ye",
          "C Liu",
          "X Ye",
          "Y Chen",
          "Y Wang",
          "Z Yan"
        ],
        "url": "https://arxiv.org/pdf/2405.02880",
        "ref_texts": "[25] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxelbased neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013",
        "ref_ids": [
          "25"
        ],
        "1": "Besides, current approaches using explicit encoding methods like grid [28] [17] and octree[25] for realtime performance, which face the challenge of exponentially expanding encoding components as the scene scale increases, leading to substantially increased storage requirements."
      },
      "S3-SLAM: Sparse Tri-plane Encoding for Neural Implicit SLAM": {
        "authors": [
          "Z Zhang",
          "Y Zhang",
          "Y Wu",
          "B Zhao",
          "X Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.18284",
        "ref_texts": "32. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 1, 2",
        "ref_ids": [
          "32"
        ],
        "1": "Existing neural implicit SLAM [10,21,25,29,32,35] excels in reconstructing high-quality scenes and accurately predicting camera poses.",
        "2": "For instance, methods like Vox-Fusion [32] and Co-SLAM [29] have fewer parameters but lose some detailed appearance information.",
        "3": "Notable efforts to enhance the efficiency of neural implicit representations include techniques such as sparse voxel octrees [32], tri-planes [8], dense grids [26,35], hash grids [14], and tensor decomposition [3,9]."
      },
      "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization": {
        "authors": [
          "P Zhu",
          "Y Zhuang",
          "B Chen",
          "L Li",
          "C Wu",
          "Z Liu"
        ],
        "url": "https://arxiv.org/pdf/2405.06241",
        "ref_texts": "[30] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "30"
        ],
        "1": "The comparative work is very comprehensive including traditional direct visual odometry [9], learningbased SLAM[10], neural implicit slam [26], [30], and more recently Gaussian Splatting-based SLAM [22], [23].",
        "2": "[30] X."
      },
      "ImTooth: Neural Implicit Tooth for Dental Augmented Reality": {
        "authors": [
          "H Li",
          "H Zhai",
          "X Yang",
          "Z Wu",
          "Y Zheng"
        ],
        "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/VR-TVCG-2023-ImTooth/ImTooth.pdf",
        "ref_texts": "[61] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality, pp. 80\u201389, 2021.",
        "ref_ids": [
          "61"
        ],
        "1": "To establish enough co-visibility with a small number of images, we borrow the concept of key-frames from SLAM methods [61], and select the key-frames based on the mutual visibility of voxels.",
        "2": "[61] X."
      },
      "MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash Representations": {
        "authors": [
          "Y Yan",
          "R He",
          "Z Liu"
        ],
        "url": "https://arxiv.org/pdf/2403.17765",
        "ref_texts": "33. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE InMUTE-SLAM 17 ternational Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 2, 3, 4, 8, 9, 11, 12",
        "ref_ids": [
          "33"
        ],
        "1": "Some [11,33] address this with octree-based voxel grids, but they still necessitate an initially defined loose boundary and struggle to reconstruct beyond these limits.",
        "2": "Vox-Fusion [33] attempts to address this by introducing octree-based voxel grids as the map representation but is still limited to the initially defined spatial scope.",
        "3": "1 Multi-map Scene Representation As previous neural implicit SLAM methods [12,25,28,33,35] are restricted to functioning within pre-defined scene boundaries, they are unsuitable for navigating and mapping large, unknown indoor environments.",
        "4": "To better evaluate our proposed MUTE-SLAM on pose estimation, we also compare with previous methods NICE-SLAM [35] and Vox-Fusion [33].",
        "5": "We compare the reconstruction performance on Replica [23] only with Co-SLAM [28] and ESLAM [12] as they significantly outperform previous methods [25,33,35].",
        "6": "62 Vox-Fusion [33] 8.",
        "7": "[33] ESLAM [12] Co."
      },
      "DF-SLAM: Neural Feature Rendering Based on Dictionary Factors Representation for High-Fidelity Dense Visual SLAM System": {
        "authors": [
          "W Wei",
          "J Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.17876",
        "ref_texts": "[7] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, \u201cVox-fusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "7"
        ],
        "1": "Vox-Fusion [7] employs a sparse octree for scene representation and does not require a predefined scene bounding box.",
        "2": "2 Baseline We compare our method to existing state-of-the-art neural implicit SLAM methods: iMAP [5], NICE-SLAM[6], Vox-Fusion [7], ESLAM [8], Co-SLAM [9], Point-SLAM\n[27] and GS-SLAM [28].",
        "3": "32Vox-Fusion* [7] Depth L1 \u2193 1.",
        "4": "73 VoxFusion* [7] 0.",
        "5": "2 VoxFusion* [7] 11.",
        "6": "31 VoxFusion* [7] 3.",
        "7": "56 Vox-Fusion* [7] 11.",
        "8": "[7] X."
      },
      "HVOFusion: Incremental Mesh Reconstruction Using Hybrid Voxel Octree": {
        "authors": [
          "S Liu",
          "J Chen",
          "J Zhu"
        ],
        "url": "https://arxiv.org/pdf/2404.17974",
        "ref_texts": "[Yang et al. , 2022 ]Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "Yang et al\\. , 2022 "
        ]
      },
      "TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields": {
        "authors": [
          "C Duan",
          "Z Yang"
        ],
        "url": "https://arxiv.org/pdf/2310.18917",
        "ref_texts": "[6] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "6"
        ],
        "1": "V ox-Fusion [6] exploits octree-based representation and achieves scalable implicit scene reconstruction.",
        "2": "V oxFusion [6] proposes an accurate and effective SLAM system based on sparse voxel octree.",
        "3": "[36] and V oxFusion [6].",
        "4": "V ox-Fusion [6] randomly selects keyframes from keyframe database to optimize the global map, which leads to incomplete reconstruction.",
        "5": "After self-supervised training of each frame, we obtain an octree-based map up to current frame and newly NICE-SLAM [5] V ox-Fusion [6] Ours (Random) Ours (Overlap)Room4-1\n Room4-2\n ToyCar3\n Teddy Fig.",
        "6": "However, NICE-SLAM [5] and V ox-Fusion [6] are unable to capture it.",
        "7": "0486 V ox-Fusion [6]RMSE [m](\u2193) 0.",
        "8": "5420 V ox-Fusion [6]MSE(\u2193) 0.",
        "9": "Object Completion The keyframe selection strategy used in V ox-Fusion [6] is unstable, its experimental results are different across multiple executions of experiments.",
        "10": "Random Keyframe selection strategy used in V ox-Fusion [6] is unable to reconstruct the 3D mesh that do not appear in the current view (Left column), ours based on overlap, however, fully reconstructs the blue car (Right column), even for some parts of the car those are not observed in current camera view.",
        "11": "[6] X."
      },
      "NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting": {
        "authors": [
          "Y Ji",
          "Y Liu",
          "G Xie",
          "B Ma",
          "Z Xie"
        ],
        "url": "https://arxiv.org/pdf/2403.11679",
        "ref_texts": "[23] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV ox fusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "23"
        ],
        "1": "503 V ox-Fusion [23] 2.",
        "2": "[23] X."
      },
      "MotionGS: Compact Gaussian Splatting SLAM by Motion Filter": {
        "authors": [
          "X Guo",
          "P Han",
          "W Zhang",
          "H Chen"
        ],
        "url": "https://arxiv.org/pdf/2405.11129",
        "ref_texts": "[18] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, pp. 499\u2013507. 1, 5",
        "ref_ids": [
          "18"
        ],
        "1": "2) Baseline Methods: We compare and analyze MotionGS against classic traditional SLAM approach (ORB-SLAM2 [6]), deep learning based method (DROID-SLAM [31]), NeRF-based SLAM methods (iMAP [17], NICE-SLAM [19], V ox-Fusion [18], ESLAM [20], Co-SLAM [22], Point-SLAM [23]), and 3DGS-based SLAM methods (MonoGS [27], SplaTAM [28], GS-SLAM [25]).",
        "2": "1, 2, 5\n[18] X."
      }
    }
  },
  {
    "title": "dp-mvs: detail preserving multi-view surface reconstruction of large-scale scenes",
    "id": 0,
    "valid_pdf_number": "9/12",
    "matched_pdf_number": "7/9",
    "matched_rate": 0.7777777777777778,
    "citations": {
      "Emo-mvs: Error-aware multi-scale iterative variable optimizer for efficient multi-view stereo": {
        "authors": [
          "H Zhou",
          "H Zhao",
          "Q Wang",
          "L Lei",
          "G Hao",
          "Y Xu",
          "Z Ye"
        ],
        "url": "https://www.mdpi.com/2072-4292/14/23/6085/pdf",
        "ref_texts": "37. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "37"
        ]
      },
      "Hash Encoding and Brightness Correction in 3D Industrial and Environmental Reconstruction of Tidal Flat Neural Radiation": {
        "authors": [
          "H Ge",
          "B Wang",
          "Z Zhu",
          "J Zhu",
          "N Zhou"
        ],
        "url": "https://www.mdpi.com/1424-8220/24/5/1451/pdf",
        "ref_texts": "20. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "20"
        ],
        "1": "With advancements in photogrammetry, methods for generating dense point clouds and constructing 3D triangular grid models from 2D images have evolved, incorporating sparse reconstruction (structure from motion, SFM) [18,19] and dense reconstruction (multiple-view stereo, MVS) [20,21]."
      },
      "Geometric Prior-Guided Self-Supervised Learning for Multi-View Stereo": {
        "authors": [
          "L Liu",
          "F Zhang",
          "W Su",
          "Y Qi",
          "W Tao"
        ],
        "url": "https://www.mdpi.com/2072-4292/15/8/2109/pdf",
        "ref_texts": "24. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "24"
        ],
        "1": "Gipuma [21], COLMAP [6], ACMM [23], DP-MVS [24], and PatchMatch MVS [25] are PatchMatch-based [26] MVS methods."
      },
      "Confidence-Guided Planar-Recovering Multiview Stereo for Weakly Textured Plane of High-Resolution Image Scenes": {
        "authors": [
          "C Fu",
          "N Huang",
          "Z Huang",
          "Y Liao",
          "X Xiong",
          "X Zhang"
        ],
        "url": "https://www.mdpi.com/2072-4292/15/9/2474/pdf",
        "ref_texts": "2. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "2"
        ],
        "1": "[2] focuses on the geometric details of reconstruction, especially the preservation of geometric details of thin structures."
      },
      "Large-Scale Mussel Farm Reconstruction with GPS Auxiliary": {
        "authors": [
          "J Zhao",
          "B Xue",
          "R Vennell"
        ],
        "url": "https://openaccess.wgtn.ac.nz/articles/conference_contribution/Large-Scale_Mussel_Farm_Reconstruction_with_GPS_Auxiliary/24633795/1/files/43285662.pdf",
        "ref_texts": "[24] Zhang, Xiaoshuai, et al. \u201cNerfusion: Fusing radiance fields for largescale scene reconstruction.\u201d Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.",
        "ref_ids": [
          "24"
        ],
        "1": "Some recent approaches leverage deep learning [24], [30], like Neural Radiance Field (NeRF), for large-scale 3D reconstruction.",
        "2": "To achieve this, we employed the widely utilized COLMAP method [32], [33], a well-established SfM technique widely applied in various studies [24], [30].",
        "3": "[24] Zhang, Xiaoshuai, et al."
      },
      "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
        "authors": [
          "H Wang",
          "W Zhang",
          "S Liu",
          "X Zhou",
          "S Zhang"
        ],
        "url": "https://arxiv.org/pdf/2405.12477",
        "ref_texts": "[45] Liyang Zhou, Zhuang Zhang, Hanqing Jiang, Han Sun, Hujun Bao, and Guofeng Zhang. Dp-mvs: Detail preserving multi-view surface reconstruction of large-scale scenes. Remote Sensing , 13(22):4569, 2021. 1",
        "ref_ids": [
          "45"
        ],
        "1": "Traditional 3D representation methods [3, 6, 10, 14, 22, 37, 45]\n\u2020Corresponding Author."
      },
      "Editorial on Special Issue \u201cTechniques and Applications of UAV-Based Photogrammetric 3D Mapping\u201d": {
        "authors": [
          "W Jiang",
          "S Jiang",
          "X Xiao"
        ],
        "url": "https://www.mdpi.com/2072-4292/14/15/3804/pdf",
        "ref_texts": "5. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef] Remote Sens. 2022 ,14, 3804 4 of 4",
        "ref_ids": [
          "5"
        ],
        "1": "[5] proposed a dense matching algorithm, termed DP-MVS, for detail-preserving 3D reconstruction."
      },
      "Rekonstruksi Model 3D dari Set Citra Menggunakan Metode SFM-MVS dan Algoritma Poisson": {
        "authors": [
          "G Hanbudi",
          "E Fauzi"
        ],
        "url": "http://www.stmik-budidarma.ac.id/ejurnal/index.php/mib/article/viewFile/4126/2804",
        "ref_texts": "[18] L. Zhou, Z. Zhang, H. Jiang, H. Sun, H. Bao, and G. Zhang, \u201cDP -MVS: Detail Preserving Multi -View Surface Reconstruction of Large -Scale Scenes,\u201d Remote Sensing , vol. 13, no. 22, p. 4569, Nov. 2021, doi: 10.3390/rs13224569. ",
        "ref_ids": [
          "18"
        ],
        "1": "Furukawa mempresentasikan metode SOTA MVS (State of The Art Multi -View Stereo) yang disebut Patch -based MVS (PMVS) dimana metode ini dimulai de ngan menentukan seed patch (patch awalan) dengan merekonstruksi sekumpulan matched key point yang cocok dan kemudian secara iteratif memperluas patch tersebut [18].",
        "2": "[18] L."
      },
      "\u0395\u03bd\u03c3\u03c9\u03bc\u03ac\u03c4\u03c9\u03c3\u03b7 \u03b4\u03b5\u03c3\u03bc\u03b5\u03cd\u03c3\u03b5\u03c9\u03bd \u03c3\u03c4\u03b7\u03bd \u03c0\u03bf\u03bb\u03c5\u03b5\u03b9\u03ba\u03bf\u03bd\u03b9\u03ba\u03ae \u03b1\u03bd\u03b1\u03ba\u03b1\u03c4\u03b1\u03c3\u03ba\u03b5\u03c5\u03ae": {
        "authors": [
          "\u0395\u039a \u03a3\u03c4\u03b1\u03b8\u03bf\u03c0\u03bf\u03cd\u03bb\u03bf\u03c5"
        ],
        "url": "https://dspace.lib.ntua.gr/xmlui/bitstream/handle/123456789/56640/PhD_Stathopoulou_Integrating_scene_priors_in_MVS.pdf",
        "ref_texts": "206 BIBLIOGRAPHY. Zheng, E., Dunn, E., Jojic, V., and Frahm, J.-M. Patchmatch based joint view selection and depthmap estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1510\u20131517, 2014. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., and Torr, P. H. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE international conference on computer vision , pages 1529\u20131537, 2015. Zhong, Y., Dai, Y., and Li, H. Self-supervised learning for stereo matching with selfimproving ability. arXiv preprint arXiv:1709.00930 , 2017. Zhou, C., Zhang, H., Shen, X., and Jia, J. Unsupervised learning of stereo matching. InProceedings of the IEEE International Conference on Computer Vision , pages 1567\u20131575, 2017. Zhou, K., Meng, X., and Cheng, B. Review of stereo matching algorithms based on deep learning. Computational intelligence and neuroscience , 2020, 2020. Zhou, L., Zhang, Z., Jiang, H., Sun, H., Bao, H., and Zhang, G. Dp-mvs: Detail preserving multi-view surface reconstruction of large-scale scenes. Remote Sensing , 13"
      }
    }
  },
  {
    "title": "intrinsicnerf: learning intrinsic neural radiance fields for editable novel view synthesis",
    "id": 5,
    "valid_pdf_number": "21/24",
    "matched_pdf_number": "12/21",
    "matched_rate": 0.5714285714285714,
    "citations": {
      "Palettenerf: Palette-based appearance editing of neural radiance fields": {
        "authors": [
          "Z Kuang",
          "F Luan",
          "S Bi",
          "Z Shu"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
        "ref_texts": "[38] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647, 2022. 2[39] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. CoRR, abs/2112.05131, 2021. 1,2",
        "ref_ids": [
          "38",
          "39"
        ],
        "1": "Introduction Neural Radiance Fields (NeRF) [23] and its variants [8, 25, 27, 39] have received increasing attention in recent years for their ability to robustly reconstruct real-world 3D scenes from 2D images and enable high-quality, photorealistic novel view synthesis.",
        "2": "Many recent works [8, 25,36,39,44] propose to speed up the training and improve the performance of the models by applying a combination of light-weight MLPs and neural feature maps or volumes.",
        "3": "[38] introduces a NeRF-based intrinsic decomposition model which enables 3D intuitive recoloring, but it does not support palette-based editing.",
        "4": "2[39] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa."
      },
      "PVO: Panoptic visual odometry": {
        "authors": [
          "W Ye",
          "X Lan",
          "S Chen",
          "Y Ming",
          "X Yu"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Ye_PVO_Panoptic_Visual_Odometry_CVPR_2023_paper.pdf",
        "ref_texts": ""
      },
      "Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images": {
        "authors": [
          "Y Liu",
          "P Wang",
          "C Lin",
          "X Long",
          "J Wang",
          "L Liu"
        ],
        "url": "https://arxiv.org/pdf/2305.17398",
        "ref_texts": "2022b. S3-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint. In NeurIPS . Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. 2018. MVSNet: Depth inference for unstructured multi-view stereo. In ECCV . Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. 2022. NeILF: Neural incident light field for physically-based material estimation. In ECCV . Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. In NeurIPS . Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview neural surface reconstruction by disentangling geometry and appearance. In NeurIPS . Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2022. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647 (2022). Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, and William AP Smith. 2020. Self-supervised outdoor scene relighting. In ECCV . Ye Yu and William AP Smith. 2019. Inverserendernet: Learning single image inverse rendering. In CVPR . Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. 2021c. NeRS: Neural reflectance surfaces for sparse-view 3d reconstruction in the wild. In NeurIPS . Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. 2022a. IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images. In CVPR . Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. 2021a. PhySG: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In CVPR . Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492 (2020). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR . Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. 2021b. NeRFactor: Neural factorization of shape and reflectance under an unknown illumination. In SIGGRAPH . Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou."
      },
      "Clean-NeRF: Reformulating NeRF to account for View-Dependent Observations": {
        "authors": [
          "X Liu",
          "YW Tai",
          "CK Tang"
        ],
        "url": "https://arxiv.org/pdf/2303.14707",
        "ref_texts": "[66] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647 , 2022. 3",
        "ref_ids": [
          "66"
        ],
        "1": "IntrinsicNeRF [66] introduces intrinsic decomposition to the NeRF-based neural rendering method, which allows for editable novel view synthesis in room-scale scenes."
      },
      "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF": {
        "authors": [
          "Y Feng",
          "Y Shang",
          "X Li",
          "T Shao",
          "C Jiang"
        ],
        "url": "https://arxiv.org/pdf/2311.13099",
        "ref_texts": "[84] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 339\u2013351, 2023. 2",
        "ref_ids": [
          "84"
        ],
        "1": "These include semantic-driven editing [3, 13, 24, 45, 66, 73], shading-driven adjustments (like relighting and texturing) [21, 43, 64, 68, 78, 84], scene modifications (such as object addition or removal) [35, 36, 76, 83, 90], face editing [27, 31, 70, 89], physics based editing from video[25, 62], and multi-purpose editing [30, 75, 82]."
      },
      "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "Y Li",
          "X Zhang",
          "B Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2404.02152",
        "ref_texts": "[63] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 339\u2013351, 2023. 2",
        "ref_ids": [
          "63"
        ],
        "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
      },
      "Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review": {
        "authors": [
          "TAQ Nguyen",
          "A Bourki",
          "M Macudzinski"
        ],
        "url": "https://arxiv.org/pdf/2402.11141",
        "ref_texts": "[277] Jingbo Zhang et al. \u201cFdnerf: Few-shot dynamic neural radiance fields for face reconstruction and expression editing\u201d. In: SIGGRAPH Asia 2022 Conference Papers . 2022, pp. 1\u20139.",
        "ref_ids": [
          "277"
        ],
        "1": "Users can manipulate facial attributes effectively by providing simple expression codes [47, 277], mask 16\n(a)\n (b) Fig.",
        "2": "[277] Jingbo Zhang et al."
      },
      "Few-Shot Neural Radiance Fields under Unconstrained Illumination": {
        "authors": [
          "SY Lee",
          "JY Choi",
          "S Kim",
          "IJ Kim",
          "J Cho"
        ],
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/28075/28156",
        "ref_texts": "12901. Rudnev, V.; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V.; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In ECCV, 615\u2013631. Springer. Sch\u00a8onberger, J. L.; Zheng, E.; Pollefeys, M.; and Frahm, J.M. 2016. Pixelwise View Selection for Unstructured MultiView Stereo. In ECCV. Sitzmann, V.; Zollh \u00a8ofer, M.; and Wetzstein, G. 2019. Scene representation networks: Continuous 3d-structureaware neural scene representations. NeurIPS, 32. Snavely, N.; Seitz, S. M.; and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3D. In ACM SIGGRAPH, 835\u2013846. Toschi, M.; De Matteo, R.; Spezialetti, R.; De Gregorio, D.; Di Stefano, L.; and Salti, S. 2023. ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects. In CVPR, 20762\u201320772. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. In CVPR, 3835\u20133844. Wang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou, H.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and Funkhouser, T. 2021. Ibrnet: Learning multi-view imagebased rendering. In CVPR, 4690\u20134699. Watson, D.; Chan, W.; Martin-Brualla, R.; Ho, J.; Tagliasacchi, A.; and Norouzi, M. 2022. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628. Wynn, J.; and Turmukhambetov, D. 2023. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR, 4180\u20134189. Xu, D.; Jiang, Y.; Wang, P.; Fan, Z.; Shi, H.; and Wang, Z. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. arXiv preprint arXiv:2204.00928. Yang, J.; Pavone, M.; and Wang, Y. 2023. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In CVPR, 8254\u20138263. Yang, S.; Cui, X.; Zhu, Y.; Tang, J.; Li, S.; Yu, Z.; and Shi, B. 2023. Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting. In CVPR, 16600\u201316609. Ye, W.; Chen, S.; Bao, C.; Bao, H.; Pollefeys, M.; Cui, Z.; and Zhang, G. 2022. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. arXiv preprint arXiv:2210.00647.Yu, A.; Ye, V.; Tancik, M.; and Kanazawa, A. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR, 4578\u20134587. Yuan, Y.-J.; Lai, Y.-K.; Huang, Y.-H.; Kobbelt, L.; and Gao, L. 2022a. Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u201316. Yuan, Y.-J.; Sun, Y.-T.; Lai, Y.-K.; Ma, Y.; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In CVPR, 18353\u201318364. The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)",
        "ref_ids": [
          "12901"
        ]
      },
      "SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild": {
        "authors": [
          "A Engelhardt",
          "A Raj",
          "M Boss",
          "Y Zhang",
          "A Kar"
        ],
        "url": "https://arxiv.org/pdf/2401.10171",
        "ref_texts": "[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang. Local-to-global registration forbundle-adjusting neural radiance fields. CVPR , pages 8264\u2013",
        "ref_ids": [
          "15"
        ],
        "1": "Other recent methods rely on rough initialization of the camera, global alignment, or a template shape for joint optimization [15,44,77,84].",
        "2": "2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16\n[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang."
      },
      "NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields": {
        "authors": [
          "N Wang",
          "L Zhang",
          "AX Chang"
        ],
        "url": "https://arxiv.org/pdf/2405.05010",
        "ref_texts": "59. Ye, W., Chen, S., Bao, C., Bao, H., Pollefeys, M., Cui, Z., Zhang, G.: Intrinsicnerf: Learningintrinsicneuralradiancefieldsforeditablenovelviewsynthesis.In:ICCV. pp. 339\u2013351 (2023) 4",
        "ref_ids": [
          "59"
        ],
        "1": "Neural rendering has spurred an exploration into implicit and hybrid representations, offering various approaches for 3D editing, such as changing global appearance [7,29], intrinsic decomposition [59,64], per-object decomposition [54, 56], geometry and texture editing [1,55,61], 3D inpainting [36,52], and others [20,39,50]."
      },
      "NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection": {
        "authors": [
          "C Huang",
          "Y Hou",
          "W Ye",
          "D Huang",
          "X Huang"
        ],
        "url": "https://arxiv.org/pdf/2402.14464",
        "ref_texts": "[Yeet al. , 2023 ]Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023.",
        "ref_ids": [
          "Yeet al\\. , 2023 "
        ]
      },
      "Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation": {
        "authors": [
          "Y Wang",
          "A Gao",
          "Y Gong",
          "Y Zeng"
        ],
        "url": "https://arxiv.org/pdf/2404.05236"
      },
      "ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained Illumination": {
        "authors": [
          "SY Lee",
          "JY Choi",
          "S Kim",
          "IJ Kim",
          "J Cho"
        ],
        "url": "https://arxiv.org/pdf/2303.11728",
        "ref_texts": "12901. Rudnev, V .; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V .; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In ECCV , 615\u2013631. Springer. Sch\u00a8onberger, J. L.; Zheng, E.; Pollefeys, M.; and Frahm, J.M. 2016. Pixelwise View Selection for Unstructured MultiView Stereo. In ECCV . Sitzmann, V .; Zollh \u00a8ofer, M.; and Wetzstein, G. 2019. Scene representation networks: Continuous 3d-structureaware neural scene representations. NeurIPS , 32. Snavely, N.; Seitz, S. M.; and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3D. In ACM SIGGRAPH , 835\u2013846. Toschi, M.; De Matteo, R.; Spezialetti, R.; De Gregorio, D.; Di Stefano, L.; and Salti, S. 2023. ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects. In CVPR , 20762\u201320772. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. In CVPR , 3835\u20133844. Wang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou, H.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and Funkhouser, T. 2021. Ibrnet: Learning multi-view imagebased rendering. In CVPR , 4690\u20134699. Watson, D.; Chan, W.; Martin-Brualla, R.; Ho, J.; Tagliasacchi, A.; and Norouzi, M. 2022. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628 . Wynn, J.; and Turmukhambetov, D. 2023. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR , 4180\u20134189. Xu, D.; Jiang, Y .; Wang, P.; Fan, Z.; Shi, H.; and Wang, Z. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. arXiv preprint arXiv:2204.00928 .Yang, J.; Pavone, M.; and Wang, Y . 2023. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In CVPR , 8254\u20138263. Yang, S.; Cui, X.; Zhu, Y .; Tang, J.; Li, S.; Yu, Z.; and Shi, B. 2023. Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting. In CVPR , 16600\u201316609. Ye, W.; Chen, S.; Bao, C.; Bao, H.; Pollefeys, M.; Cui, Z.; and Zhang, G. 2022. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. arXiv preprint arXiv:2210.00647 . Yu, A.; Ye, V .; Tancik, M.; and Kanazawa, A. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR , 4578\u20134587. Yuan, Y .-J.; Lai, Y .-K.; Huang, Y .-H.; Kobbelt, L.; and Gao, L. 2022a. Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence , 1\u201316. Yuan, Y .-J.; Sun, Y .-T.; Lai, Y .-K.; Ma, Y .; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In CVPR , 18353\u201318364. Supplementary Material In this section, we provide further details on the experiments and datasets, followed by additional ablation studies and experimental results. More Details on Experiments Experimental Details Comparisons on Phototourism F3.A subset for fewshot view synthesis was created by selecting 3 input views with similar depth bounds and frontal-facing poses. The image IDs are (185, 45, 1066), (964, 34, 478), and (82, 312, 803) for \u2018Brandenburg Gate\u2019, \u2018Sacre Coeur\u2019, and \u2018Trevi Fountain\u2019, respectively. Fig. 7 shows image samples of the dataset. Each scene has about eight test images to evaluate the performance. For the depth map comparison using Abs Rel, the original ground truth depth maps provided by Phototourism (Snavely, Seitz, and Szeliski 2006) are very noisy. Following the publicly available instructions of the dataset, we used the clean versions of the depth maps with the background masks. As a result, reported Abs Rel excludes background regions for the evaluation. More detailed information about the proposed datasets can be found in the next section. Comparisons on NeRF Extreme. For evaluating the fewshot view synthesis performance on NeRF Extreme, image IDs (0, 14, 29) were used as inputs for each scene. Given that our proposed multi-view consistency takes into account complex scene geometry, including occlusions, the optimal model and parameters might vary based on the specific scene characteristics. However, experimental results in Tab. 4of the paper were based on our final model. Ablation studies. In this paragraph, we provide detailed information about the ablation studies, especially for ablation 1-3, with albedo MLP. Ablations other than 1-3, were conducted with straightforward implementation with and without proposed consistency regularization. In the case of 1-3, we implemented the multi-view albedo consistency by directly synthesizing the albedo map using the Multi-Layered Perceptron (MLP) of NeRF. Given the viewindependent nature of albedo, we configured the MLP to output albedo as density. By doing so, the proposed framework may be free from the computational costs that come from continuous patch-wise sampling. However, as shown in Tab. 5 in the paper, the model with albedo MLP shows sub-optimal results, which has almost the same results as the model without albedo consistency (1-1). Implementation Details Neural radiance fields. Our framework is based on JAX (Bradbury et al. 2018) implementation of RegNeRF (Niemeyer et al. 2022), while partially adopting a frequency regularization mask of FreeNeRF (Yang, Pavone, and Wang 2023) when constructing MLP. Detailed algorithms of the proposed loss functions are provided in Alg. 1 and 2. Figure 7: Examples of inputs sampled from Phototourism F3.Sampled frontal-facing scenes with varying illumination from the \u2018Brandenburg Gate\u2019, \u2018Sacre Coeur\u2019, and \u2018Trevi Fountain\u2019, respectively. Intrinsic decomposition network. Building upon the concept of integrating an offline intrinsic decomposition network, our PIDNet adopts the architecture of the chosen FIDNet. Given that our ultimate model employs IIDWW (Li and Snavely 2018) as the FIDNet to provide pseudo-albedo ground truth, our PIDNet shares a similar architecture with IIDWW, albeit in a shallower configuration. However, it\u2019s worth highlighting that any intrinsic decomposition network demonstrating superior performance can substitute IIDWW as the FIDNet with a paired PIDNet that has a similar, and shallower architecture. Additional Losses In addition to the losses suggested in the main paper, we incorporates several losses to better optimize the PIDNet as described below. For Lcolor andLds, they were part of the baseline and showed a performance decrease upon removal. Edge-preserving loss. Motivated by (Godard, Mac Aodha, and Brostow 2017), we used the gradient-based edge-preserving loss, to enforce the input and the novel view patches to preserve geometric properties. Using a weight term, \u03c9(x), which already has been discussed in the paper, our edge-preserving loss on the predicted albedo can be formulated as: Ledge=X x\u2032\u2208P\u2032\u03c9(x)\u2225\u2202(\u02c6a(x)\u2212\u02c6a(x\u2032))\u22252,(12) where \u2202denotes the partial derivatives of the vertical and the horizontal directions, and P\u2032denotes all the pixels in the target image. Chromaticity consistency loss. Similar to (Ye et al.",
        "ref_ids": [
          "12901"
        ]
      },
      "DreamMat: High-quality PBR Material Generation with Geometry-and Light-aware Diffusion Models": {
        "authors": [
          "Y Zhang",
          "Y Liu",
          "Z Xie",
          "L Yang",
          "Z Liu",
          "M Yang"
        ],
        "url": "https://arxiv.org/pdf/2405.17176",
        "ref_texts": "(2023). Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. 2022. Neilf: Neural incident light field for physically-based material estimation. In ECCV . Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. In NeurIPS . Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2023. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In ICCV . Jounathan Young. 2021. xatlas. https://github.com/jpcy/xatlas.git Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. 2023. Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering. arXiv preprint arXiv:2312.11360 (2023). Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. 2023a. Texture Generation on 3D Meshes with Point-UV Diffusion. In ICCV . Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi."
      },
      "Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo": {
        "authors": [
          "Z Li",
          "Z Lu",
          "H Yan",
          "B Shi",
          "G Pan",
          "Q Zheng"
        ],
        "url": "https://arxiv.org/pdf/2404.01612",
        "ref_texts": "[30] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning intrinsic neural radiance fields for editable novel view synthesis. Proc. International Conference on Computer Vision (ICCV) , 2023. 5",
        "ref_ids": [
          "30"
        ],
        "1": "Similar to [30], the normalized color loss calculated as \u2225Nor(A)\u2212Nor(I)\u2225is implemented to help Spin-UP learn a better albedo representation, where Nor(."
      },
      "Neural Implicit Field Editing Considering Object-environment Interaction": {
        "authors": [
          "Z Zeng",
          "Z Wang",
          "Y Zhang",
          "W Cai",
          "Z Cao"
        ],
        "url": "https://arxiv.org/pdf/2311.00425",
        "ref_texts": "[36] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2023. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 339\u2013351.",
        "ref_ids": [
          "36"
        ],
        "1": "Intrinsic NeRF [36] performs unsupervised clustering of image color labels in training process to reconstruct the albedo and shading scene colors.",
        "2": "[36].",
        "3": "To verify the effectiveness of the intrinsic decomposition method we used, we compared the albedo images obtained by Intrinsic NeRF [36] with our own results, which showed our separation in shadow areas were superior to existing methods.",
        "4": "The main method Intrinsic NeRF [36], which did not release code and datasets, mentioned the reference of preprocessed Replica indoor scene dataset provided by Semantic NeRF [40].",
        "5": "Intrinsic NeRF [36] is evaluated their albedo images according to the similarity of ground truth provided by earlier methods such as PhySG [37].",
        "6": "Replica-room_0 Config PSNR \u2193SSIM\u2191LPIPS\u2193 Intrinsic NeRF [36] 30."
      },
      "Neural Radiance Field-based Visual Rendering: A Comprehensive Review": {
        "authors": [
          "M Yao",
          "Y Huo",
          "Y Ran",
          "Q Tian",
          "R Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.00714",
        "ref_texts": "[111] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, \u201cIntrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023, pp. 339\u2013351.",
        "ref_ids": [
          "111"
        ],
        "1": "Others SparseNeRF [110](depth prior), NeRF-Det [111](end-to-end detection), IntrinsicNeRF [112](Unsupervised), etc.",
        "2": "20 IntrinsicNeRF [111] (2023) possesses the capability to break down a static scene\u2019s multi-view image into consistent elements like reflectance, shading, and residual layers, achieved through intrinsic decomposition in a NeRF-based neural rendering technique.",
        "3": "[111] W."
      },
      "3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer Avenue": {
        "authors": [
          "Y Li",
          "B Cai",
          "Y Liang",
          "R Jia",
          "B Zhao",
          "M Gong"
        ],
        "url": "https://arxiv.org/pdf/2211.14823",
        "ref_texts": "[54] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, \u201cIntrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis,\u201d arXiv preprint arXiv:2210.00647 , 2022.",
        "ref_ids": [
          "54"
        ],
        "1": "There are several works [53], [54], [55] that have also exploited free scene lighting editing.",
        "2": "[54] W."
      },
      "Light Source Estimation via Intrinsic Decomposition for Novel View Synthesis": {
        "authors": [
          "DS Tetruashvili"
        ],
        "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/645817/Tetruashvili_David.pdf?sequence=1",
        "ref_texts": ""
      },
      "Clean-NeRF: Defogging using Ray Statistics Prior in Natural NeRFs": {
        "authors": [
          "X Liu",
          "YW Tai",
          "CK Tang"
        ],
        "url": "https://openreview.net/pdf?id=YHqEWF5gt8",
        "ref_texts": "612, 2004. Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Ho\u0142y \u00b4nski, and Angjoo Kanazawa. Nerfbusters: Removing ghostly artifacts from casually captured nerfs. 2023. Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D\u02c6 2nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. Advances in Neural Information Processing Systems (NeurIPS) , 35:32653\u201332666, 2022. Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In IEEE/CVF International Conference on Computer Vision (ICCV) , 2023. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for realtime rendering of neural radiance fields. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 5752\u20135761, 2021. Ye Yu and William AP Smith. Inverserendernet: Learning single image inverse rendering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 3155\u20133164, 2019. Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. Editable free-viewpoint video using a layered neural representation. ACM Transactions on Graphics (TOG) , 40(4):1\u201318, 2021a. Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 5453\u20135462, 2021b. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 586\u2013595, 2018. Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 7743\u20137753, 2022. Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and understanding with implicit scene representation. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 15838\u201315847, 2021. Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and Manmohan Chandraker. Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 2822\u20132831, 2022."
      },
      "Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields": {
        "authors": [
          "Y Yang",
          "S Hu",
          "H Wu",
          "R Baldrich",
          "D Samaras",
          "M Vanrell"
        ],
        "url": "https://neural-rendering.com/papers/22.pdf",
        "ref_texts": "[37] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, 2023. 1, 2, 3, 4",
        "ref_ids": [
          "37"
        ],
        "1": "Concurrently, there has been an exploration towards scene editing [35], such as recoloring [37] and relighting [21, 38].",
        "2": "The second approach [37], based on intrinsic decomposition[2], aims to provide an interpretable representation of a scene (in terms of reflectance and shading) suitable for image editing.",
        "3": "While IntrinsicNeRF [37] has pioneered the integration of intrinsic decomposition within NeRF, they have not utilized relighting or fully leveraged the 3D information available through neural rendering.",
        "4": "IntrinsicNeRF [37] has been a pioneer in applying intrinsic decomposition to neural rendering.",
        "5": "However, real-world scenes often require a residual term to account for discrepancies [11, 37].",
        "6": "As demonstrated in [37], the diffuse components dominate the scene, so it is crucial to prevent the training from converging to undesirable local minima (R= 0, S=\n0, Re=I).",
        "7": "1194 IntrinsicNeRF*[37] 25.",
        "8": "[4]) and the state-of-the-art neural rendering approach (IntrinsicNeRF [37]).",
        "9": "The other neural rendering method, IntrinsicNeRF [37], also fails to achieve correct decomposition, primarily attributed to the failure in distinguishing intrinsic components and also the difficulty in scene reconstruction."
      }
    }
  },
  {
    "title": "sine: semantic-driven image-based nerf editing with prior-guided editing field",
    "id": 6,
    "valid_pdf_number": "56/60",
    "matched_pdf_number": "38/56",
    "matched_rate": 0.6785714285714286,
    "citations": {
      "Shap-e: Generating conditional 3d implicit functions": {
        "authors": [
          "H Jun",
          "A Nichol"
        ],
        "url": "https://arxiv.org/pdf/2305.02463",
        "ref_texts": "[3]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. arXiv:2303.13277 , 2023.",
        "ref_ids": [
          "3"
        ],
        "1": "Since they are end-to-end differentiable, INRs also enable various downstream applications such as style transfer [72] and differentiable shape editing [3]."
      },
      "Dreameditor: Text-driven 3d scene editing with neural fields": {
        "authors": [
          "J Zhuang",
          "C Wang",
          "L Lin",
          "L Liu",
          "G Li"
        ],
        "url": "https://arxiv.org/pdf/2306.13455",
        "ref_texts": "Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for textdriven editing of natural images. In CVPR 2022 . 18208\u201318218. Chong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. Sine: Semantic-driven imagebased nerf editing with prior-guided editing field. In CVPR 2023 . 20919\u201320929. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022). Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629 (2021). Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303.13873 (2023). Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. 2022. Tango: Textdriven photorealistic and robust 3d stylization via lighting decomposition. arXiv preprint arXiv:2210.11277 (2022). Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427 (2022). Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022a. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022b. StyleGAN-NADA: CLIP-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201313. William Gao, Noam Aigerman, Thibault Groueix, Vladimir G Kim, and Rana Hanocka."
      },
      "Multimodal image synthesis and editing: A survey": {
        "authors": [
          "F Zhan",
          "Y Yu",
          "R Wu",
          "J Zhang",
          "S Lu",
          "L Liu"
        ],
        "url": "https://pure.mpg.de/rest/items/item_3487306/component/file_3487307/content",
        "ref_texts": ""
      },
      "Blended-nerf: Zero-shot object generation and blending in existing neural radiance fields": {
        "authors": [
          "O Gordon",
          "O Avrahami"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/papers/Gordon_Blended-NeRF_Zero-Shot_Object_Generation_and_Blending_in_Existing_Neural_Radiance_ICCVW_2023_paper.pdf",
        "ref_texts": ""
      },
      "Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis": {
        "authors": [
          "W Ye",
          "S Chen",
          "C Bao",
          "H Bao"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.pdf",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. In Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference , 2023.",
        "ref_ids": [
          "1"
        ],
        "1": "Given the high degree of integration of our approach with NeRF, NeRF extensions can be seamlessly incorporated into our IntrinsicNeRF, such as NeRF in the wild [12, 46, 59], NeRF in dynamic environments [33, 51, 52, 69], fast NeRF [48, 18, 10, 71], NeRF with generalization [11, 64, 72, 27], generative NeRF [55, 62], NeRF with panoptic segmentation [26, 68], NeRFbased SLAM [47, 58, 82], Geometry and Texture Editing with NeRF [1, 14] etc, which will be helpful to the comOriginal Original Recoloring RecoloringFigure 11: Recoloring on Synthetic/Real-World Data."
      },
      "Gaussianeditor: Editing 3d gaussians delicately with text instructions": {
        "authors": [
          "J Fang",
          "J Wang",
          "X Zhang",
          "L Xie",
          "Q Tian"
        ],
        "url": "https://arxiv.org/pdf/2311.16037",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "3D Scene Editing of Radiance Fields 3D Scene Editing of Radiance Fields has gained significant popularity as a recent research direction [1, 10, 15, 20, 22\u201325, 28, 34, 47, 48, 52\u201354]."
      },
      "Or-nerf: Object removing from 3d scenes guided by multiview segmentation with neural radiance fields": {
        "authors": [
          "Y Yin",
          "Z Fu",
          "F Yang",
          "G Lin"
        ],
        "url": "https://arxiv.org/pdf/2305.10503",
        "ref_texts": "[30] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field,\u201d 2023. [Online]. Available: http: //arxiv.org/abs/2303.13277",
        "ref_ids": [
          "30",
          "Online"
        ],
        "1": "Scene Object Removal NeRF has greatly facilitated the area of 3D scene editing and research [30], [31], [32], [33] focuses on various editing typesemerging in large numbers.",
        "2": "[Online].",
        "3": "[Online].",
        "4": "[Online].",
        "5": "[Online].",
        "6": "[Online].",
        "7": "[Online].",
        "8": "[Online].",
        "9": "[Online].",
        "10": "[Online].",
        "11": "[Online].",
        "12": "[Online].",
        "13": "[Online].",
        "14": "[Online].",
        "15": "[Online].",
        "16": "[Online].",
        "17": "[Online].",
        "18": "15224\n[30] C.",
        "19": "[Online].",
        "20": "[Online].",
        "21": "[Online].",
        "22": "[Online].",
        "23": "[Online].",
        "24": "[Online].",
        "25": "[Online].",
        "26": "[Online].",
        "27": "[Online].",
        "28": "[Online].",
        "29": "[Online].",
        "30": "[Online].",
        "31": "[Online].",
        "32": "[Online].",
        "33": "[Online].",
        "34": "[Online].",
        "35": "[Online].",
        "36": "[Online].",
        "37": "[Online].",
        "38": "[Online]."
      },
      "Gaussianeditor: Swift and controllable 3d editing with gaussian splatting": {
        "authors": [
          "Y Chen",
          "Z Chen",
          "C Zhang",
          "F Wang",
          "X Yang"
        ],
        "url": "https://arxiv.org/pdf/2311.14521",
        "ref_texts": "[1]Chong Bao, Yinda Zhang, and Bangbang et al. Yang. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR 2023 , pages 20919\u201320929, 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "Additionally, some works [1,10,45,46] leverage CLIP models to facilitate editing through the use of text prompts or reference images.",
        "2": "[1]Chong Bao, Yinda Zhang, and Bangbang et al.",
        "3": "Specifically, when the user clicks a point on the screen, we back-project this point into a spatial point based on the intrinsic and extrinsic parameters of the current viewpoint camera: [x, y, z ]T= [R|t]z(p)K\u22121[px, py,1]T, (9) where [R|t]andKdenote the extrinsic and intrinsic of the current camera, p,z(p)and[x, y, z ]Trefer to the userclicked pixel, its corresponding depth, and the spatial point, respectively."
      },
      "Dreamspace: Dreaming your room space with text-driven panoramic texture propagation": {
        "authors": [
          "B Yang",
          "W Dong",
          "L Ma",
          "W Hu",
          "X Liu"
        ],
        "url": "https://arxiv.org/pdf/2310.13119.pdf?!%5B%E5%9B%BE%E7%89%87%5D(https://mmbiz.qpic.cn/sz_mmbiz_png/tGynVEPiakb9lruS9sv1HdDZ7vhDqdSHTglAfA3BTYFnjkbjPq1ScXWEdvTr7zziboby5kzsWghbScUOPKSziag0g/640?wx_fmt=png)",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2, 3",
        "ref_ids": [
          "2"
        ],
        "1": ", by giving text prompts, and automatically transferring textures of our living room with enchanting and meaningful details? Over the past few years, enormous efforts have been paid in the field of scene stylization (or texture synthesis) [2, 5, 16, 18, 22, 40, 56].",
        "2": ", imitating Van Gogh\u2019s paintings instead of generating recognizable visual elements [22,56]), or focus on texture editing [2,18] on 3D objects with NeRF representation [31] but struggle to generate high-fidelity textures for the whole space and achieve real-time rendering on HMD devices.",
        "3": ", CLIP model [39]) for style transfer (or editing) [2,18], which achieves stylized results that also follow human language prompts, but these works mainly cannot be scaled to large indoor scenes that allow immersive room touring.",
        "4": "Therefore, existing works for scene-level stylization either are not applicable for immersive indoor scenescale scenarios with affordable computation on HMD devices [2,18], cannot support semantic meaningful style generation [6, 7, 11, 22, 23, 56], or require well-structured CAD model instead of real-world reconstruction [49]."
      },
      "ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields": {
        "authors": [
          "J Dong",
          "YX Wang"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf",
        "ref_texts": "[29] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. SINE: Semantic-driven image-based NeRF editing with prior-guided editing field. In CVPR , 2023.",
        "ref_ids": [
          "29"
        ]
      },
      "Advances in 3D Generation: A Survey": {
        "authors": [
          "X Li",
          "Q Zhang",
          "D Kang",
          "W Cheng",
          "Y Gao"
        ],
        "url": "https://arxiv.org/pdf/2401.17807",
        "ref_texts": ""
      },
      "Dyn-e: Local appearance editing of dynamic neural radiance fields": {
        "authors": [
          "S Zhang",
          "S Peng",
          "Y ShenTu",
          "Q Shuai",
          "T Chen"
        ],
        "url": "https://arxiv.org/pdf/2307.12909",
        "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. In CVPR . Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In ECCV . Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P Srinivasan. 2021. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV\u2018\u2019 . Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman."
      },
      "Dynvideo-e: Harnessing dynamic nerf for large-scale motion-and view-change human-centric video editing": {
        "authors": [
          "JW Liu",
          "YP Cao",
          "JZ Wu",
          "W Mao",
          "Y Gu",
          "R Zhao"
        ],
        "url": "https://arxiv.org/pdf/2310.10624",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "SINE [1] supports editing a local region of static NeRF from a single view by delivering edited contents to multi-views through pretrained NeRF priors."
      },
      "ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF": {
        "authors": [
          "J Park",
          "G Kwon",
          "JC Ye"
        ],
        "url": "https://arxiv.org/pdf/2310.02712",
        "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5855\u20135864, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , pp. 333\u2013350. Springer, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5501\u20135510, 2022. Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 , 2021. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 , 2023. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090 , 2023. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 867\u2013876, 2022. Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little nonlinearity that could. In ACM SIGGRAPH 2022 Conference Proceedings , pp. 1\u20139, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023."
      },
      "Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing": {
        "authors": [
          "J Zeng",
          "C Bao",
          "R Chen",
          "Z Dong",
          "G Zhang"
        ],
        "url": "https://arxiv.org/pdf/2308.03280",
        "ref_texts": "[2]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. arXiv preprint arXiv:2303.13277",
        "ref_ids": [
          "2"
        ],
        "1": "Several extensions and improvements have been proposed to apply NeRF to more challenging problems, such as scene reconstruction [1,8,13,29,30,32,36,38,39,44,48], generalization [24,33], novel view extrapolation [35,45], scene manipulation [2,28,40\u201342], SLAM [23,54], segmentation [20,53], human body [18,31] and so on."
      },
      "NeRF in Robotics: A Survey": {
        "authors": [
          "G Wang",
          "L Pan",
          "S Peng",
          "S Liu",
          "C Xu",
          "Y Miao"
        ],
        "url": "https://arxiv.org/pdf/2405.01333",
        "ref_texts": "[90] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in CVPR , 2023, pp. 20 919\u201320 929.",
        "ref_ids": [
          "90"
        ],
        "1": "SINE\n[90] employs a prior-guided editing field to adjust spatial point coordinates and colours for semantic-driven editing.",
        "2": "[90] C."
      },
      "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF": {
        "authors": [
          "Y Feng",
          "Y Shang",
          "X Li",
          "T Shao",
          "C Jiang"
        ],
        "url": "https://arxiv.org/pdf/2311.13099",
        "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "3"
        ],
        "1": "These include semantic-driven editing [3, 13, 24, 45, 66, 73], shading-driven adjustments (like relighting and texturing) [21, 43, 64, 68, 78, 84], scene modifications (such as object addition or removal) [35, 36, 76, 83, 90], face editing [27, 31, 70, 89], physics based editing from video[25, 62], and multi-purpose editing [30, 75, 82]."
      },
      "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "Y Li",
          "X Zhang",
          "B Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2404.02152",
        "ref_texts": "[5] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "5"
        ],
        "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
      },
      "Coarf: Controllable 3d artistic style transfer for radiance fields": {
        "authors": [
          "D Zhang",
          "C Fernandez-Labrador"
        ],
        "url": "https://arxiv.org/pdf/2404.14967",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "CLIPNeRF and SINE [1, 51] enable text-driven editing, whereas [21] distills the 2D semantic feature from LSeg [26] to train 3D semantic feature using volumetric rendering, enabling editing including colorization, translation, deletion, and text-driven editing.",
        "2": "Our Semantic Aware Nearest Neighbor Feature Matching (SANNFM) function performs nearest neighbor matching between content and style features in both VGG (FV GG r,FV GG s ) and LSeg (FLSeg r,FLSeg s ) spaces for each specific pixel x, y with label m: SANNFM (x, y, m ) =argminx\u2032,y\u2032\u2208SDsannfm, (9) where S={x\u2032, y\u2032|Ms(x\u2032, y\u2032) =m}, and the distance functionDsannfm is defined as a weighted average of the VGG cosine distance and LSeg cosine distance: Dsannfm =\u03b1\u00b7D(FV GG r(x, y),FV GG s(x\u2032, y\u2032))\n+ (1\u2212\u03b1)\u00b7D(FLSeg r(x, y),FLSeg s(x\u2032, y\u2032)),(10) where \u03b1\u2208[0,1]is a hyperparameter to control the weight of VGG and LSeg features and D(."
      },
      "Aprf: Anti-aliasing projection representation field for inverse problem in imaging": {
        "authors": [
          "Z Chen",
          "L Yang",
          "J Lai",
          "X Xie"
        ],
        "url": "https://arxiv.org/pdf/2307.05270",
        "ref_texts": "[27] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023.",
        "ref_ids": [
          "27"
        ],
        "1": "INR techniques have yielded impressive advances in image reconstruction for numerous tasks: single image super-resolution [19], video super-resolution [22], novel view synthesis [18], generative modeling [23]\u2013[25], and editing [26], [27].",
        "2": "[27] C."
      },
      "Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training": {
        "authors": [
          "R He",
          "S Huang",
          "X Nie",
          "T Hui",
          "L Liu",
          "J Dai",
          "J Han"
        ],
        "url": "https://arxiv.org/pdf/2312.01663",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , 2023. 3, 6",
        "ref_ids": [
          "2"
        ],
        "1": "CLIP-NeRF [42] and SINE [2] leverage prior models to optimize the geometry and texture of NeRF based on text descriptions or exemplar images.",
        "2": "For image-driven editing, due to the absence of existing methods for this setting, we modified several existing works to serve as baselines for comparison with our method, including: (1) Ours+Splice Loss: Splice loss [41] is proposed to disentangle structure and appearance information from an image for image editing, which is further demonstrated effective to transfer the texture from an exemplar image for NeRF editing in SINE [2]."
      },
      ": A 3D Neural Additive Model for Interpretable Shape Representation": {
        "authors": [
          "Y Jiao",
          "CJ ZDANSKI",
          "JS Kimbell",
          "A Prince"
        ],
        "url": "https://openreview.net/pdf?id=wg8NPfeMF9",
        "ref_texts": "40\u201349. PMLR, 2018. Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E Hinton. Neural additive models: Interpretable machine learning with neural nets. arXiv preprint arXiv:2004.13912 , 2020. Sercan \u00d6 Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 6679\u20136687, 2021. K Somani Arun, Thomas S Huang, and Steven D Blostein. Least-squares fitting of two 3-d point sets. IEEE Transactions on pattern analysis and machine intelligence , (5):698\u2013700, 1987. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp."
      },
      "S2RF: Semantically Stylized Radiance Fields": {
        "authors": [
          "D Lahiri",
          "N Panse",
          "M Kumar"
        ],
        "url": "https://arxiv.org/pdf/2309.01252",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023.",
        "ref_ids": [
          "2"
        ],
        "1": "Similar to previous methods [25, 12, 2, 26, 8, 6], addressing style transfer in 3D, we adopted an optimization-based approach.",
        "2": "An interesting work Sine [2], requires one image from a scene edited by the user and can generate a 3D view of the scene with the edited objects."
      },
      "Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting": {
        "authors": [
          "J Meng",
          "H Li",
          "Y Wu",
          "Q Gao",
          "S Yang",
          "J Zhang"
        ],
        "url": "https://arxiv.org/pdf/2404.01168",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition (CVPR) (2023)",
        "ref_ids": [
          "1"
        ],
        "1": "These endeavors have been geared towards refining reconstruction quality [2\u20134,15,38,43], enhancing computational efficiency [13,28,30,33,37], enabling advanced editing functionalities [1,23,40,44,45], and progressing dynamic scene representation [8,10,12,22,29].",
        "2": "We add a learnable mirror attribute m\u2208[0,1]for each Gaussian in the original 3DGS, representing the probability of it being a mirror.",
        "3": "(28) We apply SINE and Sigmoid activation function to normalize the opacity and mirror properties respectively, both of which are bounded within the range of [0, 1], while keeping all other experimental variables constant."
      },
      "ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF": {
        "authors": [
          "JH Park",
          "G Kwon",
          "JC Ye"
        ],
        "url": "https://openreview.net/pdf?id=9DvDRTTdlu",
        "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5855\u20135864, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , pp. 333\u2013350. Springer, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5501\u20135510, 2022. Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 , 2021. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 , 2023. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090 , 2023. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 867\u2013876, 2022. Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little nonlinearity that could. In ACM SIGGRAPH 2022 Conference Proceedings , pp. 1\u20139, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023."
      },
      "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds": {
        "authors": [
          "M Chen",
          "J Xie",
          "I Laina",
          "A Vedaldi"
        ],
        "url": "https://arxiv.org/pdf/2312.09246",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , 2023. 2",
        "ref_ids": [
          "1"
        ],
        "1": "Approaches that followed include 3D editing from just a single edited view [1], or via 2D sketches [44], keypoints [87], attributes [25], meshes [22, 52, 76, 78, 80] or point clouds [5]."
      },
      "CaesarNeRF: Calibrated Semantic Representation for Few-shot Generalizable Neural Rendering": {
        "authors": [
          "H Zhu",
          "T Ding",
          "T Chen",
          "I Zharkov",
          "R Nevatia"
        ],
        "url": "https://arxiv.org/pdf/2311.15510",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "1"
        ],
        "1": "In recent years, NeRF has witnessed improvements in a wide range of applications, such as photo-realistic novel view synthesis for large-scale scenes [34, 60, 71], dynamic scene decomposition and deformation [21, 27, 31, 40\u2013\n42, 44, 75, 76], occupancy or depth estimation [58, 62, 74], scene generation and editing [1, 20, 28, 30, 35, 43, 63, 64, 70], and so on."
      },
      "Neural Rendering and Its Hardware Acceleration: A Review": {
        "authors": [
          "X Yan",
          "J Xu",
          "Y Huo",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2402.00028",
        "ref_texts": "[8] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023.",
        "ref_ids": [
          "8"
        ],
        "1": "NeRF [62] and related works [60]\u2013 [8] have demonstrated good performance in learning scene representation from multi-view input data using volume rendering, which can be utilized in neural rendering-based inverse rendering frameworks.",
        "2": "[8] C."
      },
      "LLMs Meet Multimodal Generation and Editing: A Survey": {
        "authors": [
          "Y He",
          "Z Liu",
          "J Chen",
          "Z Tian",
          "H Liu",
          "X Chi",
          "R Liu"
        ],
        "url": "https://arxiv.org/pdf/2405.19334",
        "ref_texts": "[358] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with prior-guided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp.",
        "ref_ids": [
          "358"
        ],
        "1": "Sine [358] presented a prior-guided editing field that encodes finegrained geometric and texture modifications.",
        "2": "5 CLIP for 3D editing CLIP-NeRF [138] CVPR 2022 CLIP Loss NeRF CLIP Blended-NeRF [356] ICCVW 2023 CLIP Loss NeRF CLIP SKED [359] ICCV 2023 Score Distillation NeRF SD DreamEditor [360] SIGGRAPH Asia 2023 Score Distillation NeRF SD Instruct-NeRF2NeRF [361] SIGGRAPH Asia 2023 Score Distillation NeRF SD TextDeformer [357] TVCG 2022 Score Distillation Mesh SD SINE [358] CVPR 2023 Score Distillation NeRF SD Blending-NeRF [378] ICCV2023 CLIP Loss NeRF CLIP CustomNeRF [379] CVPR 2024 Score Distillation NeRF SD Paint3D [380] arXiv 2023 Mesh SD\n3D Paintbrush [362] arXiv 2023 Score Distillation NeRF SD\n20 TABLE 8: Audio datasets that can be adopted for language-based audio research.",
        "3": "[358] C."
      },
      "TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts": {
        "authors": [
          "J Zhuang",
          "D Kang",
          "YP Cao",
          "G Li",
          "L Lin"
        ],
        "url": "https://arxiv.org/pdf/2401.14828",
        "ref_texts": "Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-a-scene: Extracting multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers . 1\u201312. Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In CVPR 2022 . 18208\u201318218. Chong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. SINE: Semantic-driven imagebased nerf editing with prior-guided editing field. In CVPR 2023 . 20919\u201320929. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. InstructPix2Pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022). de Charette Raoul Cao, Anh-Quan. 2023. SceneRF: Self-supervised monocular 3D scene reconstruction with radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 9387\u20139398. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. 2023. Segment Anything in 3D with NeRFs. In NeurIPS . Jun-Kun Chen, Jipeng Lyu, and Yu-Xiong Wang. 2023c. Neuraleditor: Editing neural radiance fields via manipulating point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 12439\u201312448. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023b. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303.13873 (2023). Yiwen Chen, Zilong Chen, and Chi .eta Zhang. 2023a. GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting. arXiv preprint arXiv:2311.14521"
      },
      "Consolidating Attention Features for Multi-view Image Editing": {
        "authors": [
          "O Patashnik",
          "R Gal",
          "D Cohen-Or",
          "JY Zhu"
        ],
        "url": "https://arxiv.org/pdf/2402.14792",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 3",
        "ref_ids": [
          "2"
        ],
        "1": "Recent works employ advances in text-based image editing to edit an implicit 3D representation with text [2, 20, 47, 54, 55]."
      },
      "Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning": {
        "authors": [
          "W Dong",
          "B Yang",
          "L Ma",
          "X Liu",
          "L Cui",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2405.08054",
        "ref_texts": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. 2018. Learning representations and generative models for 3d point clouds. In International conference on machine learning . PMLR, 40\u201349. Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, and Zhaopeng Cui. 2024. GeneAvatar: Generic ExpressionAware Volumetric Head Avatar Editing from a Single Image. arXiv preprint arXiv:2404.02152 (2024). Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 20919\u201320929. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. Multidiffusion: Fusing diffusion paths for controlled image generation. (2023). Shariq Farooq Bhat, Niloy J Mitra, and Peter Wonka. 2023. LooseControl: Lifting ControlNet for Generalized Depth Conditioning. arXiv preprint arXiv:2312.03079"
      },
      "RHINO: Regularizing the Hash-based Implicit Neural Representation": {
        "authors": [
          "H Zhu",
          "F Liu",
          "Q Zhang",
          "X Cao",
          "Z Ma"
        ],
        "url": "https://arxiv.org/pdf/2309.12642",
        "ref_texts": "(2023) Sine: Semantic-driven image-based nerf editing with priorguided editing field. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp 20919\u201320929 Barron JT, Mildenhall B, Tancik M, Hedman P, Martin-Brualla R, Srinivasan PP (2021) Mip-nerf: A multiscale representation for antialiasing neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision , pp 5855\u20135864 Cao A, Johnson J (2023) Hexplane: A fast representation for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp 130\u2013141 Chabra R, Lenssen JE, Ilg E, Schmidt T, Straub J, Lovegrove S, Newcombe R (2020) Deep local shapes: Learning local sdf priors for detailed 3d reconstruction. In: European Conference on Computer Vision , Springer, pp 608\u2013625 Chan ER, Lin CZ, Chan MA, Nagano K, Pan B, De Mello S, Gallo O, Guibas LJ, Tremblay J, Khamis S, et al. (2022) Efficient geometryaware 3d generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 16123\u201316133 Chen A, Xu Z, Zhao F, Zhang X, Xiang F, Yu J, Su H (2021) MVSNeRF: Fast generalizable radiance field reconstruction from multiview stereo. In: Proceedings of the IEEE/CVF International Conference on Computer Vision , pp 14124\u201314133 Chen A, Xu Z, Geiger A, Yu J, Su H (2022) Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision , Springer, pp 333\u2013350 Chen Y, Lu L, Karniadakis GE, Dal Negro L (2020) Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express 28(8):11618\u201311633 Dupont E, Goli \u00b4nski A, Alizadeh M, Teh YW, Doucet A (2021) Coin: Compression with implicit neural representations. arXiv preprint arXiv:210303123 Fang J, Yi T, Wang X, Xie L, Zhang X, Liu W, Nie\u00dfner M, Tian Q"
      },
      "Advances in 3D Neural Stylization: A Survey": {
        "authors": [
          "Y Chen",
          "G Shao",
          "KC Shum",
          "BS Hua"
        ],
        "url": "https://arxiv.org/pdf/2311.18328"
      },
      "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting": {
        "authors": [
          "K Liu",
          "F Zhan",
          "M Xu",
          "C Theobalt",
          "L Shao"
        ],
        "url": "https://arxiv.org/pdf/2403.07807",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 4",
        "ref_ids": [
          "1"
        ],
        "1": "Consequently, previous studies have resorted to learning-based methods for editing radiance fields [1,6,7,14,23\u201325, 34,42,45,46,50,52,57], guided by images [1,7,24,53], text [6,14,45,46,57], or other forms of user input [6,25,50], encompassing modifications such as deformation [34,50,52], appearance changes [6,14,24,45,46,57], removal [6], relighting [42], and inpainting [23,28]."
      },
      "DATENeRF: Depth-Aware Text-based Editing of NeRFs": {
        "authors": [
          "S Rojas",
          "J Philip",
          "K Zhang",
          "S Bi",
          "F Luan"
        ],
        "url": "https://arxiv.org/pdf/2404.04526",
        "ref_texts": "2. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 3",
        "ref_ids": [
          "2"
        ],
        "1": "SINE [2] transfers edits from a single edited image across the entire scene using a ViT model [6] as a semantic texture prior."
      },
      "NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields": {
        "authors": [
          "N Wang",
          "L Zhang",
          "AX Chang"
        ],
        "url": "https://arxiv.org/pdf/2405.05010",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: CVPR. pp. 20919\u201320929 (2023) 4",
        "ref_ids": [
          "1"
        ],
        "1": "Neural rendering has spurred an exploration into implicit and hybrid representations, offering various approaches for 3D editing, such as changing global appearance [7,29], intrinsic decomposition [59,64], per-object decomposition [54, 56], geometry and texture editing [1,55,61], 3D inpainting [36,52], and others [20,39,50]."
      },
      "Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review": {
        "authors": [
          "TAQ Nguyen",
          "A Bourki",
          "M Macudzinski"
        ],
        "url": "https://arxiv.org/pdf/2402.11141",
        "ref_texts": ""
      },
      "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields": {
        "authors": [
          "JN Dihlmann",
          "A Engelhardt",
          "H Lensch"
        ],
        "url": "https://arxiv.org/pdf/2401.01647",
        "ref_texts": "[2]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. pages 20919\u201320929, 2023. 3",
        "ref_ids": [
          "2"
        ],
        "1": "On the other hand, SINE [2] allows direct NeRF editing by changing a reference image in 2D space."
      },
      "Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise": {
        "authors": [
          "Q Huang",
          "Y Liao",
          "Y Hao",
          "P Zhou"
        ],
        "url": "https://arxiv.org/pdf/2401.01216",
        "ref_texts": "[11] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui, \u201cSine: Semanticdriven image-based nerf editing with prior-guided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 20919\u201320929.",
        "ref_ids": [
          "11"
        ],
        "1": "There are currently many improvements and application research on NeRF, including accelerated training of NeRF [8]\u2013[10], editing research on NeRF [11]\u2013\n[13], generalization research on NeRF [14]\u2013[17], and largescale scenes [18], [19], etc."
      },
      "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation": {
        "authors": [
          "D Jiang",
          "Z Ke",
          "X Zhou",
          "X Shi"
        ],
        "url": "https://arxiv.org/pdf/2310.16858",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 2",
        "ref_ids": [
          "2"
        ],
        "1": "Additionally, researchers have proposed blending editing methods [2, 14], which combine an auxiliary editing field with original NeRF to support creative editing."
      },
      "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh": {
        "authors": [
          "X Gao",
          "X Li",
          "Y Zhuang",
          "Q Zhang",
          "W Hu"
        ],
        "url": "https://arxiv.org/pdf/2405.17811",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 4",
        "ref_ids": [
          "1"
        ],
        "1": "Some other work [1,32,34,44] edit the NeRF in texture level which is not the focus of this paper."
      },
      "NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs": {
        "authors": [
          "M Fischer",
          "Z Li",
          "T Nguyen-Phuoc",
          "A Bozic"
        ],
        "url": "https://arxiv.org/pdf/2402.08622",
        "ref_texts": "[4] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 2",
        "ref_ids": [
          "4"
        ],
        "1": "Most of the aforementioned methods, however, ignore semantic similarity while performing stylization or appearance editing, with the exception of [4, 30, 50], who perform region-based stylization or appearance-editing of NeRFs, but do not change geometry.",
        "2": ", excluding topological changes [4, 30, 68])."
      },
      "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance Fields": {
        "authors": [
          "K Zhou",
          "L Hong",
          "E Xie",
          "Y Yang",
          "Z Li"
        ],
        "url": "https://arxiv.org/pdf/2312.15856",
        "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 1, 9",
        "ref_ids": [
          "3"
        ],
        "1": "In contrast, approaches like Get3D [17] and SINE [3] offer direct editing in the feature space, deviating from the high-resolution 2D feature maps common in 2D editing.",
        "2": "Subsequently, we leverage Du and{eto estimate the scene flow \u222beby applying the scene flow MLP proposed in DynPoint [75] and SINE [3]."
      },
      "Survey on controlable image synthesis with deep learning": {
        "authors": [
          "S Zhang",
          "J Li",
          "L Yang"
        ],
        "url": "https://arxiv.org/pdf/2307.10275",
        "ref_texts": "[171] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023.",
        "ref_ids": [
          "171"
        ],
        "1": "[171] proposed SINE, a novel approach for editing a neural radiance field (NeRF) with a single image or text prompts.",
        "2": "[171] C."
      },
      "Three-Dimensional-Consistent Scene Inpainting via Uncertainty-Aware Neural Radiance Field": {
        "authors": [
          "M Wang",
          "Q Yu",
          "H Liu"
        ],
        "url": "https://www.mdpi.com/2079-9292/13/2/448/pdf",
        "ref_texts": "9. Bao, C.; Zhang, Y.; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; Cui, Z. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 17\u201324 June 2023; pp. 20919\u201320929.",
        "ref_ids": [
          "9"
        ],
        "1": "Numerous endeavors have aimed at augmenting their performance and expanding their applicability via, for instance, improving the training speed [3\u20135], reducing view input requirements [6,7], facilitating scene editing [8,9], and extending their functionality to dynamic scenes [10,11].",
        "2": "Driven by the needs of practical applications, NeRf editing methods [9,12\u201319] have emerged as a focal point of current research.",
        "3": "Furthermore, research related to interactive NeRF editing has focused more on interactive target selection [16] or semantic editing [9]."
      },
      "SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field": {
        "authors": [
          "Z Song",
          "W Duan",
          "Y Zhang",
          "S Wang",
          "S Ma"
        ],
        "url": "https://arxiv.org/pdf/2402.16366",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "2"
        ],
        "1": "Neural Radiance Field Neural radiance field [28] has shown great ability in 3D reconstruction, and motivated massive follow-up works, such as editing [2, 43] and speeding-up [7, 29]."
      },
      "3D StreetUnveiler with Semantic-Aware 2DGS": {
        "authors": [
          "J Xu",
          "Y Wang",
          "Y Zhao",
          "Y Fu",
          "S Gao"
        ],
        "url": "https://arxiv.org/pdf/2405.18416",
        "ref_texts": "[2]Chong Bao, Yinda Zhang, and Bangbang et al. Yang. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , pages 20919\u201320929, 2023.",
        "ref_ids": [
          "2"
        ],
        "1": "With the rapid development of Neural Scene Representation, editing a 3D scene has been explored by lots of works [10,88,75,81,2,23,19,40].",
        "2": "Subsequent works [2,23,19,40] utilized CLIP models to provide editing guidance from text prompts or reference images.",
        "3": "[2]Chong Bao, Yinda Zhang, and Bangbang et al."
      },
      "Semantic-Human: Neural Rendering of Humans from Monocular Video with Human Parsing": {
        "authors": [
          "J Zhang",
          "P Shi",
          "Z Gu",
          "Y Zhou",
          "Z Wang"
        ],
        "url": "https://arxiv.org/pdf/2308.09894",
        "ref_texts": "Bao, C.; Zhang, Y .; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; and Cui, Z. 2023. Sine: Semantic-driven imagebased nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 20919\u201320929. Barron, J. T.; Mildenhall, B.; Tancik, M.; Hedman, P.; Martin-Brualla, R.; and Srinivasan, P. P. 2021. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 5855\u20135864. Cao, A.; and Johnson, J. 2023. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 130\u2013141. Chen, Y .; Wang, X.; Chen, X.; Zhang, Q.; Li, X.; Guo, Y .; Wang, J.; and Wang, F. 2023. UV V olumes for real-time rendering of editable free-view human performance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 16621\u201316631. Cheng, W.; Xu, S.; Piao, J.; Qian, C.; Wu, W.; Lin, K.-Y .; and Li, H. 2022. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798 . Deng, C. L.; and Tartaglione, E. 2023. Compressing explicit voxel grid representations: fast nerfs become also small. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 1236\u20131245. Fridovich-Keil, S.; Meanti, G.; Warburg, F. R.; Recht, B.; and Kanazawa, A. 2023. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 12479\u201312488. Gong, K.; Liang, X.; Li, Y .; Chen, Y .; Yang, M.; and Lin, L."
      },
      "VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality": {
        "authors": [
          "Y Jiang",
          "C Yu",
          "T Xie",
          "X Li",
          "Y Feng",
          "H Wang",
          "M Li"
        ],
        "url": "https://arxiv.org/pdf/2401.16663",
        "ref_texts": "(2009), 114\u2013123. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 20919\u201320929. Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629 (2021). Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. 2023. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. arXiv preprint arXiv:2311.14521"
      },
      "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation": {
        "authors": [
          "S Shen",
          "J Xu",
          "Y Yuan",
          "X Yang",
          "Q Shen"
        ],
        "url": "https://arxiv.org/pdf/2405.05800"
      },
      "LIVE: LaTex Interactive Visual Editing": {
        "authors": [
          "J Lin"
        ],
        "url": "https://arxiv.org/pdf/2405.06762",
        "ref_texts": "[33] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 20 919\u201320 929.",
        "ref_ids": [
          "33"
        ],
        "1": "2021\n|{z}2022\n|{z}2023\n|{z}\n\u22c6\u2192 \u22c6\u22c6\u2192 \u22c6 \u22c6 \u22c6\u2192\n\u2191 \u2191 \u2191\n[13]NeRF\n[28]pixelnerf [29]NeRF -[15]Mip -nerf [14]D-nerf[30]Headnerf [21]Block -nerf [31]Mip -nerf360[32]Nope -nerf [33]Sine The main function of Gitem FlowGraph is automatically generating a time sequence or other developing sequence flow interactive graph to describe the development of one issue.",
        "2": "[33] C."
      },
      "Plasticine3D: Non-rigid 3D editting with text guidance": {
        "authors": [
          "Y Chen",
          "A Chen",
          "S Chen",
          "R Yi"
        ],
        "url": "https://arxiv.org/pdf/2312.10111",
        "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 1, 3",
        "ref_ids": [
          "3"
        ],
        "1": "(4) Directly perform a controllable non-rigid transformation with a modified 3D NeRF representation[3], however they often require an additional guidance (e.",
        "2": "SINE[3] on the other hand, realizes controllable non-rigid editing by a modified 3D NeRF representation."
      },
      "ViFu: Multiple 360 Objects Reconstruction with Clean Background via Visible Part Fusion": {
        "authors": [
          "T Xu",
          "T Ikeda",
          "K Nishiwaki"
        ],
        "url": "https://arxiv.org/pdf/2404.09426",
        "ref_texts": "[27] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023.",
        "ref_ids": [
          "27"
        ],
        "1": "Another direction explores object-level manipulations on scene content, enabling editing to object appearance [26], [27] or geometry [4], [5].",
        "2": "[27] C."
      },
      "ViFu: Visible Part Fusion for Multiple Scene Radiance Fields": {
        "authors": [
          "T Xu",
          "T Ikeda",
          "K Nishiwaki"
        ],
        "url": "https://openreview.net/pdf?id=C3msSjudA7",
        "ref_texts": "Kaxlamangla S. Arun, T. S. Huang, and Steven D. Blostein. Least-squares fitting of two 3-d point sets. IEEE Transactions on Pattern Analysis and Machine Intelligence , PAMI-9:698\u2013700, 1987. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR) , pp. 510\u2013517. IEEE, 2015. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , 2022. Blender Online Community. Blender a 3D modelling and rendering package . Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org . Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Michael Hickman, Krista Reymann, Thomas Barlow McHugh, and Vincent Vanhoucke. Google scanned objects: A highquality dataset of 3d scanned household items. 2022 International Conference on Robotics and Automation (ICRA) , pp. 2553\u20132560, 2022. Martin Ester, Hans-Peter Kriegel, J \u00a8org Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Knowledge Discovery and Data Mining , 1996. Jiading Fang, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, and Matthew R Walter. Nerfuser: Large-scale scene representation by nerf fusion. arXiv preprint arXiv:2305.13307 , 2023. Martin A. Fischler and Robert C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM , 24:381\u2013395, 1981. Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas Funkhouser. Object-centric neural scene rendering. arXiv preprint arXiv:2012.08503 , 2020. Won Jun Jang and Lourdes de Agapito. Codenerf: Disentangled neural radiance fields for object categories. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 12929\u2013"
      },
      "Blended-NeRF: Zero-Shot Object Generation and Blending In Existing Neural Radiance Fields (Supplementary Material)": {
        "authors": [
          "O Gordon",
          "O Avrahami",
          "D Lischinski"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/supplemental/Gordon_Blended-NeRF_Zero-Shot_Object_ICCVW_2023_supplemental.pdf",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. arXiv preprint arXiv:2303.13277 , 2023. 5",
        "ref_ids": [
          "1"
        ],
        "1": "After sampling a camera pose, we recenter its rays around the ROI by moving its center location according to the center of mass inside the ROI (tracked by exponential moving average during training), but allow with a probability p\u2208[0,1](hyperparameter, set to 0.",
        "2": "In SINE [1], they suggest a method for editing NeRF scene by only editing a single view, and than apply the edit to the entire scene."
      }
    }
  },
  {
    "title": "mobile3dscanner: an online 3d scanner for high-quality object reconstruction with a mobile device",
    "id": 2,
    "valid_pdf_number": "2/4",
    "matched_pdf_number": "1/2",
    "matched_rate": 0.5,
    "citations": {
      "A portable V-SLAM based solution for advanced visual 3D mobile mapping": {
        "authors": [
          "A Torresani"
        ],
        "url": "https://iris.unitn.it/bitstream/11572/362031/1/PhD_thesis_final.pdf",
        "ref_texts": "[39] X. Xiang, H. Jiang, G. Zhang, Y. Yu, C. Li, X. Yang, D. Chen, and H.Bao, \u201cMobile3dscanner: Anonline3dscannerforhigh-qualityobject 93 reconstruction with a mobile device,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 27, no. 11, pp. 4245\u20134255, 2021.",
        "ref_ids": [
          "39"
        ],
        "1": "Since then, different real-time 3D reconstruction applications have been developed, both in the research [39] and commercial domains [40, 41, 42].",
        "2": "[39] X."
      },
      "\u30e2\u30eb\u30d5\u30a9\u30ed\u30b8\u30fc\u6f14\u7b97\u3092\u7528\u3044\u305f\u70b9\u7fa4\u30c7\u30fc\u30bf\u306e\u6b20\u640d\u691c\u51fa\u6cd5": {
        "authors": [
          "\u624b\u5cf6\u88d5\u8a5e\uff0c \u4e2d\u5c3e\u4eae\uff0c \u677e\u7530\u671d\u967d\uff0c \u5d8b\u7530\u82f1\u6a39\uff0c \u517c\u7530\u4e00\u5e78"
        ],
        "url": "https://www.jstage.jst.go.jp/article/jjsde/59/2/59_2023.2993/_pdf",
        "ref_texts": "8) Xiang, X., Jiang, H., Zhang, G., Yu, Y., Li, C., Yang , X., Chen, D. and Bao, H.: Mobile3DScanner: An "
      }
    }
  },
  {
    "title": "mobile3drecon: real-time monocular 3d reconstruction on a mobile phone",
    "id": 3,
    "valid_pdf_number": "32/44",
    "matched_pdf_number": "23/32",
    "matched_rate": 0.71875,
    "citations": {
      "Neuralrecon: Real-time coherent 3d reconstruction from monocular video": {
        "authors": [
          "J Sun",
          "Y Xie",
          "L Chen",
          "X Zhou"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.pdf",
        "ref_texts": "[51] Xingbin Yang, L. Zhou, Hanqing Jiang, Z. Tang, Yuanbo Wang, H. Bao, and Guofeng Zhang. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE TVCG , 2020. 2",
        "ref_ids": [
          "51"
        ],
        "1": "[46,51] optimize this line of research towards low power consumption on mobile platforms.",
        "2": "2\n[51] Xingbin Yang, L."
      },
      "Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation": {
        "authors": [
          "Z Li",
          "Z Chen",
          "X Liu",
          "J Jiang"
        ],
        "url": "https://link.springer.com/content/pdf/10.1007/s11633-023-1458-0.pdf",
        "ref_texts": "\u00a0X.\u00a0B.\u00a0Yang,\u00a0 L.\u00a0Y.\u00a0Zhou,\u00a0 H.\u00a0Q.\u00a0Jiang,\u00a0 Z.\u00a0L.\u00a0Tang,\u00a0 Y.\u00a0B. Wang,\u00a0 H.\u00a0J.\u00a0Bao,\u00a0 G.\u00a0F.\u00a0Zhang.\u00a0 Mobile3DRecon:\u00a0 Real-time monocular\u00a0 3D\u00a0reconstruction\u00a0 on\u00a0a\u00a0mobile\u00a0 phone.\u00a0 IEEE Transactions on Visualization and Computer Graphics , vol.\u00a026,\u00a0no.\u00a012,\u00a0pp.\u00a03446\u20133456,\u00a0 2020.\u00a0 DOI:\u00a0 10.1109/TVCG."
      },
      "Simplerecon: 3d reconstruction without 3d convolutions": {
        "authors": [
          "M Sayed",
          "J Gibson",
          "J Watson",
          "V Prisacariu"
        ],
        "url": "https://arxiv.org/pdf/2208.14743",
        "ref_texts": "77. Yang, X., Zhou, L., Jiang, H., Tang, Z., Wang, Y., Bao, H., Zhang, G.: Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics (2020)",
        "ref_ids": [
          "77"
        ],
        "1": "io/simplerecon 1 Introduction Generating 3D reconstructions of a scene is a challenging problem in computer vision which is useful for tasks such as robotic navigation, autonomous driving, content placement for augmented reality and historical preservation [47,77]."
      },
      "What's the Situation With Intelligent Mesh Generation: A Survey and Perspectives": {
        "authors": [
          "N Lei",
          "Z Li",
          "Z Xu",
          "Y Li",
          "X Gu"
        ],
        "url": "https://arxiv.org/pdf/2211.06009",
        "ref_texts": "[73] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE TVCG , vol. 26, no. 12, pp. 3446\u2013",
        "ref_ids": [
          "73"
        ],
        "1": "[57] \u2713 \u2713 \u2713 U-Net Normal, depth maps DMC [58] \u2713 \u2713 \u2713 \u2713 \u2713 DMC Occupancy and vertex displacement CoMA [59] \u2713 \u2713\u2713 \u2713 Autoencoder Point position 3D-CFCN [60] \u2713 \u2713 \u2713 OctNet-based U-Net Truncated signed distance field MGN [11] \u2713 \u2713 \u2713 \u2713 \u2713 MLP Vertices position 3DN [35] \u2713 \u2713 \u2713 \u2713 \u2713 PointNet/VGG Vertices position TMN [12] \u2713 \u2713 \u2713 \u2713 ResNet/MLP Vertices position and errors ONet [61] \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 ResNet/PointNet Grid occupancy N3DMM [62] \u2713 \u2713\u2713 \u2713 Spiral-Conv GAN Vertices position PGAN [63] \u2713 \u2713\u2713 WGAN Geometry image HumanMeshNet [13] \u2713 \u2713 \u2713 \u2713 Resnet-18 Vertices position DISN [64] \u2713 \u2713 \u2713 \u2713 VGG-16 Signed distance field IM-Net [65] \u2713\u2713 \u2713\u2713 \u2713 IM-Net Signed distance field Scan2Mesh [66] \u2713 \u2713 \u2713 \u2713 3D-Conv GNN Mesh face DGP [67] \u2713 \u2713 \u2713 \u2713 MLP Local parametrization Mesh R-CNN [14] \u2713 \u2713 \u2713 \u2713 Mesh R-CNN Occupancy and point position DeepSDF [68] \u2713 \u2713 \u2713 MLP Signed distance field Pixel2mesh++ [15] \u2713 \u2713 \u2713 \u2713 VGG; GCN Vertex position PQ-Net [69] \u2713\u2713 \u2713 \u2713\u2713 Seq2Seq Autoencoder Signed distance field BCNet [16] \u2713 \u2713 \u2713 \u2713 ResNet; GAT; Spiral-Conv SMPL parameters; vertices position PolyGen [70] \u2713 \u2713\u2713 \u2713 \u2713 Transformer-based Predict vertices and faces sequentially DGTS [71] \u2713 \u2713 \u2713 \u2713 GCN Displacement vector per face Neural Subdivision [72] \u2713 \u2713 \u2713 \u2713 MLP Predict vertex position Mobile3drecon [73] \u2713 \u2713 \u2713 Res-UNet Depth map Sal [74] \u2713 \u2713 \u2713 MLP Unsigned distance field Pixel2mesh2 [17] \u2713 \u2713 \u2713 \u2713 GCN;G-Resnet Vertex position Voxel2mesh [40] \u2713 \u2713 \u2713 \u2713 GCN-3D Vertex position X-ray2shape [18] \u2713 \u2713 \u2713 \u2713 CNN;GCN Vertex position Liet al.",
        "2": "According to the different selected implicit functions, these IMG methods are divided into four categories: radial basis functions (RBFs) [52], occupancy fields [32], [54], [57], [58], [61], [73], [78], [84], [88], [91], [110], [119], [121], signed distance functions (SDFs) [39], [60], [64], [65], [69], [76], [77], [80], [87], [90], [92], [94]\u2013[97], [99], [101], [103], [107]\u2013[109], [113], [117], [120], and unsigned distance function (UDFs) [74], [86], [89], [100].",
        "3": "Currently, various occupancy-based methods with different network architectures and training strategies have been proposed to continuously improve the efficiency, robustness, and accuracy [32], [57], [73], [78], [84], [88], [91], [110], [119], [121].",
        "4": "83 Mobile3drecon [73] Real-time mesh generation Real-time dense mesh reconstruction Poorly maintained sharp features 3.",
        "5": "Existing methodologies [13], [28], [73], [90], [91], [93], [95], [110], [119], however, fall short of satisfying these criteria simultaneously, resulting in scene meshes that tend to be overly smooth and lacking in detail.",
        "6": "[73] X."
      },
      "Transformr: Pose-aware object substitution for composing alternate mixed realities": {
        "authors": [
          "M Kari",
          "T Grosse-Puppendahl"
        ],
        "url": "https://mkari.de/pubs/ismar2021-transformr.pdf",
        "ref_texts": "[63] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang. Mobile3drecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020. doi: 10.1109/TVCG.2020.3023634",
        "ref_ids": [
          "63"
        ],
        "1": "Index Terms: Human-centered computing\u2014Mixed / augmented reality\u2014;\u2014\n1 I NTRODUCTION Continuous advances in geometric scene understanding have contributed to the physical coherence of virtual objects in mixed reality scenes, for example through improvements in mesh reconstruction [63], occlusion shading [7], visual-inertial odometry [20,57], or light source estimation [59].",
        "2": "3347875\n[63] X."
      },
      "GR-PSN: learning to estimate surface normal and reconstruct photometric stereo images": {
        "authors": [
          "Y Ju",
          "B Shi",
          "Y Chen",
          "H Zhou",
          "J Dong"
        ],
        "url": "https://figshare.le.ac.uk/articles/journal_contribution/GR-PSN_Learning_to_Estimate_Surface_Normal_and_Reconstruct_Photometric_Stereo_Images/24463159/1/files/42980308.pdf",
        "ref_texts": "[3] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "3"
        ],
        "1": "\u2726\n1 I NTRODUCTION RECOVERING the 3D shape of an object is a pivotal problem in many computer graphics and vision applications because it can further improve the understanding of images and scenes [1], [2], [3], [4]."
      },
      "MobiDepth: real-time depth estimation using on-device dual cameras": {
        "authors": [
          "J Zhang",
          "H Yang",
          "J Ren",
          "D Zhang",
          "B He",
          "T Cao"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3495243.3560517",
        "ref_texts": "[54] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. 2020. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics 26, 12 (2020), 3446\u20133456.",
        "ref_ids": [
          "54"
        ],
        "1": "3560517\n1 INTRODUCTION In recent years, the mobile industry and research community have significantly invested in augmented reality (AR) and virtual reality (VR) applications for mobile devices [8,11,20,37,53,54].",
        "2": "[54] propose a keyframe-based real-time surface mesh generation approach to reconstruct 3D objects from single RGB image."
      },
      "Mononeuralfusion: Online monocular neural 3d reconstruction with geometric priors": {
        "authors": [
          "ZX Zou",
          "SS Huang",
          "YP Cao",
          "TJ Mu",
          "Y Shan"
        ],
        "url": "https://arxiv.org/pdf/2209.15153",
        "ref_texts": "[18] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.[19] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, \u201cDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras,\u201d in IEEE ICCV , 2019, pp. 8977\u20138986.",
        "ref_ids": [
          "18",
          "19"
        ],
        "1": "With the progress of deep learning, some pioneering works [16], [17], [18] adopt the single-view depth estimation to monoc\u2022Zi-Xin Zou and Tai-Jiang Mu are with BNRist, the Department of Computer Science and Technology, Tsinghua University, Beijing, China.",
        "2": "However, given effective deep learning based monocular depth estimation approaches [19], [20], [21], it is still challenging to generate consistent depth estimation across different views, making it difficult to build coherent 3D reconstruction for large-scale VA/AR applications.",
        "3": "Mobile3DRecon [18] uses a multi-view semi-global matching method followed by a depth refinement post-processing for robust monocular depth estimation.",
        "4": "[18] X.",
        "5": "[19] A."
      },
      "FarfetchFusion: Towards Fully Mobile Live 3D Telepresence Platform": {
        "authors": [
          "K Lee",
          "J Yi",
          "Y Lee"
        ],
        "url": "https://scholar.archive.org/work/q4ljkmkzpjcgrc77uzr33txllu/access/wayback/https://dl.acm.org/doi/pdf/10.1145/3570361.3592525",
        "ref_texts": "[56] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang. Mobile3drecon: Real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "56"
        ],
        "1": "Existing mobile 3D reconstruction systems that leverages TSDF-based volumetric fusion focus on reconstructing static 3D scenes [38,42,56].",
        "2": "[56] X."
      },
      "The robodepth challenge: Methods and advancements towards robust depth estimation": {
        "authors": [
          "L Kong",
          "Y Niu",
          "S Xie",
          "H Hu",
          "LX Ng"
        ],
        "url": "https://arxiv.org/pdf/2307.15061",
        "ref_texts": "[119] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3drecon: real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics (TVCG) , 26(12):3446\u20133456, 2020. 9",
        "ref_ids": [
          "119"
        ],
        "1": "1 Overview Depth estimation is a fundamental task in 3D vision with vital applications, such as autonomous driving [93], augmented reality [123], virtual reality [59], and 3D reconstruction [119]."
      },
      "Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding": {
        "authors": [
          "X Zuo",
          "P Samangouei",
          "Y Zhou",
          "Y Di",
          "M Li"
        ],
        "url": "https://arxiv.org/pdf/2401.01970",
        "ref_texts": "[59] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3drecon: realtime monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u2013",
        "ref_ids": [
          "59"
        ],
        "1": "To estimate the dense 3d voxel cells, probabilistic fusion methods were firstly [22] used and researchers also developed end-to-end learn-able methods [50], by using either depth sensors [22] or monocular camera systems [59]."
      },
      "Efficient view path planning for autonomous implicit reconstruction": {
        "authors": [
          "J Zeng",
          "Y Li",
          "Y Ran",
          "S Li",
          "F Gao",
          "L Li"
        ],
        "url": "https://arxiv.org/pdf/2209.13159"
      },
      "Real-time globally consistent 3D reconstruction with semantic priors": {
        "authors": [
          "SS Huang",
          "H Chen",
          "J Huang",
          "H Fu"
        ],
        "url": "https://shishenghuang.github.io/index/Papers/semanticfusion/paper.pdf",
        "ref_texts": "[4] Y. Zhang, W. Xu, Y. Tong, and K. Zhou, \u201cOnline structure analysis for real-time indoor scene reconstruction,\u201d ACM Trans. Graphics , vol. 34, no. 5, pp. 159:1\u2013159:13, 2015.",
        "ref_ids": [
          "4"
        ],
        "1": "Most of early works have focused on either 3D reconstruction [1], [2], [3], [4], [5], [6], [7], [8], [9] or 3D semantic segmentation [10], [11], [12], [13], [14], [15], [16] separately.",
        "2": "[4] Y."
      },
      "Coli-ba: Compact linearization based solver for bundle adjustment": {
        "authors": [
          "Z Ye",
          "G Li",
          "H Liu",
          "Z Cui",
          "H Bao"
        ],
        "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/CoLi/CoLi.pdf",
        "ref_texts": "[45] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Transactions on Visualization and Computer Graphics, 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "45"
        ],
        "1": "Facing large scenes, the convergence becomes extremely slow, resulting in many offline reconstruction systems timecosting [3,35, 45].",
        "2": "[45] X."
      },
      "Robust real-time AUV self-localization based on stereo vision-inertia": {
        "authors": [
          "Y Wang",
          "D Gu",
          "X Ma",
          "J Wang"
        ],
        "url": "https://repository.essex.ac.uk/34811/1/Robust_Real_Time_AUV_Self_Localization_Based_on_Stereo_Vision_Inertia__final_version.pdf",
        "ref_texts": "[36] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone,\u201d IEEE Trans. Vis. Comput. Graphics , vol. 26, no. 12, pp. 3446\u2013",
        "ref_ids": [
          "36"
        ],
        "1": "In the literature, some kinds of visionbased methods have been applied to the localization of unmanned equipment [33], [34], [35], [36].",
        "2": "[36] X.",
        "3": ": DeepSLAM: A ROBUST MONOCULAR SLAM SYSTEM WITH UNSUPERVISED DL 3587\n[36] R."
      },
      "Time-Distributed Framework for 3D Reconstruction Integrating Fringe Projection with Deep Learning": {
        "authors": [
          "AH Nguyen",
          "Z Wang"
        ],
        "url": "https://www.mdpi.com/1424-8220/23/16/7284/pdf",
        "ref_texts": "32. Yang, X.; Zhuo, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef]",
        "ref_ids": [
          "32"
        ]
      },
      "Multi-sensor fusion self-supervised deep odometry and depth estimation": {
        "authors": [
          "Y Wan",
          "Q Zhao",
          "C Guo",
          "C Xu",
          "L Fang"
        ],
        "url": "https://www.mdpi.com/2072-4292/14/5/1228/pdf",
        "ref_texts": "3. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed]",
        "ref_ids": [
          "3"
        ],
        "1": "Introduction Dense depth estimation from an RGB image is the fundamental issue for 3D scene reconstruction that is useful for computer vision applications, such as automatic driving [1], simultaneous localization and mapping (SLAM) [2], and 3D scene understanding [3]."
      },
      "SimpleMapping: Real-time visual-inertial dense mapping with deep multi-view stereo": {
        "authors": [
          "Y Xin",
          "X Zuo",
          "D Lu"
        ],
        "url": "https://arxiv.org/pdf/2306.08648",
        "ref_texts": "[61] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "61"
        ],
        "1": "Mobile3DRecon [61] employs a multi-view semi-global matching method to recover a dense depth map, which is subsequently refined by a lightweight CNN-based single-view depth refinement neural network.",
        "2": "[61] X."
      },
      "Monocular Depth Estimation: Lightweight Convolutional and Matrix Capsule Feature-Fusion Network": {
        "authors": [
          "Y Wang",
          "H Zhu"
        ],
        "url": "https://www.mdpi.com/1424-8220/22/17/6344/pdf",
        "ref_texts": "5. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed]",
        "ref_ids": [
          "5"
        ],
        "1": "Obtaining a depth image of a real-world scene through depth estimation provides data that can serve as the basis for many applications, such as robots [1], autonomous driving [2], SLAM [3], augmented reality [4], 3D reconstruction [5], and segmentation [6]."
      },
      "Attention-enhanced cross-modal localization between 360 images and point clouds": {
        "authors": [
          "Z Zhao",
          "H Yu",
          "C Lyv",
          "W Yang",
          "S Scherer"
        ],
        "url": "https://arxiv.org/pdf/2212.02757",
        "ref_texts": "[2] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "2"
        ],
        "1": "I NTRODUCTION Locating the position of an image inthe point cloud map is of great importance for mobile robots and autonomous vehicles with numerous applications such as Simultaneous Localization and Mapping (SLAM) [1] and Virtual Reality [2].",
        "2": "[2] X."
      },
      "A Comprehensive Review of Vision-Based 3D Reconstruction Methods": {
        "authors": [
          "L Zhou",
          "G Wu",
          "Y Zuo",
          "X Chen",
          "H Hu"
        ],
        "url": "https://www.mdpi.com/1424-8220/24/7/2314/pdf",
        "ref_texts": "321. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed] Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
        "ref_ids": [
          "321"
        ],
        "1": "The popularity of mobile devices enables users to easily conduct image-based 3D scanning and 3D reconstruction [321]."
      },
      "Depth Completion with Multiple Balanced Bases and Confidence for Dense Monocular SLAM": {
        "authors": [
          "W Xie",
          "G Chu",
          "Q Qian",
          "Y Yu",
          "H Li",
          "D Chen"
        ],
        "url": "https://arxiv.org/pdf/2309.04145",
        "ref_texts": "[60] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3DRecon: real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics, 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "60"
        ],
        "1": "Our fusion method is based on an online incremental mesh generation method Mobile3DRecon [60]."
      },
      "Real-time hybrid mapping of populated indoor scenes using a low-cost monocular uav": {
        "authors": [
          "S Golodetz",
          "M Vankadari",
          "A Everitt"
        ],
        "url": "https://arxiv.org/pdf/2203.02453",
        "ref_texts": "[42] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone,\u201d TVCG , 2020.",
        "ref_ids": [
          "42"
        ],
        "1": "We select the keyframe that maximises the following score (a simplified version of that in [42]): Sf(k) =I[\u2206f t(k)\u2265\u03c4tand\u2206f \u03b8(k)\u2264\u03c4\u03b8]e(\u2212(\u2206f t(k)\u2212\u03b4t)2/\u03c32 t)(3) In this, Idenotes the binary indicator function, fdenotes the frame,kdenotes a keyframe, \u2206f tand\u2206f \u03b8respectively denote the baseline (m) and angle (\u25e6) betweenfandk,\u03b4t= 0.",
        "2": "[42] X."
      },
      "Method for automated data collection for 3d reconstruction": {
        "authors": [
          "M Zaslavskiy",
          "R Shestopalov"
        ],
        "url": "https://fruct.org/publications/volume-32/fruct32/files/Zas.pdf"
      },
      "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization": {
        "authors": [
          "P Zhu",
          "Y Zhuang",
          "B Chen",
          "L Li",
          "C Wu",
          "Z Liu"
        ],
        "url": "https://arxiv.org/pdf/2405.06241",
        "ref_texts": "[12] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: Real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "12"
        ],
        "1": "Another study [11], [12] combines a real-time VO/SLAM system with a Multi-View Stereo (MVS) network for parallel tracking and dense depth estimation, and then the Truncated Signed Distance Function (TSDF) [13] is used to fuse depth maps and extract mesh.",
        "2": "[12] X."
      },
      "DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge": {
        "authors": [
          "Y Mao",
          "M Li",
          "J Liu",
          "J Liu",
          "Z Qin",
          "C Chu",
          "J Xu"
        ],
        "url": "https://arxiv.org/pdf/2405.17102"
      },
      "The present and future of mixed reality in China": {
        "authors": [
          "G Zhang",
          "X Zhou",
          "F Tian",
          "H Zha",
          "Y Wang"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3481619",
        "ref_texts": "25. Yang, X., Zhou, L., Jiang, H. Tang, Z., Wang, Y., Bao, H., Zhang, G. Mobile3DRecon: real-time monocular ",
        "ref_ids": [
          "25"
        ]
      },
      "Parallel Implementation of 3D Model Reconstruction of Monocular Video Frames in a Dynamic Environment.": {
        "authors": [
          "GM Fathy",
          "HA Hassan",
          "WM Sheta",
          "F Omara"
        ],
        "url": "https://inass.org/wp-content/uploads/2022/05/2022083153-2.pdf",
        "ref_texts": "[20] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang , \u201cMobile3drecon: real time monocular 3d reconstruction on a mobile phone \u201d, IEEE Transactions on Visual -Ization and Computer Graphics , Vol. 26, No. 12, pp. ",
        "ref_ids": [
          "20"
        ],
        "1": "Scene Methods Device Time \n(T/F) \n[19] Single Static object Monocular SLAM NVIDIA GeForce TITAN X without CUDA 21 ms \n[21] Single Dynamic object Markless \n3D human motion capture GeForce RTX 2070 without CUDA 40 ms \n[22] Single Dynamic object GCN network Nvidia GeForce RTX 2080Ti \n-without CUDA 23 ms \n[20] Full Static scene Online incremental mesh generation a single CPU thread third -party library OpenCV 2 57.",
        "2": "[20] X."
      },
      "A portable V-SLAM based solution for advanced visual 3D mobile mapping": {
        "authors": [
          "A Torresani"
        ],
        "url": "https://iris.unitn.it/bitstream/11572/362031/1/PhD_thesis_final.pdf",
        "ref_texts": "[100] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201dIEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "100"
        ],
        "1": "Despite in the latest years some works were able to achieve it [48, 100], important compromises on the image resolutions, maximum scene size and reconstruction accuracy are still necessary.",
        "2": "[100] X."
      },
      "\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0441\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0440\u0435\u0445\u043c\u0435\u0440\u043d\u043e\u0439 \u0440\u0435\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0438": {
        "authors": [
          "\u0420\u041f \u0428\u0435\u0441\u0442\u043e\u043f\u0430\u043b\u043e\u0432",
          "\u041c\u041c \u0417\u0430\u0441\u043b\u0430\u0432\u0441\u043a\u0438\u0439"
        ],
        "url": "https://etu.ru/assets/files/Faculty-FKTI/MO/sbornik-2022-moevm.pdf#page=16",
        "ref_texts": ""
      },
      "On-Device 3D Foot Reconstruction for Digital Sizing": {
        "authors": [
          "N Hassan"
        ],
        "url": "https://tspace.library.utoronto.ca/bitstream/1807/129715/2/Hassan_Najah_202211_MAS_thesis.pdf",
        "ref_texts": ""
      },
      "Isovist computation of outdoor environment with semi-dense line SLAM and monocular camera": {
        "authors": [
          "T Le Jan",
          "M Servi\u00e8res",
          "T Leduc",
          "V Tourre"
        ],
        "url": "https://hal.science/hal-03368466/document",
        "ref_texts": ""
      }
    }
  }
]
  var barDiv = document.getElementById("bar");
  for (var paper of papers) {
    // console.log(paper.title);
    count++;
    var flag = 0;

    citations = paper.citations
    for (var citationName in citations) {
      if (citations.hasOwnProperty(citationName)) {
        var citation = citations[citationName];
        for (author of citation.authors) {
          if (author in fellows) {
            flag = 1;
            break;
          }
        }
      }
      if(flag==1)
      {
        break;
      }
    }
    if (flag == 1) {
      var content = "<div class='item item-fellow' id='" + count + "'>" + paper.title + "</div>"
    }
    else {
      var content = "<div class='item' id='" + count + "'>" + paper.title + "</div>"
    }

    var objDiv = document.createElement("div");
    objDiv.innerHTML = content
    barDiv.appendChild(objDiv);
  }



  // 主页面内容js

  count = 0;
  var wrapperDiv = document.getElementById("wrapper");
  for (var paper of papers) {
    // console.log(paper.title)
    count++;
    var content = "<div class='paper' id='" + count + "'> <h1>" + paper.title + "</h1>"
      + "<ol>"
    citations = paper.citations
    for (var citationName in citations) {
      if (citations.hasOwnProperty(citationName)) {
        var citation = citations[citationName];
        var flag = 0;
        for (author of citation.authors) {
          if (author in fellows) {
            flag = 1;
          }
        }
        if (flag == 0) {
            content += "<li><div class='name'><a href='"  + citation.url + "' target='_blank'> " + citationName + "</a></div>"
          content += "<div class='author'>"
        }
        else {
          content += "<li><div class='name famous'><a href='" + citation.url + "'target='_blank'> " + citationName + "</a></div>"
          content += "<div class='author'>"
        }
        for (author of citation.authors) {
          if (author in fellows) {
            content += "<span>" + author + " (" + fellows[author] + ")</span> , "
          }
          else {
            content += author + ", "
          }
        }
        content += "</div>"
        if(flag==0)
        {
          content += "<div class='conment'><ul>"
        }
        else
        {
          content += "<div class='conment conment-fellows'><ul>"
        }
        ref_ids=citation.ref_ids;
        for (var key in citation) {
          var inputString=citation[key];
          if (citation.hasOwnProperty(key) && !isNaN(key)) {
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\[${number}\\]`, 'g');
              inputString = inputString.replace(regex, `<span>[${number}]</span>`);
              
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\[${number},`, 'g');
              inputString = inputString.replace(regex, `[<span>${number}</span>,`);
              
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`,${number}\\]`, 'g');
              inputString = inputString.replace(regex, `,<span>${number}</span>]`);
              
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`,${number},`, 'g');
              inputString = inputString.replace(regex, `,<span>${number}</span>,`);
              
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\s${number},`, 'g');
              inputString = inputString.replace(regex, ` <span>${number}</span>,`);
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`,${number}\\s`, 'g');
              inputString = inputString.replace(regex, `,<span>${number}</span> `);
            });
            ref_ids.forEach(number => {
              const regex = new RegExp(`\\s${number}\\]`, 'g');
              inputString = inputString.replace(regex, ` <span>${number}</span>]`);
            });
            content += "<li>" + inputString + "</li>";
          }
        }
        
        content += "</ul></div></li>"
      }
    }
    content += "</ol></div>";
    var objDiv = document.createElement("div");
    objDiv.innerHTML = content
    wrapperDiv.appendChild(objDiv);
  }


  // 点击切换
  const sidebarItems = document.querySelectorAll('.item');
  const contentItems = document.querySelectorAll('.paper');

  sidebarItems.forEach((item, index) => {
    item.addEventListener('click', () => {
      contentItems.forEach(contentItem => {
        contentItem.classList.add('hidden');
      });
      contentItems[index].classList.remove('hidden');

      sidebarItems.forEach(sidebarItem => {
        sidebarItem.classList.remove('active');
      });

      item.classList.add('active');


    });
  });

</script>

<style>
  ul li span{
    color:#f0d002;
    font-weight: 700;
  }
  #bar {
    position: fixed;
    top: 30px;
    left: 0;
    width: 22vw;
    padding-right: 1vw;
    padding-left: 2vw;
    height: 95vh;
    overflow: auto
  }

  .item {
    margin-bottom: 10px;
    border-radius: 10px;
    border: #fff 2px solid;
    padding: 5px;
    padding-left: 10px;
    transition: 0.3s;

  }

  .item:hover {

    border-color: rgb(12, 119, 186);
  }

  .active {
    border-color: rgb(12, 119, 186);
  }

  body {
    background-color: rgb(231, 231, 231);
  }

  #wrapper {

    margin-left: 26vw;
    margin-right: 2vw;
    background-color: #fff;
    padding: 40px;
  }

  .famous {
    color: rgb(12, 119, 186);
    font-weight: 800;

  }
  .item-fellow{
    color:rgb(12, 119, 186);
    font-weight:800;
  }

  .author span {
    /* color: rgb(12, 119, 186); */
    font-weight: 800;
  }
  .conment-fellows{
    border-left: solid rgb(12, 119, 186) 2px !important;
  }

  a {
    text-decoration: none;
    color: inherit;
    transition: 0.3s;
    display: inline-block;
    font-weight: 600;
    font-size: 19px;
  }

  a:hover {
    font-size: 21px;
    font-weight: 900 !important;
  }

  li div {
    margin-bottom: 15px;
    font-size: 17px
  }

  ol>li {
    margin-bottom: 40px;
  }

  .conment {
    max-height: 300px;
    overflow: auto;
    border-left: solid grey;
  }

  .author {
    font-style: italic;
    color: rgb(81, 81, 81)
  }

  .hidden {
    display: none;
  }

  ol.no-number {
    list-style-type: none;
    padding-left: 0;
    /* 可选，去除默认的缩进 */
  }
</style>

</html>